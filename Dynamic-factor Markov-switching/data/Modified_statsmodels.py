from __future__ import division, absolute_import, print_function
import numpy as np
import pandas as pd
from collections import OrderedDict
from scipy.stats import norm
from statsmodels.tsa.statespace.simulation_smoother import SimulationSmoother
from statsmodels.tsa.statespace.kalman_smoother import SmootherResults
import statsmodels.tsa.base.tsa_model as tsbase
import statsmodels.base.wrapper as wrap
from statsmodels.tools.numdiff import (_get_epsilon, approx_hess_cs, approx_fprime_cs, approx_fprime)
from statsmodels.tools.eval_measures import aic, bic, hqic
from statsmodels.tools.tools import pinv_extended
import statsmodels.genmod._prediction as pred
from statsmodels.genmod.families.links import identity
from scipy.special import logsumexp
from statsmodels.multivariate.pca import PCA
from statsmodels.regression.linear_model import OLS
from statsmodels.tsa.vector_ar.var_model import VAR
from statsmodels.tools.tools import Bunch
from statsmodels.tools.data import _is_using_pandas
from statsmodels.tsa.tsatools import lagmat
from statsmodels.tools.decorators import cache_readonly
from statsmodels.tsa.statespace import tools
import warnings


#README: This is not the Master File and only was provided to include modified statsmodels functions.
#Please note, that this file contains a modified statsmodels library which was initally developed by
#Valera Likhosherstov. The goal of his development was to implement a dynamic factor with regime swichting
#into the statsmodels framework. However, his results were never merged, such that there is no dynamic-factor
#model with Markov swichting currently available in the statsmodels library or in other libraries. The adjustments
#that I made were basically to transfer this code from 2016 to the newer version of statsmodels in order to find a way
#to make this code compatible. The initial comments by Valera Likhosherstov are still included in this file using docstrings
#Reference: https://github.com/ValeryTyumen/statsmodels/tree/MS_SSM_start/statsmodels/tsa/statespace
#Reference2: https://github.com/statsmodels/statsmodels/pull/2921/commits

sw_ms = {
    'data': {
        'ip': [
            36.0, 36.7, 37.2, 38.0, 38.6, 38.6, 37.7, 36.4, 36.4, 36.1,
            36.3, 38.6, 39.6, 39.2, 38.9, 38.6, 38.5, 38.1, 37.9, 37.9,
            37.5, 37.4, 36.9, 36.2, 36.3, 36.2, 36.4, 37.2, 37.7, 38.3,
            38.7, 39.1, 39.0, 39.8, 40.4, 40.7, 40.4, 41.1, 41.3, 41.4,
            41.3, 41.2, 41.6, 41.7, 41.9, 42.0, 42.2, 42.2, 42.5, 42.9,
            43.2, 43.6, 44.1, 44.3, 44.1, 44.2, 44.6, 44.9, 45.1, 45.1,
            45.5, 45.8, 45.8, 46.5, 46.8, 46.9, 47.2, 47.5, 47.7, 47.0,
            48.5, 49.1, 49.6, 49.9, 50.6, 50.8, 51.2, 51.6, 52.1, 52.3,
            52.4, 52.9, 53.2, 53.8, 54.4, 54.7, 55.5, 55.5, 56.1, 56.3,
            56.6, 56.7, 57.2, 57.6, 57.2, 57.3, 57.6, 57.0, 56.6, 57.2,
            56.7, 56.7, 56.5, 57.6, 57.5, 58.0, 58.8, 59.5, 59.4, 59.6,
            59.8, 59.9, 60.6, 60.8, 60.7, 60.9, 61.1, 61.2, 62.0, 62.2,
            62.6, 63.0, 63.5, 63.2, 63.0, 63.6, 63.9, 64.1, 64.1, 64.1,
            63.5, 63.3, 62.1, 62.1, 62.0, 61.9, 61.8, 61.6, 61.7, 61.6,
            61.2, 60.0, 59.6, 61.0, 61.5, 61.3, 61.3, 61.6, 61.9, 62.2,
            62.0, 61.7, 62.7, 63.1, 63.4, 64.1, 65.6, 66.0, 66.5, 67.6,
            67.5, 67.7, 67.6, 68.5, 69.2, 70.2, 71.1, 71.7, 71.8, 72.8,
            72.8, 73.0, 73.4, 73.9, 74.4, 74.3, 74.9, 75.2, 75.2, 74.0,
            73.0, 72.7, 73.0, 72.9, 73.8, 74.0, 73.6, 73.4, 73.7, 73.2,
            71.1, 68.1, 66.3, 65.3, 64.1, 64.7, 64.5, 65.3, 65.7, 66.9,
            67.6, 67.9, 68.6, 69.1, 69.9, 71.1, 70.9, 71.2, 72.0, 72.1,
            72.5, 72.9, 73.1, 73.4, 74.6, 75.2, 75.5, 75.9, 76.6, 77.7,
            78.3, 78.9, 78.9, 79.0, 79.4, 79.4, 79.5, 79.1, 78.8, 79.0,
            80.0, 82.0, 82.3, 83.1, 83.3, 83.6, 84.1, 84.5, 85.2, 85.4,
            85.1, 85.8, 86.1, 85.2, 86.2, 86.1, 85.6, 85.3, 85.5, 86.0,
            85.7, 85.6, 85.9, 86.2, 86.2, 84.5, 82.5, 81.5, 81.2, 82.4,
            83.5, 84.0, 85.5, 85.9, 85.2, 85.4, 85.7, 85.0, 85.6, 86.1,
            87.1, 86.9, 86.5, 85.8, 84.8, 84.1, 82.4, 84.2, 83.7, 83.2,
            82.7, 82.4, 82.0, 81.6, 81.0, 80.3, 80.0, 79.3, 80.8, 80.7,
            81.3, 82.3, 83.2, 83.7, 85.3, 86.5, 87.9, 88.6, 88.8, 89.2,
            91.0, 90.9, 91.9, 92.4, 93.0, 93.5, 93.9, 94.0, 93.9, 93.2,
            93.3, 92.8, 93.1, 93.8, 94.1, 94.5, 94.7, 94.4, 94.1, 94.5,
            95.0, 94.2, 94.6, 95.6, 96.1, 95.5, 94.6, 94.8, 94.7, 94.3,
            94.8, 94.9, 95.0, 95.6, 96.3, 96.8, 96.507, 97.885, 98.249,
            98.774, 99.396, 100.251, 100.616, 100.92, 100.716, 102.052,
            102.158, 102.825, 103.157, 103.389, 103.401, 104.254, 103.982,
            104.012, 104.61, 105.206, 104.684, 104.994, 105.599, 106.271,
            106.641, 106.157, 107.072, 107.071, 106.702, 106.36, 105.306,
            105.845, 105.376, 104.988, 105.357, 106.06, 105.519, 106.091,
            106.423, 105.722, 106.479, 106.669, 106.492, 106.798, 106.815,
            106.322, 104.967, 104.538, 104.005, 103.105, 102.098, 102.614,
            103.466, 104.419, 104.703, 104.813, 105.748, 105.634, 105.583,
            105.19, 104.881, 105.783, 106.449, 106.929, 107.502, 107.153,
            108.07, 108.011, 108.2, 108.845, 109.915, 110.447, 110.65,
            111.305, 111.395, 111.379, 111.125, 111.462, 111.952, 112.193,
            112.542, 112.653, 113.699, 114.711, 114.744, 115.626, 116.618,
            116.714, 117.351, 118.023, 118.231, 119.128, 119.0, 119.5, 120.3,
            121.7, 122.2
        ],
        'gmyxpq': [
            1318.7, 1326.1, 1338.1, 1349.3, 1354.6, 1359.3, 1360.7, 1346.1,
            1344.1, 1346.7, 1366.1, 1381.6, 1387.1, 1379.9, 1378.0, 1380.7,
            1390.9, 1390.5, 1388.0, 1388.4, 1385.1, 1385.6, 1383.0, 1377.9,
            1389.5, 1390.9, 1391.3, 1400.4, 1408.7, 1415.9, 1417.6, 1425.5,
            1425.8, 1439.8, 1454.8, 1462.4, 1459.1, 1464.3, 1475.7, 1481.9,
            1480.9, 1491.1, 1497.5, 1495.7, 1494.4, 1500.7, 1505.3, 1512.3,
            1505.6, 1515.5, 1515.1, 1520.7, 1529.1, 1530.7, 1533.4, 1541.5,
            1552.3, 1564.1, 1564.9, 1573.7, 1576.5, 1586.2, 1593.8, 1604.5,
            1609.6, 1618.2, 1628.2, 1636.3, 1646.2, 1649.7, 1663.7, 1677.1,
            1676.5, 1678.3, 1686.4, 1691.9, 1708.8, 1722.0, 1726.3, 1735.0,
            1738.8, 1759.5, 1775.6, 1784.3, 1783.1, 1786.1, 1791.1, 1798.7,
            1806.6, 1817.4, 1822.9, 1830.0, 1829.1, 1841.4, 1846.5, 1848.4,
            1863.9, 1863.2, 1865.3, 1864.4, 1872.1, 1880.5, 1889.2, 1897.1,
            1893.7, 1897.8, 1909.1, 1923.2, 1922.1, 1937.9, 1941.0, 1948.3,
            1961.7, 1971.6, 1982.3, 1984.5, 2000.3, 2002.1, 2011.4, 2022.2,
            2020.1, 2025.1, 2037.2, 2040.2, 2051.0, 2056.0, 2074.2, 2079.9,
            2086.9, 2093.0, 2098.3, 2097.4, 2092.0, 2087.2, 2090.6, 2098.3,
            2093.0, 2087.6, 2096.6, 2101.7, 2101.1, 2091.7, 2091.7, 2093.1,
            2104.6, 2103.8, 2105.4, 2107.0, 2116.7, 2114.4, 2117.3, 2130.3,
            2130.2, 2134.9, 2147.2, 2166.6, 2177.2, 2188.3, 2195.1, 2208.1,
            2215.8, 2199.2, 2232.7, 2256.6, 2267.8, 2289.8, 2314.0, 2327.8,
            2331.1, 2331.6, 2330.8, 2324.0, 2346.6, 2355.5, 2355.0, 2368.6,
            2379.6, 2409.3, 2428.2, 2421.8, 2390.7, 2364.1, 2334.9, 2322.9,
            2324.4, 2330.2, 2334.8, 2327.5, 2330.3, 2337.2, 2314.7, 2305.3,
            2276.6, 2255.9, 2259.0, 2256.5, 2264.0, 2271.2, 2273.5, 2294.9,
            2309.9, 2326.6, 2328.9, 2328.0, 2349.0, 2362.5, 2370.7, 2380.6,
            2391.0, 2388.2, 2392.3, 2398.1, 2402.1, 2401.9, 2424.9, 2432.3,
            2432.8, 2439.8, 2447.1, 2452.7, 2469.9, 2473.8, 2496.8, 2510.8,
            2524.5, 2527.7, 2534.5, 2546.8, 2550.0, 2562.3, 2590.8, 2620.4,
            2623.4, 2640.0, 2642.0, 2654.8, 2668.7, 2685.4, 2692.5, 2702.8,
            2701.6, 2714.6, 2726.6, 2712.6, 2711.3, 2717.9, 2725.7, 2727.0,
            2728.8, 2737.0, 2744.2, 2749.7, 2753.7, 2744.1, 2731.4, 2706.4,
            2686.5, 2684.0, 2676.4, 2692.2, 2706.6, 2741.9, 2760.9, 2781.2,
            2776.2, 2772.6, 2775.7, 2775.9, 2772.4, 2781.5, 2799.9, 2814.3,
            2813.5, 2805.3, 2793.9, 2781.9, 2768.6, 2776.1, 2781.7, 2800.5,
            2806.1, 2788.1, 2776.4, 2771.7, 2762.0, 2758.7, 2756.5, 2767.6,
            2767.8, 2760.2, 2764.4, 2778.0, 2791.7, 2800.3, 2814.1, 2804.3,
            2826.3, 2862.2, 2879.3, 2900.7, 2922.4, 2963.6, 2982.9, 2987.2,
            2986.8, 3007.7, 3023.7, 3038.6, 3064.3, 3046.0, 3060.1, 3098.8,
            3081.3, 3092.9, 3103.6, 3118.7, 3098.2, 3109.9, 3104.1, 3104.5,
            3105.1, 3123.6, 3118.7, 3154.3, 3139.7, 3158.7, 3187.4, 3227.7,
            3212.3, 3199.5, 3198.3, 3211.0, 3216.3, 3206.7, 3209.9, 3233.1,
            3220.2, 3240.1, 3240.6, 3242.8, 3233.5, 3232.3, 3242.1, 3255.8,
            3258.5, 3309.4, 3292.7, 3345.5, 3307.9, 3322.3, 3335.3, 3342.1,
            3341.6, 3350.7, 3355.2, 3357.6, 3362.6, 3410.6, 3386.5, 3411.5,
            3427.6, 3445.5, 3455.2, 3448.3, 3429.0, 3422.5, 3429.0, 3424.8,
            3414.2, 3432.1, 3447.5, 3457.2, 3457.4, 3476.7, 3487.8, 3495.0,
            3481.5, 3482.6, 3485.1, 3464.3, 3461.2, 3429.0, 3439.3, 3467.5,
            3419.2, 3414.3, 3426.2, 3418.3, 3415.8, 3425.4, 3410.4, 3404.6,
            3410.5, 3412.8, 3401.1, 3431.7, 3412.1, 3440.8, 3445.8, 3447.5,
            3446.9, 3446.3, 3449.4, 3455.9, 3473.1, 3496.7, 3501.1, 3706.5,
            3461.0, 3465.2, 3475.1, 3523.8, 3533.0, 3514.8, 3506.0, 3543.1,
            3539.6, 3556.1, 3571.8, 3589.3, 3566.9, 3618.9, 3629.0, 3641.9,
            3652.7, 3649.3, 3654.2, 3665.0, 3683.6, 3735.7, 3727.3, 3751.4,
            3760.8],
        'mtq': [
            177579.0, 180295.0, 182424.0, 185177.0, 187213.0, 187464.0,
            186905.0, 180702.0, 180205.0, 180599.0, 181001.0, 186020.0,
            189271.0, 188409.0, 187454.0, 187916.0, 185445.0, 185299.0,
            184402.0, 183416.0, 185475.0, 184494.0, 182100.0, 182475.0,
            178627.0, 179133.0, 182209.0, 181214.0, 183946.0, 187282.0,
            185026.0, 189399.0, 190051.0, 192566.0, 194393.0, 195194.0,
            195352.0, 195537.0, 198194.0, 198356.0, 198124.0, 197286.0,
            198071.0, 199471.0, 198952.0, 200913.0, 203713.0, 200284.0,
            201169.0, 204347.0, 204465.0, 206370.0, 204950.0, 206727.0,
            209795.0, 208574.0, 208791.0, 211015.0, 208723.0, 213334.0,
            214560.0, 214575.0, 214008.0, 217537.0, 219879.0, 219279.0,
            222959.0, 222257.0, 224339.0, 220983.0, 223310.0, 230443.0,
            229885.0, 230640.0, 235579.0, 236142.0, 233649.0, 234691.0,
            238911.0, 237557.0, 237547.0, 239786.0, 243138.0, 243988.0,
            246684.0, 246953.0, 250964.0, 249027.0, 248149.0, 250815.0,
            249321.0, 250896.0, 250438.0, 251112.0, 250109.0, 251214.0,
            252621.0, 252020.0, 253265.0, 253761.0, 253796.0, 254250.0,
            254249.0, 257298.0, 256699.0, 254730.0, 261763.0, 267141.0,
            265864.0, 264932.0, 267361.0, 267119.0, 268528.0, 271129.0,
            274384.0, 270066.0, 271294.0, 275450.0, 277670.0, 275368.0,
            275855.0, 276373.0, 277968.0, 279301.0, 278781.0, 278746.0,
            280072.0, 281564.0, 283152.0, 285865.0, 282135.0, 281792.0,
            279400.0, 279282.0, 276126.0, 274134.0, 277364.0, 278207.0,
            278441.0, 277112.0, 276793.0, 272273.0, 267731.0, 276499.0,
            279715.0, 281661.0, 282908.0, 284258.0, 286259.0, 289727.0,
            288174.0, 286277.0, 289563.0, 290422.0, 295093.0, 297156.0,
            300201.0, 297396.0, 302716.0, 305077.0, 306605.0, 307938.0,
            308992.0, 314304.0, 317743.0, 321720.0, 326386.0, 331125.0,
            335572.0, 336568.0, 334362.0, 332728.0, 331496.0, 330711.0,
            335267.0, 330662.0, 330325.0, 337743.0, 342034.0, 337065.0,
            338234.0, 337134.0, 340708.0, 339270.0, 340309.0, 338974.0,
            340112.0, 335795.0, 332271.0, 327459.0, 323059.0, 312343.0,
            314166.0, 312571.0, 303419.0, 308306.0, 307936.0, 310588.0,
            313554.0, 315628.0, 316972.0, 317251.0, 315777.0, 319167.0,
            327307.0, 329157.0, 331645.0, 334615.0, 333800.0, 337423.0,
            338535.0, 337677.0, 338062.0, 334574.0, 340175.0, 350306.0,
            348170.0, 352455.0, 356339.0, 357357.0, 356424.0, 359677.0,
            360474.0, 360774.0, 361351.0, 362999.0, 365148.0, 369826.0,
            361385.0, 368449.0, 371288.0, 381447.0, 380766.0, 381737.0,
            379394.0, 385200.0, 383969.0, 388325.0, 390207.0, 391501.0,
            390311.0, 386651.0, 397051.0, 384979.0, 395454.0, 389307.0,
            389666.0, 389991.0, 388185.0, 388053.0, 387094.0, 385697.0,
            392835.0, 387465.0, 378012.0, 370276.0, 363974.0, 362525.0,
            368215.0, 369349.0, 377120.0, 384277.0, 383965.0, 384437.0,
            386997.0, 385511.0, 383406.0, 384070.0, 380834.0, 380509.0,
            380508.0, 378832.0, 376090.0, 370117.0, 367217.0, 362912.0,
            360956.0, 367357.0, 366724.0, 366034.0, 368412.0, 363473.0,
            363756.0, 360466.0, 360457.0, 357085.0, 358425.0, 355895.0,
            364244.0, 362022.0, 367358.0, 367559.0, 373644.0, 383026.0,
            383113.0, 382368.0, 388192.0, 392992.0, 397711.0, 404330.0,
            406597.0, 406454.0, 406443.0, 409273.0, 412897.0, 416925.0,
            414612.0, 413394.0, 413473.0, 415489.0, 419077.0, 419995.0,
            419796.0, 420338.0, 423347.0, 423752.0, 428318.0, 422405.0,
            423369.0, 430477.0, 431734.0, 426741.0, 430444.0, 428476.0,
            434221.0, 432399.0, 432011.0, 440859.0, 436751.0, 440049.0,
            443290.0, 442924.0, 453686.0, 447725.0, 447543.0, 455846.0,
            442515.0, 457546.0, 457077.0, 455965.0, 456567.0, 457796.0,
            461888.0, 459794.0, 464413.0, 463428.0, 462360.0, 465492.0,
            464547.0, 470126.0, 476501.0, 474210.0, 475560.0, 479893.0,
            476922.0, 478295.0, 479596.0, 483231.0, 484494.0, 491754.0,
            491361.0, 484434.0, 481752.0, 486518.0, 484551.0, 482811.0,
            478270.0, 491174.0, 487014.0, 481808.0, 485802.0, 486619.0,
            482128.0, 489452.0, 493604.0, 488049.0, 490649.0, 492459.0,
            488061.0, 493015.0, 484521.0, 482954.0, 476887.0, 472486.0,
            467502.0, 470922.0, 469071.0, 478060.0, 480621.0, 479321.0,
            484246.0, 484883.0, 485639.0, 486130.0, 484716.0, 477881.0,
            484727.0, 486571.0, 488381.0, 489532.0, 486602.0, 491755.0,
            495231.0, 491825.0, 496145.0, 498844.0, 501508.0, 510863.0,
            512323.0, 512635.0, 511548.0, 511941.0, 513911.0, 519971.0,
            515918.0, 524224.0, 527070.0, 529760.0, 534944.0, 541026.0,
            539396.0, 545071.0, 552520.0, 549492.0, 550033.0, 553990.0,
            549988.0, 565578.0, 564681.0, 566945.0, 572659.0, 578177.0,
            576156.0
        ],
        'lpnag': [
            52408.0, 52568.0, 52883.0, 53132.0, 53422.0, 53584.0, 53663.0,
            53220.0, 53257.0, 53196.0, 53509.0, 54040.0, 54185.0, 54414.0,
            54287.0, 54634.0, 54362.0, 54276.0, 54214.0, 54198.0, 54063.0,
            53982.0, 53845.0, 53577.0, 53534.0, 53380.0, 53510.0, 53462.0,
            53677.0, 53916.0, 54027.0, 54222.0, 54285.0, 54376.0, 54622.0,
            54744.0, 54709.0, 55018.0, 55107.0, 55459.0, 55514.0, 55561.0,
            55643.0, 55778.0, 55849.0, 55912.0, 55936.0, 55918.0, 55935.0,
            56055.0, 56153.0, 56454.0, 56513.0, 56563.0, 56688.0, 56823.0,
            56962.0, 57152.0, 57126.0, 57252.0, 57269.0, 57603.0, 57732.0,
            57784.0, 57975.0, 58121.0, 58311.0, 58510.0, 58798.0, 58691.0,
            59114.0, 59335.0, 59398.0, 59683.0, 59864.0, 60124.0, 60363.0,
            60595.0, 60860.0, 61085.0, 61367.0, 61578.0, 61882.0, 62230.0,
            62386.0, 62720.0, 63087.0, 63317.0, 63560.0, 63978.0, 64185.0,
            64344.0, 64433.0, 64655.0, 64854.0, 65076.0, 65215.0, 65208.0,
            65338.0, 65323.0, 65478.0, 65654.0, 65831.0, 65964.0, 66089.0,
            66111.0, 66591.0, 66776.0, 66606.0, 67029.0, 67132.0, 67417.0,
            67495.0, 67783.0, 68003.0, 68219.0, 68365.0, 68603.0, 68855.0,
            69161.0, 69272.0, 69542.0, 69791.0, 69948.0, 70180.0, 70498.0,
            70668.0, 70799.0, 70833.0, 70993.0, 70941.0, 71127.0, 71018.0,
            71165.0, 71347.0, 71251.0, 70993.0, 70905.0, 70969.0, 70789.0,
            70857.0, 70416.0, 70296.0, 70666.0, 70718.0, 70657.0, 70746.0,
            70936.0, 71129.0, 71163.0, 71219.0, 71220.0, 71527.0, 71532.0,
            71734.0, 71996.0, 72303.0, 72525.0, 72808.0, 73061.0, 73341.0,
            73643.0, 73636.0, 73929.0, 74115.0, 74527.0, 74881.0, 75235.0,
            75474.0, 75908.0, 76137.0, 76312.0, 76516.0, 76738.0, 76758.0,
            77018.0, 77164.0, 77502.0, 77833.0, 77992.0, 77953.0, 78177.0,
            78177.0, 78261.0, 78407.0, 78434.0, 78517.0, 78478.0, 78498.0,
            78569.0, 78238.0, 77565.0, 77145.0, 76742.0, 76419.0, 76298.0,
            76459.0, 76388.0, 76626.0, 76980.0, 77188.0, 77499.0, 77619.0,
            77915.0, 78326.0, 78606.0, 78819.0, 79134.0, 79192.0, 79258.0,
            79485.0, 79581.0, 79842.0, 79842.0, 80141.0, 80338.0, 80517.0,
            80794.0, 81221.0, 81610.0, 81977.0, 82381.0, 82760.0, 82974.0,
            83431.0, 83661.0, 84031.0, 84271.0, 84464.0, 84808.0, 85338.0,
            86083.0, 86404.0, 86811.0, 87037.0, 87324.0, 87434.0, 87797.0,
            88249.0, 88559.0, 88728.0, 88985.0, 89426.0, 89363.0, 89681.0,
            89955.0, 90019.0, 90159.0, 90149.0, 90360.0, 90466.0, 90617.0,
            90729.0, 90876.0, 90995.0, 90780.0, 90316.0, 89974.0, 89676.0,
            89964.0, 90046.0, 90334.0, 90550.0, 90774.0, 91003.0, 91095.0,
            91206.0, 91219.0, 91142.0, 91285.0, 91410.0, 91320.0, 91191.0,
            91216.0, 91014.0, 90831.0, 90448.0, 90474.0, 90337.0, 90031.0,
            89965.0, 89703.0, 89380.0, 89177.0, 88995.0, 88787.0, 88649.0,
            88675.0, 88826.0, 88758.0, 88946.0, 89211.0, 89497.0, 89886.0,
            90313.0, 89973.0, 91088.0, 91408.0, 91727.0, 92110.0, 92524.0,
            93043.0, 93312.0, 93650.0, 93952.0, 94325.0, 94647.0, 94885.0,
            95186.0, 95499.0, 95829.0, 95997.0, 96249.0, 96397.0, 96734.0,
            96896.0, 97163.0, 97280.0, 97465.0, 97696.0, 97878.0, 98098.0,
            98286.0, 98500.0, 98599.0, 98718.0, 98796.0, 98974.0, 99096.0,
            98973.0, 99276.0, 99435.0, 99747.0, 99980.0, 100145.0, 100394.0,
            100543.0, 100772.0, 101005.0, 101367.0, 101564.0, 101713.0,
            102047.0, 102266.0, 102430.0, 102980.0, 103200.0, 103544.0,
            103593.0, 104063.0, 104349.0, 104611.0, 104794.0, 105156.0,
            105397.0, 105549.0, 105789.0, 106070.0, 106400.0, 106703.0,
            107046.0, 107276.0, 107466.0, 107636.0, 107725.0, 107871.0,
            107939.0, 108026.0, 108200.0, 108266.0, 108588.0, 108695.0,
            108994.0, 109294.0, 109486.0, 109491.0, 109770.0, 109911.0,
            109698.0, 109561.0, 109487.0, 109284.0, 109115.0, 108983.0,
            108806.0, 108529.0, 108351.0, 108126.0, 108156.0, 108179.0,
            108106.0, 108209.0, 108237.0, 108211.0, 108129.0, 108123.0,
            108074.0, 108067.0, 108148.0, 108350.0, 108478.0, 108510.0,
            108651.0, 108708.0, 108802.0, 108994.0, 109109.0, 109280.0,
            109490.0, 109856.0, 109804.0, 110096.0, 110285.0, 110372.0,
            110628.0, 110714.0, 110923.0, 111112.0, 111366.0, 111610.0,
            111711.0, 111919.0, 112298.0, 112699.0, 112951.0, 113334.0,
            113624.0, 113914.0, 114186.0, 114348.0, 114882.0, 115113.0,
            115282.0
        ],
        'dcoinc': [
            43.8, 44.2, 44.6, 45.0, 45.3, 45.4, 45.3, 44.6, 44.5, 44.5, 44.8,
            45.8, 46.2, 46.1, 45.9, 46.0, 45.9, 45.8, 45.7, 45.7, 45.6, 45.5,
            45.3, 45.0, 45.0, 44.9, 45.1, 45.3, 45.6, 46.0, 46.1, 46.4, 46.5,
            46.8, 47.2, 47.4, 47.3, 47.6, 47.9, 48.1, 48.1, 48.2, 48.4, 48.5,
            48.5, 48.6, 48.8, 48.8, 48.8, 49.1, 49.2, 49.5, 49.6, 49.8, 49.9,
            50.0, 50.2, 50.5, 50.5, 50.8, 50.9, 51.2, 51.2, 51.6, 51.9, 52.0,
            52.3, 52.5, 52.8, 52.5, 53.2, 53.7, 53.8, 54.0, 54.4, 54.6, 54.9,
            55.2, 55.5, 55.7, 55.8, 56.2, 56.7, 57.0, 57.3, 57.5, 57.9, 58.0,
            58.3, 58.6, 58.8, 58.9, 59.0, 59.3, 59.4, 59.5, 59.8, 59.7, 59.7,
            59.8, 59.9, 60.0, 60.1, 60.5, 60.5, 60.6, 61.2, 61.7, 61.6, 61.9,
            62.1, 62.3, 62.6, 62.9, 63.1, 63.1, 63.4, 63.7, 64.0, 64.2, 64.3,
            64.6, 64.9, 65.0, 65.1, 65.4, 65.7, 65.9, 66.0, 66.2, 66.0, 66.1,
            65.7, 65.7, 65.7, 65.7, 65.6, 65.5, 65.6, 65.5, 65.5, 64.9, 64.6,
            65.3, 65.6, 65.6, 65.7, 65.9, 66.1, 66.3, 66.2, 66.2, 66.6, 66.8,
            67.2, 67.6, 68.2, 68.3, 68.8, 69.2, 69.4, 69.5, 69.8, 70.4, 70.8,
            71.5, 72.1, 72.6, 72.9, 73.2, 73.3, 73.3, 73.5, 73.8, 74.0, 74.1,
            74.3, 75.0, 75.4, 75.1, 74.7, 74.5, 74.4, 74.2, 74.5, 74.5, 74.6,
            74.4, 74.3, 74.2, 73.4, 72.2, 71.5, 71.0, 70.4, 70.6, 70.7, 70.9,
            71.2, 71.8, 72.2, 72.5, 72.6, 72.9, 73.6, 74.1, 74.3, 74.7, 74.9,
            75.0, 75.2, 75.4, 75.6, 75.5, 76.2, 76.7, 76.8, 77.2, 77.6, 78.0,
            78.4, 78.8, 79.2, 79.4, 79.8, 80.0, 80.3, 80.6, 80.4, 80.9, 81.6,
            82.7, 82.9, 83.4, 83.5, 83.9, 84.2, 84.6, 85.0, 85.3, 85.3, 85.5,
            86.2, 85.5, 86.1, 86.1, 86.1, 86.1, 86.1, 86.4, 86.4, 86.4, 86.8,
            86.7, 86.3, 85.5, 84.6, 84.2, 84.2, 84.7, 85.3, 86.0, 86.5, 86.8,
            86.8, 86.8, 86.9, 86.8, 86.7, 86.9, 87.3, 87.3, 87.1, 86.7, 86.3,
            85.9, 85.3, 85.8, 85.7, 85.7, 85.7, 85.2, 84.9, 84.6, 84.4, 84.0,
            84.0, 83.9, 84.4, 84.3, 84.6, 85.0, 85.6, 86.2, 86.8, 86.7, 87.7,
            88.4, 88.9, 89.5, 90.2, 90.7, 91.2, 91.5, 91.8, 92.4, 92.6, 92.8,
            93.2, 93.1, 93.5, 93.8, 93.8, 94.1, 94.5, 94.8, 94.9, 94.8, 94.8,
            95.2, 95.4, 95.4, 95.6, 96.1, 96.2, 96.3, 96.4, 97.1, 96.9, 96.8,
            97.1, 97.2, 97.8, 97.7, 97.9, 98.5, 98.1, 99.0, 99.1, 99.3, 99.5,
            99.7, 100.1, 100.3, 100.5, 101.3, 101.2, 102.0, 101.8, 102.3,
            102.7, 102.9, 103.0, 103.4, 103.5, 103.7, 103.8, 104.5, 104.6,
            105.3, 105.6, 105.6, 105.8, 106.0, 105.7, 105.6, 105.4, 105.8,
            105.6, 105.6, 106.1, 106.3, 106.2, 106.8, 107.2, 107.0, 107.2,
            107.3, 107.1, 107.0, 106.7, 106.3, 105.9, 105.9, 105.2, 104.9,
            104.8, 104.9, 105.1, 105.3, 105.4, 105.4, 105.6, 105.6, 105.4,
            105.4, 105.3, 105.8, 106.0, 106.2, 106.2, 106.3, 106.6, 106.5,
            106.9, 107.4, 107.7, 109.8, 107.9, 108.2, 108.3, 108.9, 109.0,
            109.2, 109.1, 109.8, 109.9, 110.3, 110.9, 111.5, 111.4, 112.1,
            112.7, 112.9, 113.2, 113.6, 113.7, 114.4, 114.7, 115.3, 115.8,
            116.4, 116.6
        ]
    },
    'start': 1,
    'start_params_std_data': [
        0.8808, 1 - 0.9526,
        0.5, 0.5, 0.5, 0.5, 0, 0,
        0.4242, -0.0303,
        0.4242, -0.0303, 0.4242, -0.0303, 0.4242, -0.0303, 0.4242, -0.0303,
        0.5 * 0.5, 0.5 * 0.5, 0.5 * 0.5, 0.5 * 0.5,
        -1.8, 0.5
    ],
    'start_params_nonstd_data': [
        0.8409, 1 - 0.9728,
        0.5584, 0.2151, 0.4468, 0.1223, 0.0114, 0.0110,
        0.3459, -0.0299,
        -0.0169, -0.0001, -0.3199, -0.0256, -0.3612, -0.0326, -0.1044, 0.2640,
        0.5082 * 0.5082, 0.5535 * 0.5535, 0.8029 * 0.8029, 0.1360 * 0.1360,
        -1.4986, 0.2619
    ],
    'optimized_params_std_data': [
        0.8341, 1 - 0.9720,
        0.6331, 0.3358, 0.4280, 0.5211, -0.0134, 0.1504,
        0.3528, -0.0311,
        -0.0583, -0.0008, -0.3111, -0.0242, -0.3722, -0.0346, 0.0196, 0.3216,
        0.5253 * 0.5253, 0.8571 * 0.8571, 0.7517 * 0.7517, 0.5755 * 0.5755,
        -1.4416, 0.2378
    ],
    'optimized_params_nonstd_data': [
        0.8341, 1 - 0.9720,
        0.5805, 0.2179, 0.4567, 0.1305, -0.0033, 0.0377,
        0.3528, -0.0311,
        -0.0583, -0.0008, -0.3111, -0.0242, -0.3722, -0.0346, 0.0196, 0.3216,
        0.4816 * 0.4816, 0.5561 * 0.5561, 0.8023 * 0.8023, 0.1442 * 0.1442,
        -1.4416, 0.2378
    ],
    'start_loglike_nonstd_data': -1292.7289,
    'optimized_loglike_std_data': -2072.8758,
    'optimized_loglike_nonstd_data': -1280.4266,
    'new_ind': [
        35.8229, 36.5043, 37.2901, 37.9409, 38.1651, 37.7844, 36.4131, 36.2202,
        36.0659, 36.5329, 38.3299, 39.0986, 39.078, 38.7569, 38.9053, 38.6027,
        38.132, 37.9224, 37.8115, 37.4627, 37.2364, 36.708, 35.9874, 35.839,
        35.6044, 35.9022, 36.3175, 36.8482, 37.5603, 37.8346, 38.3166, 38.428,
        38.9738, 39.6208, 39.9869, 39.8159, 40.4021, 40.7476, 41.1447, 41.1948,
        41.1515, 41.4448, 41.6709, 41.8265, 41.9909, 42.2202, 42.1784, 42.3023,
        42.7079, 42.9869, 43.4928, 43.787, 43.9258, 44.0872, 44.2988, 44.6226,
        45.0207, 45.062, 45.285, 45.5765, 46.0308, 46.2222, 46.6111, 47.0066,
        47.2514, 47.6495, 47.9931, 48.406, 47.9855, 48.8923, 49.6101, 49.8636,
        50.2251, 50.799, 51.1516, 51.5124, 51.8938, 52.4037, 52.6967, 52.9699,
        53.4219, 53.9138, 54.4871, 54.8723, 55.2473, 55.9171, 56.1134, 56.4641,
        56.9442, 57.2269, 57.3941, 57.6137, 57.9521, 58.0262, 58.2505, 58.5289,
        58.3346, 58.3295, 58.5342, 58.5203, 58.7303, 58.8431, 59.3594, 59.4329,
        59.571, 60.3255, 60.9339, 60.7402, 61.1182, 61.4035, 61.6353, 61.9854,
        62.3351, 62.6306, 62.7904, 62.9955, 63.298, 63.8137, 64.1059, 64.3,
        64.6052, 65.0144, 65.0992, 65.2373, 65.6508, 65.9337, 66.1291, 66.222,
        66.4231, 66.2046, 66.2327, 65.7209, 65.7199, 65.7745, 65.5903, 65.3175,
        65.1643, 65.2958, 65.1199, 64.9885, 64.1562, 63.6735, 64.5229, 64.9073,
        64.8395, 64.9547, 65.2511, 65.5634, 65.7532, 65.7304, 65.6516, 66.1978,
        66.3992, 66.7225, 67.255, 67.9794, 68.2569, 68.6828, 69.2532, 69.5001,
        69.7658, 69.8301, 70.4009, 70.8834, 71.5492, 72.2084, 72.7388, 73.0473,
        73.6054, 73.7509, 73.8293, 74.0676, 74.3501, 74.5549, 74.6759, 74.9485,
        75.414, 75.8291, 75.5856, 75.1534, 75.0598, 75.0843, 74.9924, 75.3077,
        75.375, 75.3558, 75.2021, 75.1827, 75.0111, 74.0573, 72.5062, 71.5399,
        70.8589, 70.0589, 70.142, 70.2498, 70.5167, 70.8397, 71.5341, 71.9475,
        72.283, 72.5467, 72.8959, 73.5813, 74.1805, 74.3455, 74.7121, 74.9857,
        75.0893, 75.3923, 75.5835, 75.8054, 75.8453, 76.4028, 76.9237, 77.1356,
        77.4842, 78.0219, 78.5706, 78.9532, 79.3899, 79.7049, 79.9046, 80.2981,
        80.5039, 80.7765, 80.9598, 80.9057, 81.2379, 81.9121, 83.0084, 83.334,
        83.7381, 83.9074, 84.2167, 84.4359, 84.8156, 85.316, 85.5912, 85.6038,
        85.8452, 86.3199, 85.9416, 86.3447, 86.496, 86.4198, 86.4239, 86.4459,
        86.6621, 86.6963, 86.7444, 86.9773, 87.0656, 86.9472, 86.167, 85.1716,
        84.5735, 84.3311, 84.828, 85.3224, 85.7956, 86.3435, 86.6165, 86.6647,
        86.742, 86.8209, 86.6906, 86.7055, 86.8823, 87.2188, 87.1478, 86.9306,
        86.6517, 86.1636, 85.683, 84.9381, 85.3872, 85.2725, 84.9676, 84.8374,
        84.4939, 84.0666, 83.7161, 83.3767, 82.9543, 82.7234, 82.5079, 83.0448,
        82.9807, 83.2828, 83.7353, 84.2218, 84.7398, 85.3923, 85.4199, 86.3741,
        87.003, 87.2631, 87.7225, 88.4057, 88.7791, 89.1747, 89.481, 89.8286,
        90.2244, 90.5083, 90.6608, 90.8474, 90.9025, 91.162, 91.2591, 91.4262,
        91.6587, 91.951, 92.1809, 92.378, 92.3554, 92.3678, 92.6569, 92.8946,
        92.8438, 93.0439, 93.3774, 93.5846, 93.5688, 93.5051, 93.8246, 93.8696,
        93.7234, 94.0004, 94.1792, 94.4897, 94.6934, 94.8733, 95.223, 95.1229,
        95.6225, 95.8796, 96.1585, 96.3633, 96.5804, 96.883, 97.0865, 97.1855,
        97.7695, 97.9113, 98.2702, 98.3276, 98.6481, 98.9499, 99.2158, 99.2751,
        99.5229, 99.7471, 99.9176, 99.9782, 100.3008, 100.5744, 100.9472,
        101.2309, 101.2318, 101.424, 101.5389, 101.4863, 101.4473, 101.2396,
        101.4784, 101.4723, 101.3977, 101.6553, 101.8871, 101.8878, 102.232,
        102.4711, 102.3266, 102.5615, 102.7089, 102.524, 102.4908, 102.3751,
        102.059, 101.5901, 101.3597, 100.9565, 100.5665, 100.2226, 100.2566,
        100.4647, 100.6817, 100.7377, 100.8262, 101.0385, 101.0291, 100.9574,
        100.8491, 100.8156, 101.0445, 101.2606, 101.4848, 101.6301, 101.6374,
        101.9124, 101.941, 102.0863, 102.3808, 102.6781, 103.2662, 103.1871,
        103.3822, 103.3759, 103.5632, 103.709, 103.8424, 103.9944, 104.2174,
        104.4312, 104.6232, 104.9945, 105.383, 105.4255, 105.7702, 106.2356,
        106.4784, 106.7056, 107.0036, 107.1589, 107.5621, 107.7468, 108.0039,
        108.4249, 108.8569, 109.006
    ],
    'prtt0': [
        0.0021, 0.0016, 0.0008, 0.0025, 0.0278, 0.4171, 0.9993, 0.428, 0.4945,
        0.0214, 0.0, 0.01, 0.1411, 0.4692, 0.1025, 0.4986, 0.8274, 0.6861,
        0.5841, 0.8197, 0.7389, 0.9515, 0.986, 0.6413, 0.7509, 0.1195, 0.0177,
        0.0034, 0.0011, 0.0218, 0.0033, 0.0435, 0.002, 0.0017, 0.0116, 0.1814,
        0.0023, 0.0136, 0.0062, 0.0542, 0.0938, 0.0138, 0.0186, 0.0255, 0.0234,
        0.0153, 0.0818, 0.0383, 0.0056, 0.0138, 0.0027, 0.0137, 0.0291, 0.0237,
        0.0172, 0.0084, 0.0058, 0.0554, 0.0169, 0.0107, 0.0039, 0.0251, 0.0056,
        0.0065, 0.0165, 0.0053, 0.0088, 0.0054, 0.4658, 0.0004, 0.0027, 0.025,
        0.0072, 0.002, 0.0114, 0.0075, 0.0066, 0.003, 0.0144, 0.0124, 0.0038,
        0.0039, 0.0024, 0.0088, 0.0071, 0.0011, 0.035, 0.0079, 0.0037, 0.0145,
        0.0242, 0.0163, 0.0077, 0.045, 0.0165, 0.0116, 0.1864, 0.1055, 0.0314,
        0.0882, 0.0231, 0.037, 0.0024, 0.0575, 0.034, 0.0005, 0.0034, 0.2417,
        0.0107, 0.0149, 0.0165, 0.0071, 0.0082, 0.0117, 0.0248, 0.0178, 0.0097,
        0.0026, 0.0147, 0.0205, 0.0094, 0.0052, 0.0457, 0.0291, 0.0048, 0.0137,
        0.0202, 0.0364, 0.0185, 0.2084, 0.0978, 0.7123, 0.2956, 0.205, 0.4207,
        0.5949, 0.5246, 0.1852, 0.4213, 0.4132, 0.9843, 0.9003, 0.0044, 0.0201,
        0.1244, 0.0477, 0.0129, 0.0103, 0.0213, 0.0735, 0.1221, 0.0029, 0.0291,
        0.0091, 0.0024, 0.0009, 0.0226, 0.0049, 0.0021, 0.022, 0.013, 0.0449,
        0.0016, 0.0052, 0.0013, 0.0019, 0.0043, 0.0138, 0.002, 0.0386, 0.045,
        0.0163, 0.0116, 0.0188, 0.0305, 0.0116, 0.0037, 0.0067, 0.254, 0.6508,
        0.4272, 0.2793, 0.3598, 0.0435, 0.0813, 0.107, 0.2369, 0.1805, 0.3578,
        0.9907, 0.9998, 0.9811, 0.9676, 0.9899, 0.3241, 0.189, 0.0416, 0.0136,
        0.0009, 0.0096, 0.0097, 0.0133, 0.0073, 0.001, 0.0029, 0.0373, 0.0068,
        0.0131, 0.0352, 0.0095, 0.0209, 0.016, 0.0503, 0.0018, 0.0042, 0.024,
        0.0074, 0.0024, 0.003, 0.0086, 0.0048, 0.0115, 0.0201, 0.0053, 0.0213,
        0.0118, 0.0219, 0.088, 0.0092, 0.0011, 0.0001, 0.0358, 0.0069, 0.0271,
        0.0093, 0.0175, 0.0058, 0.0032, 0.0161, 0.0626, 0.0164, 0.0035, 0.4218,
        0.013, 0.0395, 0.1163, 0.0928, 0.0897, 0.0267, 0.0637, 0.0597, 0.0181,
        0.0409, 0.1444, 0.9347, 0.9954, 0.9397, 0.7774, 0.0429, 0.0078, 0.005,
        0.0026, 0.0173, 0.0514, 0.0448, 0.0465, 0.154, 0.1058, 0.0399, 0.0102,
        0.1069, 0.2774, 0.5077, 0.8676, 0.9243, 0.9886, 0.0573, 0.3154, 0.5844,
        0.4616, 0.7662, 0.8905, 0.8717, 0.8764, 0.924, 0.7934, 0.7652, 0.0313,
        0.212, 0.0227, 0.005, 0.004, 0.0035, 0.0014, 0.0808, 0.0001, 0.0048,
        0.0219, 0.0038, 0.0011, 0.0126, 0.0062, 0.0112, 0.0077, 0.006, 0.0128,
        0.026, 0.02, 0.0462, 0.013, 0.0379, 0.0237, 0.0155, 0.0106, 0.0161,
        0.019, 0.0727, 0.0715, 0.0131, 0.0165, 0.0882, 0.0229, 0.0087, 0.0197,
        0.0709, 0.1069, 0.0112, 0.0566, 0.1689, 0.0198, 0.0273, 0.0097, 0.0192,
        0.021, 0.007, 0.1178, 0.0032, 0.0194, 0.012, 0.0186, 0.0165, 0.0097,
        0.0191, 0.035, 0.0015, 0.0424, 0.0073, 0.0501, 0.0088, 0.0112, 0.0128,
        0.0456, 0.0139, 0.0165, 0.0225, 0.0444, 0.0085, 0.0128, 0.0064, 0.0126,
        0.0653, 0.0229, 0.0348, 0.0937, 0.1109, 0.2947, 0.0378, 0.1014, 0.1529,
        0.0244, 0.0196, 0.0689, 0.0081, 0.0171, 0.1463, 0.0227, 0.0319, 0.1932,
        0.1424, 0.244, 0.5748, 0.8641, 0.7359, 0.9007, 0.9026, 0.8817, 0.4636,
        0.1504, 0.0606, 0.0798, 0.0565, 0.0228, 0.0755, 0.1246, 0.1921, 0.1618,
        0.0338, 0.0231, 0.0176, 0.027, 0.0622, 0.0128, 0.0584, 0.0294, 0.0107,
        0.0106, 0.0018, 0.1329, 0.0297, 0.0817, 0.0257, 0.0296, 0.0296, 0.0263,
        0.0163, 0.0173, 0.0196, 0.006, 0.0065, 0.0554, 0.0077, 0.004, 0.0186,
        0.0161, 0.01, 0.0257, 0.0049, 0.025, 0.0131, 0.0046, 0.0052, 0.0307
    ],
    'smooth0': [
        0.0004, 0.0003, 0.001, 0.032, 0.4481, 0.9541, 0.9969, 0.2288, 0.1409,
        0.0036, 0.0, 0.1506, 0.6284, 0.744, 0.7368, 0.9442, 0.9706, 0.959,
        0.9634, 0.9822, 0.9811, 0.9886, 0.9588, 0.488, 0.3462, 0.0242, 0.0031,
        0.0006, 0.0003, 0.0041, 0.0013, 0.0076, 0.0004, 0.0008, 0.0113, 0.0355,
        0.0006, 0.0036, 0.0045, 0.0217, 0.0197, 0.0041, 0.0059, 0.0076, 0.0076,
        0.0088, 0.0195, 0.0072, 0.0013, 0.0025, 0.0008, 0.0045, 0.0075, 0.0055,
        0.0035, 0.0019, 0.0027, 0.0117, 0.0035, 0.002, 0.0011, 0.0047, 0.0012,
        0.0016, 0.0033, 0.0016, 0.0054, 0.0186, 0.1249, 0.0001, 0.0008, 0.0047,
        0.0013, 0.0004, 0.0023, 0.0015, 0.0012, 0.0008, 0.0031, 0.0022, 0.0007,
        0.0007, 0.0005, 0.0017, 0.0012, 0.0004, 0.0067, 0.0015, 0.001, 0.0042,
        0.0057, 0.004, 0.0037, 0.0129, 0.0089, 0.0183, 0.062, 0.0307, 0.0154,
        0.0205, 0.0068, 0.007, 0.0014, 0.0136, 0.0058, 0.0002, 0.0055, 0.0518,
        0.0026, 0.0035, 0.0033, 0.0015, 0.002, 0.0034, 0.0057, 0.0036, 0.0018,
        0.0007, 0.0038, 0.0042, 0.0021, 0.0024, 0.0115, 0.0063, 0.0029, 0.0164,
        0.0475, 0.1145, 0.2034, 0.5478, 0.5964, 0.769, 0.6879, 0.7217, 0.8032,
        0.8189, 0.7967, 0.7864, 0.894, 0.9254, 0.964, 0.5972, 0.0025, 0.0152,
        0.0309, 0.0103, 0.0036, 0.0053, 0.015, 0.0288, 0.023, 0.0009, 0.0056,
        0.0016, 0.0004, 0.0003, 0.0041, 0.0009, 0.0007, 0.0055, 0.0044, 0.0078,
        0.0003, 0.0009, 0.0002, 0.0004, 0.001, 0.0026, 0.001, 0.012, 0.0099,
        0.0039, 0.0035, 0.0058, 0.0073, 0.0044, 0.0095, 0.0876, 0.5041, 0.5246,
        0.3883, 0.329, 0.312, 0.2645, 0.4454, 0.6004, 0.7564, 0.8259, 0.9449,
        0.9997, 1.0, 0.9983, 0.9901, 0.948, 0.1105, 0.0438, 0.0084, 0.0023,
        0.0002, 0.0021, 0.0022, 0.0026, 0.0012, 0.0002, 0.0011, 0.0073, 0.0019,
        0.0043, 0.0074, 0.0029, 0.0057, 0.0056, 0.0088, 0.0004, 0.0012, 0.0046,
        0.0013, 0.0004, 0.0006, 0.0017, 0.0012, 0.0029, 0.004, 0.0016, 0.0054,
        0.0047, 0.0104, 0.0167, 0.0016, 0.0002, 0.0, 0.0072, 0.0021, 0.0056,
        0.0023, 0.0033, 0.0011, 0.0012, 0.0078, 0.0154, 0.0069, 0.0123, 0.1186,
        0.0141, 0.0417, 0.0634, 0.0541, 0.0506, 0.0472, 0.094, 0.1244, 0.1749,
        0.477, 0.8362, 0.9963, 0.995, 0.8213, 0.3687, 0.0081, 0.0015, 0.001,
        0.0017, 0.0167, 0.0395, 0.0497, 0.0721, 0.1082, 0.0966, 0.1013, 0.1702,
        0.7017, 0.8885, 0.9608, 0.9894, 0.988, 0.9718, 0.5741, 0.8775, 0.9342,
        0.9452, 0.978, 0.981, 0.9645, 0.9345, 0.867, 0.6072, 0.3656, 0.0279,
        0.0457, 0.0042, 0.0009, 0.0007, 0.0007, 0.0008, 0.0142, 0.0, 0.0013,
        0.0039, 0.0007, 0.0003, 0.0025, 0.0014, 0.0023, 0.0016, 0.0017, 0.0043,
        0.0077, 0.0074, 0.0107, 0.0048, 0.0094, 0.0055, 0.0037, 0.0034, 0.0066,
        0.0119, 0.0237, 0.0158, 0.0049, 0.0095, 0.0198, 0.0058, 0.0046, 0.0151,
        0.0311, 0.0272, 0.0104, 0.0333, 0.0368, 0.0055, 0.0059, 0.0028, 0.0053,
        0.0055, 0.0051, 0.0221, 0.0009, 0.0044, 0.0032, 0.0044, 0.0037, 0.0029,
        0.0055, 0.0062, 0.0006, 0.0089, 0.003, 0.0098, 0.0021, 0.0031, 0.005,
        0.01, 0.0039, 0.0052, 0.0073, 0.0087, 0.002, 0.0029, 0.0027, 0.0098,
        0.0277, 0.0244, 0.0496, 0.0839, 0.0903, 0.0907, 0.031, 0.0454, 0.0359,
        0.0088, 0.0109, 0.02, 0.0118, 0.0521, 0.1422, 0.1411, 0.3312, 0.6529,
        0.7295, 0.8673, 0.9516, 0.9685, 0.9454, 0.9362, 0.8489, 0.6255, 0.1707,
        0.0557, 0.0337, 0.0355, 0.0302, 0.032, 0.0679, 0.0779, 0.0696, 0.0382,
        0.0092, 0.0068, 0.0069, 0.0113, 0.0148, 0.0061, 0.014, 0.0061, 0.0023,
        0.0023, 0.0021, 0.0363, 0.0147, 0.0203, 0.0082, 0.0088, 0.008, 0.0063,
        0.0041, 0.0042, 0.0038, 0.0014, 0.0028, 0.0104, 0.0015, 0.0011, 0.0044,
        0.0036, 0.0028, 0.005, 0.0015, 0.0053, 0.0026, 0.0014, 0.0051, 0.0307
    ]
}

class OptionWrapper(object):
    def __init__(self, mask_attribute, mask_value):
        # Name of the class-level bitmask attribute
        self.mask_attribute = mask_attribute
        # Value of this option
        self.mask_value = mask_value

    def __get__(self, obj, objtype):
        # Return True / False based on whether the bit is set in the bitmask
        return bool(getattr(obj, self.mask_attribute, 0) & self.mask_value)

    def __set__(self, obj, value):
        mask_attribute_value = getattr(obj, self.mask_attribute, 0)
        if bool(value):
            value = mask_attribute_value | self.mask_value
        else:
            value = mask_attribute_value & ~self.mask_value
        setattr(obj, self.mask_attribute, value)


class MatrixWrapper(object):
    def __init__(self, name, attribute):
        self.name = name
        self.attribute = attribute
        self._attribute = '_' + attribute

    def __get__(self, obj, objtype):
        matrix = getattr(obj, self._attribute, None)
        # # Remove last dimension if the array is not actually time-varying
        # if matrix is not None and matrix.shape[-1] == 1:
        #     return np.squeeze(matrix, -1)
        return matrix

    def __set__(self, obj, value):
        value = np.asarray(value, order="F")
        shape = obj.shapes[self.attribute]

        if len(shape) == 3:
            value = self._set_matrix(obj, value, shape)
        else:
            value = self._set_vector(obj, value, shape)

        setattr(obj, self._attribute, value)

    def _set_matrix(self, obj, value, shape):
        # Expand 1-dimensional array if possible
        if (value.ndim == 1 and shape[0] == 1
                and value.shape[0] == shape[1]):
            value = value[None, :]

        # Enforce that the matrix is appropriate size
        validate_matrix_shape(
            self.name, value.shape, shape[0], shape[1], obj.nobs
        )

        # Expand time-invariant matrix
        if value.ndim == 2:
            value = np.array(value[:, :, None], order="F")

        return value

    def _set_vector(self, obj, value, shape):
        # Enforce that the vector has appropriate length
        validate_vector_shape(
            self.name, value.shape, shape[0], obj.nobs
        )

        # Expand the time-invariant vector
        if value.ndim == 1:
            value = np.array(value[:, None], order="F")

        return value
class Representation(object):
    r"""
    State space representation of a time series process
    Parameters
    ----------
    k_endog : array_like or integer
        The observed time-series process :math:`y` if array like or the
        number of variables in the process if an integer.
    k_states : int
        The dimension of the unobserved state process.
    k_posdef : int, optional
        The dimension of a guaranteed positive definite covariance matrix
        describing the shocks in the measurement equation. Must be less than
        or equal to `k_states`. Default is `k_states`.
    initial_variance : float, optional
        Initial variance used when approximate diffuse initialization is
        specified. Default is 1e6.
    initialization : {'approximate_diffuse','stationary','known'}, optional
        Initialization method for the initial state.
    initial_state : array_like, optional
        If known initialization is used, the mean of the initial state's
        distribution.
    initial_state_cov : array_like, optional
        If known initialization is used, the covariance matrix of the initial
        state's distribution.
    nobs : integer, optional
        If an endogenous vector is not given (i.e. `k_endog` is an integer),
        the number of observations can optionally be specified. If not
        specified, they will be set to zero until data is bound to the model.
    dtype : dtype, optional
        If an endogenous vector is not given (i.e. `k_endog` is an integer),
        the default datatype of the state space matrices can optionally be
        specified. Default is `np.float64`.
    design : array_like, optional
        The design matrix, :math:`Z`. Default is set to zeros.
    obs_intercept : array_like, optional
        The intercept for the observation equation, :math:`d`. Default is set
        to zeros.
    obs_cov : array_like, optional
        The covariance matrix for the observation equation :math:`H`. Default
        is set to zeros.
    transition : array_like, optional
        The transition matrix, :math:`T`. Default is set to zeros.
    state_intercept : array_like, optional
        The intercept for the transition equation, :math:`c`. Default is set to
        zeros.
    selection : array_like, optional
        The selection matrix, :math:`R`. Default is set to zeros.
    state_cov : array_like, optional
        The covariance matrix for the state equation :math:`Q`. Default is set
        to zeros.
    **kwargs
        Additional keyword arguments. Not used directly. It is present to
        improve compatibility with subclasses, so that they can use `**kwargs`
        to specify any default state space matrices (e.g. `design`) without
        having to clean out any other keyword arguments they might have been
        passed.
    Attributes
    ----------
    nobs : int
        The number of observations.
    k_endog : int
        The dimension of the observation series.
    k_states : int
        The dimension of the unobserved state process.
    k_posdef : int
        The dimension of a guaranteed positive
        definite covariance matrix describing
        the shocks in the measurement equation.
    shapes : dictionary of name:tuple
        A dictionary recording the initial shapes
        of each of the representation matrices as
        tuples.
    initialization : str
        Kalman filter initialization method. Default is unset.
    initial_variance : float
        Initial variance for approximate diffuse
        initialization. Default is 1e6.
    Notes
    -----
    A general state space model is of the form
    .. math::
        y_t & = Z_t \alpha_t + d_t + \varepsilon_t \\
        \alpha_t & = T_t \alpha_{t-1} + c_t + R_t \eta_t \\
    where :math:`y_t` refers to the observation vector at time :math:`t`,
    :math:`\alpha_t` refers to the (unobserved) state vector at time
    :math:`t`, and where the irregular components are defined as
    .. math::
        \varepsilon_t \sim N(0, H_t) \\
        \eta_t \sim N(0, Q_t) \\
    The remaining variables (:math:`Z_t, d_t, H_t, T_t, c_t, R_t, Q_t`) in the
    equations are matrices describing the process. Their variable names and
    dimensions are as follows
    Z : `design`          :math:`(k\_endog \times k\_states \times nobs)`
    d : `obs_intercept`   :math:`(k\_endog \times nobs)`
    H : `obs_cov`         :math:`(k\_endog \times k\_endog \times nobs)`
    T : `transition`      :math:`(k\_states \times k\_states \times nobs)`
    c : `state_intercept` :math:`(k\_states \times nobs)`
    R : `selection`       :math:`(k\_states \times k\_posdef \times nobs)`
    Q : `state_cov`       :math:`(k\_posdef \times k\_posdef \times nobs)`
    In the case that one of the matrices is time-invariant (so that, for
    example, :math:`Z_t = Z_{t+1} ~ \forall ~ t`), its last dimension may
    be of size :math:`1` rather than size `nobs`.
    References
    ----------
    .. [1] Durbin, James, and Siem Jan Koopman. 2012.
       Time Series Analysis by State Space Methods: Second Edition.
       Oxford University Press.
    """

    endog = None
    r"""
    (array) The observation vector, alias for `obs`.
    """
    design = MatrixWrapper('design', 'design')
    r"""
    (array) Design matrix: :math:`Z~(k\_endog \times k\_states \times nobs)`
    """
    obs_intercept = MatrixWrapper('observation intercept', 'obs_intercept')
    r"""
    (array) Observation intercept: :math:`d~(k\_endog \times nobs)`
    """
    obs_cov = MatrixWrapper('observation covariance matrix', 'obs_cov')
    r"""
    (array) Observation covariance matrix:
    :math:`H~(k\_endog \times k\_endog \times nobs)`
    """
    transition = MatrixWrapper('transition', 'transition')
    r"""
    (array) Transition matrix: :math:`T~(k\_states \times k\_states \times nobs)`
    """
    state_intercept = MatrixWrapper('state intercept', 'state_intercept')
    r"""
    (array) State intercept: :math:`c~(k\_states \times nobs)`
    """
    selection = MatrixWrapper('selection', 'selection')
    r"""
    (array) Selection matrix: :math:`R~(k\_states \times k\_posdef \times nobs)`
    """
    state_cov = MatrixWrapper('state covariance matrix', 'state_cov')
    r"""
    (array) State covariance matrix: :math:`Q~(k\_posdef \times k\_posdef \times nobs)`
    """

    def __init__(self, k_endog, k_states, k_posdef=None,
                 initial_variance=1e6, nobs=0, dtype=np.float64,
                 design=None, obs_intercept=None, obs_cov=None,
                 transition=None, state_intercept=None, selection=None,
                 state_cov=None, statespace_classes=None, **kwargs):
        self.shapes = {}

        # Check if k_endog is actually the endog array
        endog = None
        if isinstance(k_endog, np.ndarray):
            endog = k_endog
            # If so, assume that it is either column-ordered and in wide format
            # or row-ordered and in long format
            if endog.flags['C_CONTIGUOUS'] and (endog.shape[0] > 1 or nobs == 1):
                endog = endog.T
            k_endog = endog.shape[0]

        # Endogenous array, dimensions, dtype
        self.k_endog = k_endog
        if k_endog < 1:
            raise ValueError('Number of endogenous variables in statespace'
                             ' model must be a positive number.')
        self.nobs = nobs

        # Get dimensions from transition equation
        if k_states < 1:
            raise ValueError('Number of states in statespace model must be a'
                             ' positive number.')
        self.k_states = k_states
        self.k_posdef = k_posdef if k_posdef is not None else k_states

        # Bind endog, if it was given
        if endog is not None:
            self.bind(endog)

        # Record the shapes of all of our matrices
        # Note: these are time-invariant shapes; in practice the last dimension
        # may also be `self.nobs` for any or all of these.
        self.shapes = {
            'obs': (self.k_endog, self.nobs),
            'design': (self.k_endog, self.k_states, 1),
            'obs_intercept': (self.k_endog, 1),
            'obs_cov': (self.k_endog, self.k_endog, 1),
            'transition': (self.k_states, self.k_states, 1),
            'state_intercept': (self.k_states, 1),
            'selection': (self.k_states, self.k_posdef, 1),
            'state_cov': (self.k_posdef, self.k_posdef, 1),
        }

        # Representation matrices
        # These matrices are only used in the Python object as containers,
        # which will be copied to the appropriate _statespace object if a
        # filter is called.
        scope = locals()
        for name, shape in self.shapes.items():
            if name == 'obs':
                continue
            # Create the initial storage array for each matrix
            setattr(self, '_' + name, np.zeros(shape, dtype=dtype, order="F"))

            # If we were given an initial value for the matrix, set it
            # (notice it is being set via the descriptor)
            if scope[name] is not None:
                setattr(self, name, scope[name])

        # Options
        self._compatibility_mode = tools.compatibility_mode
        self.initial_variance = initial_variance
        self.prefix_statespace_map = (statespace_classes
                                      if statespace_classes is not None
                                      else tools.prefix_statespace_map.copy())

        # State-space initialization data
        self.initialization = kwargs.get('initialization', None)
        self._initial_state = None
        self._initial_state_cov = None
        self._initial_variance = None

        if self.initialization == 'approximate_diffuse':
            self.initialize_approximate_diffuse()
        elif self.initialization == 'stationary':
            self.initialize_stationary()
        elif self.initialization == 'known':
            if not 'initial_state' in kwargs:
                raise ValueError('Initial state must be provided when "known"'
                                 ' is the specified initialization method.')
            if not 'initial_state_cov' in kwargs:
                raise ValueError('Initial state covariance matrix must be'
                                 ' provided when "known" is the specified'
                                 ' initialization method.')
            self.initialize_known(kwargs['initial_state'],
                                  kwargs['initial_state_cov'])
        elif self.initialization is not None:
            raise ValueError("Invalid state space initialization method.")

        # Matrix representations storage
        self._representations = {}

        # Setup the underlying statespace object storage
        self._statespaces = {}

    def __getitem__(self, key):
        _type = type(key)
        # If only a string is given then we must be getting an entire matrix
        if _type is str:
            if key not in self.shapes:
                raise IndexError('"%s" is an invalid state space matrix name'
                                 % key)
            matrix = getattr(self, '_' + key)

            # See note on time-varying arrays, below
            if matrix.shape[-1] == 1:
                return matrix[[slice(None)]*(matrix.ndim-1) + [0]]
            else:
                return matrix
        # Otherwise if we have a tuple, we want a slice of a matrix
        elif _type is tuple:
            name, slice_ = key[0], key[1:]
            if name not in self.shapes:
                raise IndexError('"%s" is an invalid state space matrix name'
                                 % name)

            matrix = getattr(self, '_' + name)

            # Since the model can support time-varying arrays, but often we
            # will instead have time-invariant arrays, we want to allow setting
            # a matrix slice like mod['transition',0,:] even though technically
            # it should be mod['transition',0,:,0]. Thus if the array in
            # question is time-invariant but the last slice was excluded,
            # add it in as a zero.
            if matrix.shape[-1] == 1 and len(slice_) <= matrix.ndim-1:
                slice_ = slice_ + (0,)

            return matrix[slice_]
        # Otherwise, we have only a single slice index, but it is not a string
        else:
            raise IndexError('First index must the name of a valid state space'
                             ' matrix.')

    def __setitem__(self, key, value):
        _type = type(key)
        # If only a string is given then we must be setting an entire matrix
        if _type is str:
            if key not in self.shapes:
                raise IndexError('"%s" is an invalid state space matrix name'
                                 % key)
            setattr(self, key, value)
        # If it's a tuple (with a string as the first element) then we must be
        # setting a slice of a matrix
        elif _type is tuple:
            name, slice_ = key[0], key[1:]
            if name not in self.shapes:
                raise IndexError('"%s" is an invalid state space matrix name'
                                 % key[0])

            # Change the dtype of the corresponding matrix
            dtype = np.array(value).dtype
            matrix = getattr(self, '_' + name)
            valid_types = ['f', 'd', 'F', 'D']
            if not matrix.dtype == dtype and dtype.char in valid_types:
                matrix = getattr(self, '_' + name).real.astype(dtype)

            # Since the model can support time-varying arrays, but often we
            # will instead have time-invariant arrays, we want to allow setting
            # a matrix slice like mod['transition',0,:] even though technically
            # it should be mod['transition',0,:,0]. Thus if the array in
            # question is time-invariant but the last slice was excluded,
            # add it in as a zero.
            if matrix.shape[-1] == 1 and len(slice_) == matrix.ndim-1:
                slice_ = slice_ + (0,)

            # Set the new value
            matrix[slice_] = value
            setattr(self, name, matrix)
        # Otherwise we got a single non-string key, (e.g. mod[:]), which is
        # invalid
        else:
            raise IndexError('First index must the name of a valid state space'
                             ' matrix.')

    @property
    def prefix(self):
        """
        (str) BLAS prefix of currently active representation matrices
        """
        arrays = (
            self._design, self._obs_intercept, self._obs_cov,
            self._transition, self._state_intercept, self._selection,
            self._state_cov
        )
        if self.endog is not None:
            arrays = (self.endog,) + arrays
        return find_best_blas_type(arrays)[0]

    @property
    def dtype(self):
        """
        (dtype) Datatype of currently active representation matrices
        """
        return tools.prefix_dtype_map[self.prefix]

    @property
    def time_invariant(self):
        """
        (bool) Whether or not currently active representation matrices are
        time-invariant
        """
        return (
            self._design.shape[2] == self._obs_intercept.shape[1] ==
            self._obs_cov.shape[2] == self._transition.shape[2] ==
            self._state_intercept.shape[1] == self._selection.shape[2] ==
            self._state_cov.shape[2]
        )

    @property
    def _statespace(self):
        prefix = self.prefix
        if prefix in self._statespaces:
            return self._statespaces[prefix]
        return None

    @property
    def obs(self):
        r"""
        (array) Observation vector: :math:`y~(k\_endog \times nobs)`
        """
        return self.endog

    def bind(self, endog):
        """
        Bind data to the statespace representation
        Parameters
        ----------
        endog : array
            Endogenous data to bind to the model. Must be column-ordered
            ndarray with shape (`k_endog`, `nobs`) or row-ordered ndarray with
            shape (`nobs`, `k_endog`).
        Notes
        -----
        The strict requirements arise because the underlying statespace and
        Kalman filtering classes require Fortran-ordered arrays in the wide
        format (shaped (`k_endog`, `nobs`)), and this structure is setup to
        prevent copying arrays in memory.
        By default, numpy arrays are row (C)-ordered and most time series are
        represented in the long format (with time on the 0-th axis). In this
        case, no copying or re-ordering needs to be performed, instead the
        array can simply be transposed to get it in the right order and shape.
        Although this class (Representation) has stringent `bind` requirements,
        it is assumed that it will rarely be used directly.
        """
        if not isinstance(endog, np.ndarray):
            raise ValueError("Invalid endogenous array; must be an ndarray.")

        # Make sure we have a 2-dimensional array
        # Note: reshaping a 1-dim array into a 2-dim array by changing the
        #       shape tuple always results in a row (C)-ordered array, so it
        #       must be shaped (nobs, k_endog)
        if endog.ndim == 1:
            # In the case of nobs x 0 arrays
            if self.k_endog == 1:
                endog.shape = (endog.shape[0], 1)
            # In the case of k_endog x 0 arrays
            else:
                endog.shape = (1, endog.shape[0])
        if not endog.ndim == 2:
            raise ValueError('Invalid endogenous array provided; must be'
                             ' 2-dimensional.')

        # Check for valid column-ordered arrays
        if endog.flags['F_CONTIGUOUS'] and endog.shape[0] == self.k_endog:
            pass
        # Check for valid row-ordered arrays, and transpose them to be the
        # correct column-ordered array
        elif endog.flags['C_CONTIGUOUS'] and endog.shape[1] == self.k_endog:
            endog = endog.T
        # Invalid column-ordered arrays
        elif endog.flags['F_CONTIGUOUS']:
            raise ValueError('Invalid endogenous array; column-ordered'
                             ' arrays must have first axis shape of'
                             ' `k_endog`.')
        # Invalid row-ordered arrays
        elif endog.flags['C_CONTIGUOUS']:
            raise ValueError('Invalid endogenous array; row-ordered'
                             ' arrays must have last axis shape of'
                             ' `k_endog`.')
        # Non-contiguous arrays
        else:
            raise ValueError('Invalid endogenous array; must be ordered in'
                             ' contiguous memory.')

        # In some corner cases (e.g. np.array(1., ndmin=2) with numpy < 1.8)
        # we may still have a non-fortran contiguous array, so double-check
        # that now
        if not endog.flags['F_CONTIGUOUS']:
            endog = np.asfortranarray(endog)

        # Set a flag for complex data
        self._complex_endog = np.iscomplexobj(endog)

        # Set the data
        self.endog = endog
        self.nobs = self.endog.shape[1]

        # Reset shapes
        if hasattr(self, 'shapes'):
            self.shapes['obs'] = self.endog.shape

    def initialize_known(self, initial_state, initial_state_cov):
        """
        Initialize the statespace model with known distribution for initial
        state.
        These values are assumed to be known with certainty or else
        filled with parameters during, for example, maximum likelihood
        estimation.
        Parameters
        ----------
        initial_state : array_like
            Known mean of the initial state vector.
        initial_state_cov : array_like
            Known covariance matrix of the initial state vector.
        """
        initial_state = np.asarray(initial_state, order="F")
        initial_state_cov = np.asarray(initial_state_cov, order="F")

        if not initial_state.shape == (self.k_states,):
            raise ValueError('Invalid dimensions for initial state vector.'
                             ' Requires shape (%d,), got %s' %
                             (self.k_states, str(initial_state.shape)))
        if not initial_state_cov.shape == (self.k_states, self.k_states):
            raise ValueError('Invalid dimensions for initial covariance'
                             ' matrix. Requires shape (%d,%d), got %s' %
                             (self.k_states, self.k_states,
                              str(initial_state.shape)))

        self._initial_state = initial_state
        self._initial_state_cov = initial_state_cov
        self.initialization = 'known'

    def initialize_approximate_diffuse(self, variance=None):
        """
        Initialize the statespace model with approximate diffuse values.
        Rather than following the exact diffuse treatment (which is developed
        for the case that the variance becomes infinitely large), this assigns
        an arbitrary large number for the variance.
        Parameters
        ----------
        variance : float, optional
            The variance for approximating diffuse initial conditions. Default
            is 1e6.
        """
        if variance is None:
            variance = self.initial_variance

        self._initial_variance = variance
        self.initialization = 'approximate_diffuse'

    def initialize_stationary(self):
        """
        Initialize the statespace model as stationary.
        """
        self.initialization = 'stationary'

    def _initialize_representation(self, prefix=None):
        if prefix is None:
            prefix = self.prefix
        dtype = tools.prefix_dtype_map[prefix]

        # If the dtype-specific representation matrices do not exist, create
        # them
        if prefix not in self._representations:
            # Copy the statespace representation matrices
            self._representations[prefix] = {}
            for matrix in self.shapes.keys():
                if matrix == 'obs':
                    self._representations[prefix][matrix] = (
                        self.obs.astype(dtype)
                    )
                else:
                    # Note: this always makes a copy
                    self._representations[prefix][matrix] = (
                        getattr(self, '_' + matrix).astype(dtype)
                    )
        # If they do exist, update them
        else:
            for matrix in self.shapes.keys():
                existing = self._representations[prefix][matrix]
                if matrix == 'obs':
                    existing = self.obs.astype(dtype)[:]
                else:
                    new = getattr(self, '_' + matrix).astype(dtype)
                    if existing.shape == new.shape:
                        existing[:] = new[:]
                    else:
                        existing = new


        # Determine if we need to (re-)create the _statespace models
        # (if time-varying matrices changed)
        if prefix in self._statespaces:
            ss = self._statespaces[prefix]
            create = (
                not ss.obs.shape[1] == self.endog.shape[1] or
                not ss.design.shape[2] == self.design.shape[2] or
                not ss.obs_intercept.shape[1] == self.obs_intercept.shape[1] or
                not ss.obs_cov.shape[2] == self.obs_cov.shape[2] or
                not ss.transition.shape[2] == self.transition.shape[2] or
                not (ss.state_intercept.shape[1] ==
                     self.state_intercept.shape[1]) or
                not ss.selection.shape[2] == self.selection.shape[2] or
                not ss.state_cov.shape[2] == self.state_cov.shape[2]
            )
        else:
            create = True

        # (re-)create if necessary
        if create:
            if prefix in self._statespaces:
                del self._statespaces[prefix]

            # Setup the base statespace object
            cls = self.prefix_statespace_map[prefix]
            self._statespaces[prefix] = cls(
                self._representations[prefix]['obs'],
                self._representations[prefix]['design'],
                self._representations[prefix]['obs_intercept'],
                self._representations[prefix]['obs_cov'],
                self._representations[prefix]['transition'],
                self._representations[prefix]['state_intercept'],
                self._representations[prefix]['selection'],
                self._representations[prefix]['state_cov']
            )

        return prefix, dtype, create

    def _initialize_state(self, prefix=None, complex_step=False):
        if prefix is None:
            prefix = self.prefix
        dtype = tools.prefix_dtype_map[prefix]

        # (Re-)initialize the statespace model
        if self.initialization == 'known':
            self._statespaces[prefix].initialize_known(
                self._initial_state.astype(dtype),
                self._initial_state_cov.astype(dtype)
            )
        elif self.initialization == 'approximate_diffuse':
            self._statespaces[prefix].initialize_approximate_diffuse(
                self._initial_variance
            )
        elif self.initialization == 'stationary':
            self._statespaces[prefix].initialize_stationary(complex_step)
        else:
            raise RuntimeError('Statespace model not initialized.')

class FrozenRepresentation(object):
    """
    Frozen Statespace Model
    Takes a snapshot of a Statespace model.
    Parameters
    ----------
    model : Representation
        A Statespace representation
    Attributes
    ----------
    nobs : int
        Number of observations.
    k_endog : int
        The dimension of the observation series.
    k_states : int
        The dimension of the unobserved state process.
    k_posdef : int
        The dimension of a guaranteed positive definite
        covariance matrix describing the shocks in the
        measurement equation.
    dtype : dtype
        Datatype of representation matrices
    prefix : str
        BLAS prefix of representation matrices
    shapes : dictionary of name:tuple
        A dictionary recording the shapes of each of
        the representation matrices as tuples.
    endog : array
        The observation vector.
    design : array
        The design matrix, :math:`Z`.
    obs_intercept : array
        The intercept for the observation equation, :math:`d`.
    obs_cov : array
        The covariance matrix for the observation equation :math:`H`.
    transition : array
        The transition matrix, :math:`T`.
    state_intercept : array
        The intercept for the transition equation, :math:`c`.
    selection : array
        The selection matrix, :math:`R`.
    state_cov : array
        The covariance matrix for the state equation :math:`Q`.
    missing : array of bool
        An array of the same size as `endog`, filled
        with boolean values that are True if the
        corresponding entry in `endog` is NaN and False
        otherwise.
    nmissing : array of int
        An array of size `nobs`, where the ith entry
        is the number (between 0 and `k_endog`) of NaNs in
        the ith row of the `endog` array.
    time_invariant : bool
        Whether or not the representation matrices are time-invariant
    initialization : str
        Kalman filter initialization method.
    initial_state : array_like
        The state vector used to initialize the Kalamn filter.
    initial_state_cov : array_like
        The state covariance matrix used to initialize the Kalamn filter.
    """
    _model_attributes = [
        'model', 'prefix', 'dtype', 'nobs', 'k_endog', 'k_states',
        'k_posdef', 'time_invariant', 'endog', 'design', 'obs_intercept',
        'obs_cov', 'transition', 'state_intercept', 'selection',
        'state_cov', 'missing', 'nmissing', 'shapes', 'initialization',
        'initial_state', 'initial_state_cov', 'initial_variance'
    ]
    _attributes = _model_attributes

    def __init__(self, model):
        # Initialize all attributes to None
        for name in self._attributes:
            setattr(self, name, None)

        # Update the representation attributes
        self.update_representation(model)

    def update_representation(self, model):
        # Model
        self.model = model
        self._compatibility_mode = model._compatibility_mode

        # Data type
        self.prefix = model.prefix
        self.dtype = model.dtype

        # Copy the model dimensions
        self.nobs = model.nobs
        self.k_endog = model.k_endog
        self.k_states = model.k_states
        self.k_posdef = model.k_posdef
        self.time_invariant = model.time_invariant

        # Save the state space representation at the time
        self.endog = model.endog
        self.design = model._design.copy()
        self.obs_intercept = model._obs_intercept.copy()
        self.obs_cov = model._obs_cov.copy()
        self.transition = model._transition.copy()
        self.state_intercept = model._state_intercept.copy()
        self.selection = model._selection.copy()
        self.state_cov = model._state_cov.copy()

        self.missing = np.array(model._statespaces[self.prefix].missing,
                                copy=True)
        self.nmissing = np.array(model._statespaces[self.prefix].nmissing,
                                 copy=True)

        # Save the final shapes of the matrices
        self.shapes = dict(model.shapes)
        for name in self.shapes.keys():
            if name == 'obs':
                continue
            self.shapes[name] = getattr(self, name).shape
        self.shapes['obs'] = self.endog.shape

        # Save the state space initialization
        self.initialization = model.initialization

        if model.initialization is not None:
            model._initialize_state()
            self.initial_state = np.array(
                model._statespaces[self.prefix].initial_state, copy=True)
            self.initial_state_cov = np.array(
                model._statespaces[self.prefix].initial_state_cov, copy=True)

compatibility_mode = False
has_trmm = True
prefix_dtype_map = {
    's': np.float32, 'd': np.float64, 'c': np.complex64, 'z': np.complex128
}
prefix_statespace_map = {}
prefix_kalman_filter_map = {}
prefix_kalman_smoother_map = {}
prefix_simulation_smoother_map = {}
prefix_pacf_map = {}
prefix_sv_map = {}
prefix_reorder_missing_matrix_map = {}
prefix_reorder_missing_vector_map = {}
prefix_copy_missing_matrix_map = {}
prefix_copy_missing_vector_map = {}
prefix_copy_index_matrix_map = {}
prefix_copy_index_vector_map = {}


def _is_left_stochastic(matrix):
    # This method checks if `matrix` is left stochastic

    # Matrix should have square shape
    if matrix.ndim != 2 or matrix.shape[0] != matrix.shape[1]:
        return False

    # Comparing by eps is highly important due to floating point imperfect
    # accuracy
    eps = 1e-8

    # Check if all elements are non-negative by eps
    if np.any(matrix < -eps):
        return False

    # If some elements are negative, but insignificantly small, set them
    # to zero
    matrix[matrix < 0] = 0

    # Check if every column represents a discrete probability distribution
    if not np.all(np.fabs(matrix.sum(axis=0) - 1) < eps):
        return False

    # If all checks are passed, return True
    return True



class RegimePartition(object):
    r"""
    Markov switching regimes partition, forming a Markov chain itself
    Parameters
    ----------
    partition : array_like
        Array of size, equal to the number of regimes, encoding the partition
        of regimes set. That is, i-th position of `partition` array contains
        index of partition the i-th regime belongs. If there are :math:`k`
        partitions, than partition indices are from segment :math:`[0, k-1]`.
        Example of 6 regimes partition into 3 sets: `[0, 2, 1, 0, 1, 2]`.
    Notes
    -----
    Suppose that we have :math:`n` Markov switching regimes and partition
    :math:`\lbrace A_1, A_2, ... , A_n \rbrace` of this regime set. Sometimes
    it happens that these subsets form a Markov chain themselves. That is,
    for every :math:`1 \leqslant k, l \leqslant n`:
    .. math::
        Pr[x_t \in A_l | x_{t-1} = i_1 ] = Pr[x_t \in A_l | x_{t-1} = i_2] = ...
        = Pr[x_t \in A_l | x_{t-1} = i_{m_k}]
    where :math:`A_k = \lbrace i_1, i_2, ... , i_{m_k} \rbrace` and :math:`x_t`
    is a sequence of regimes.
    Then we can speak of probabilities
    .. math::
        p_{lk} = Pr[x_t \in A_l | x_{t-1} in A_k ] =
        Pr[x_t \in A_l | x_{t-1} = i_{j}]
    which do not depend on moment :math:`t` and initial regimes distribution.
    This class handles this partition: validates that it forms a Markov chain
    and finds its transition probability matrix.
    See Also
    --------
    statsmodels.tsa.statespace.regime_switching.kim_smoother.KimSmoother.smooth
    """

    def __init__(self, partition):

        self.size = max(partition) + 1
        self._partition = np.array(partition)

    def get_subset_index(self, regime):
        """
        Get an element of partition to which regime belongs
        Parameters
        ----------
        regime : int
            Regime index
        Returns
        -------
        subset_index : int
            Index of partition, containing `regime`
        """
        return self._partition[regime]

    def get_mask(self, subset_index):
        """
        Get a boolean mask of regimes, belonging to subset
        Parameters
        ----------
        subset_index : int
            Index of partition element
        Returns
        -------
        mask : array_like
            Array of size, equal to the number of regimes, where each boolean
            element indicates whether corresponding regime is in the subset.
        """
        return self._partition == subset_index

    def get_transition_probabilities(self, regime_transition):
        """
        Get a transition probability matrix of partition, if it forms Markov
        chain. Otherwise, raise a `ValueError`.
        Parameters
        ----------
        regime_transition : array_like
            Left stochastic matrix of order, equal to the number of regimes,
            representing their transition probabilities.
        Returns
        -------
        transition_probs : array_like
            Left-stochastic matrix, representing transition probabilities
            between elements of partition.
        See Also
        --------
        statsmodels.tsa.statespace.regime_switching.kim_smoother._KimSmoother.\
        __call__
        """

        # Transform provided matrix into numpy array
        regime_transition = np.asarray(regime_transition)

        # Check whether provided matrix is left stochastic
        if not _is_left_stochastic(regime_transition):
            raise ValueError('Provided matrix is not left stochastic')

        # Check whether shapes of matrix and partition conform
        if regime_transition.shape[0] != self._partition.shape[0]:
            raise ValueError('Order of the matrix and number of regimes are \
                    unequal')

        dtype = regime_transition.dtype
        k_regimes = self._partition.shape[0]
        partition_size = self.size

        transition_probs = np.zeros((partition_size, partition_size),
                dtype=dtype)

        regimes = np.arange(k_regimes)

        # All comparisons should use epsilon error
        eps = 1e-8

        # For every (k, l)
        for prev_subset_index in range(partition_size):
            for curr_subset_index in range(partition_size):

                probability_not_set = True

                # For every i_{j} \in A_k
                for prev_regime in regimes[self.get_mask(prev_subset_index)]:

                    # Calculate Pr[ x_t \in A_l | x_{t-1} = i_{j} ]
                    transition_prob = 0.0
                    for curr_regime in regimes[self.get_mask(
                            curr_subset_index)]:
                        transition_prob += regime_transition[curr_regime,
                                prev_regime]

                    # if all such probabilities are equal, fill them in
                    # `transition_probs`, else raise exception
                    if probability_not_set:
                        transition_probs[curr_subset_index,
                                prev_subset_index] = transition_prob
                        probability_not_set = False
                    else:
                        if np.fabs(transition_probs[curr_subset_index,
                                prev_subset_index] - transition_prob) > eps:
                            raise ValueError('Provided partition doesn\'t form \
                                    Markov chain')

        return transition_probs

class MarkovSwitchingParams(object):
    def __init__(self, k_regimes):
        self.k_regimes = k_regimes

        self.k_params = 0
        self.k_parameters = OrderedDict()
        self.switching = OrderedDict()
        self.slices_purpose = OrderedDict()
        self.relative_index_regime_purpose = [
            OrderedDict() for i in range(self.k_regimes)]
        self.index_regime_purpose = [
            OrderedDict() for i in range(self.k_regimes)]
        self.index_regime = [[] for i in range(self.k_regimes)]

    def __getitem__(self, key):
        _type = type(key)

        # Get a slice for a block of parameters by purpose
        if _type is str:
            return self.slices_purpose[key]
        # Get a slice for a block of parameters by regime
        elif _type is int:
            return self.index_regime[key]
        elif _type is tuple:
            if not len(key) == 2:
                raise IndexError('Invalid index')
            if type(key[1]) == str and type(key[0]) == int:
                return self.index_regime_purpose[key[0]][key[1]]
            elif type(key[0]) == str and type(key[1]) == int:
                return self.index_regime_purpose[key[1]][key[0]]
            else:
                raise IndexError('Invalid index')
        else:
            raise IndexError('Invalid index')

    def __setitem__(self, key, value):
        _type = type(key)

        if _type is str:
            value = np.array(value, dtype=bool, ndmin=1)
            k_params = self.k_params
            self.k_parameters[key] = (
                value.size + np.sum(value) * (self.k_regimes - 1))
            self.k_params += self.k_parameters[key]
            self.switching[key] = value
            self.slices_purpose[key] = np.s_[k_params:self.k_params]

            for j in range(self.k_regimes):
                self.relative_index_regime_purpose[j][key] = []
                self.index_regime_purpose[j][key] = []

            offset = 0
            for i in range(value.size):
                switching = value[i]
                for j in range(self.k_regimes):
                    # Non-switching parameters
                    if not switching:
                        self.relative_index_regime_purpose[j][key].append(
                            offset)
                    # Switching parameters
                    else:
                        self.relative_index_regime_purpose[j][key].append(
                            offset + j)
                offset += 1 if not switching else self.k_regimes

            for j in range(self.k_regimes):
                offset = 0
                indices = []
                for k, v in self.relative_index_regime_purpose[j].items():
                    v = (np.r_[v] + offset).tolist()
                    self.index_regime_purpose[j][k] = v
                    indices.append(v)
                    offset += self.k_parameters[k]
                self.index_regime[j] = np.concatenate(indices)
        else:
            raise IndexError('Invalid index')

def set_mode(compatibility=None):
    global compatibility_mode, has_trmm, prefix_statespace_map,        \
        prefix_kalman_filter_map, prefix_kalman_smoother_map,          \
        prefix_simulation_smoother_map, prefix_pacf_map, prefix_sv_map

    # Determine mode automatically if none given
    if compatibility is None:
        try:
            from scipy.linalg import cython_blas
            compatibility = False
        except ImportError:
            compatibility = True

    # If compatibility was False, make sure that is possible
    if not compatibility:
        try:
            from scipy.linalg import cython_blas
        except ImportError:
            warnings.warn('Minimum dependencies not met. Compatibility mode'
                          ' enabled.')
            compatibility = True

    # Initialize the appropriate mode
    if not compatibility:
        from scipy.linalg import cython_blas
        from statsmodels.tsa.statespace import (_representation, _kalman_filter, _kalman_smoother,
                       _simulation_smoother, _tools)
        compatibility_mode = False

        prefix_statespace_map.update({
            's': _representation.sStatespace, 'd': _representation.dStatespace,
            'c': _representation.cStatespace, 'z': _representation.zStatespace
        })
        prefix_kalman_filter_map.update({
            's': _kalman_filter.sKalmanFilter, 'd': _kalman_filter.dKalmanFilter,
            'c': _kalman_filter.cKalmanFilter, 'z': _kalman_filter.zKalmanFilter
        })
        prefix_kalman_smoother_map.update({
            's': _kalman_smoother.sKalmanSmoother,
            'd': _kalman_smoother.dKalmanSmoother,
            'c': _kalman_smoother.cKalmanSmoother,
            'z': _kalman_smoother.zKalmanSmoother
        })
        prefix_simulation_smoother_map.update({
            's': _simulation_smoother.sSimulationSmoother,
            'd': _simulation_smoother.dSimulationSmoother,
            'c': _simulation_smoother.cSimulationSmoother,
            'z': _simulation_smoother.zSimulationSmoother
        })
        prefix_pacf_map.update({
            's': _tools._scompute_coefficients_from_multivariate_pacf,
            'd': _tools._dcompute_coefficients_from_multivariate_pacf,
            'c': _tools._ccompute_coefficients_from_multivariate_pacf,
            'z': _tools._zcompute_coefficients_from_multivariate_pacf
        })
        prefix_sv_map.update({
            's': _tools._sconstrain_sv_less_than_one,
            'd': _tools._dconstrain_sv_less_than_one,
            'c': _tools._cconstrain_sv_less_than_one,
            'z': _tools._zconstrain_sv_less_than_one
        })
        prefix_reorder_missing_matrix_map.update({
            's': _tools.sreorder_missing_matrix,
            'd': _tools.dreorder_missing_matrix,
            'c': _tools.creorder_missing_matrix,
            'z': _tools.zreorder_missing_matrix
        })
        prefix_reorder_missing_vector_map.update({
            's': _tools.sreorder_missing_vector,
            'd': _tools.dreorder_missing_vector,
            'c': _tools.creorder_missing_vector,
            'z': _tools.zreorder_missing_vector
        })
        prefix_copy_missing_matrix_map.update({
            's': _tools.scopy_missing_matrix,
            'd': _tools.dcopy_missing_matrix,
            'c': _tools.ccopy_missing_matrix,
            'z': _tools.zcopy_missing_matrix
        })
        prefix_copy_missing_vector_map.update({
            's': _tools.scopy_missing_vector,
            'd': _tools.dcopy_missing_vector,
            'c': _tools.ccopy_missing_vector,
            'z': _tools.zcopy_missing_vector
        })
        prefix_copy_index_matrix_map.update({
            's': _tools.scopy_index_matrix,
            'd': _tools.dcopy_index_matrix,
            'c': _tools.ccopy_index_matrix,
            'z': _tools.zcopy_index_matrix
        })
        prefix_copy_index_vector_map.update({
            's': _tools.scopy_index_vector,
            'd': _tools.dcopy_index_vector,
            'c': _tools.ccopy_index_vector,
            'z': _tools.zcopy_index_vector
        })
    else:
        from . import _statespace
        from ._pykalman_smoother import _KalmanSmoother
        compatibility_mode = True

        try:
            from scipy.linalg.blas import dtrmm
        except ImportError:
            has_trmm = False

        prefix_statespace_map.update({
            's': _statespace.sStatespace, 'd': _statespace.dStatespace,
            'c': _statespace.cStatespace, 'z': _statespace.zStatespace
        })
        prefix_kalman_filter_map.update({
            's': _statespace.sKalmanFilter, 'd': _statespace.dKalmanFilter,
            'c': _statespace.cKalmanFilter, 'z': _statespace.zKalmanFilter
        })
        prefix_kalman_smoother_map.update({
            's': _KalmanSmoother, 'd': _KalmanSmoother,
            'c': _KalmanSmoother, 'z': _KalmanSmoother
        })
        prefix_simulation_smoother_map.update({
            's': None, 'd': None, 'c': None, 'z': None
        })
        if has_trmm:
            prefix_pacf_map.update({
                's': _statespace._scompute_coefficients_from_multivariate_pacf,
                'd': _statespace._dcompute_coefficients_from_multivariate_pacf,
                'c': _statespace._ccompute_coefficients_from_multivariate_pacf,
                'z': _statespace._zcompute_coefficients_from_multivariate_pacf
            })
            prefix_sv_map.update({
                's': _statespace._sconstrain_sv_less_than_one,
                'd': _statespace._dconstrain_sv_less_than_one,
                'c': _statespace._cconstrain_sv_less_than_one,
                'z': _statespace._zconstrain_sv_less_than_one
            })
        prefix_reorder_missing_matrix_map.update({
            's': _statespace.sreorder_missing_matrix,
            'd': _statespace.dreorder_missing_matrix,
            'c': _statespace.creorder_missing_matrix,
            'z': _statespace.zreorder_missing_matrix
        })
        prefix_reorder_missing_vector_map.update({
            's': _statespace.sreorder_missing_vector,
            'd': _statespace.dreorder_missing_vector,
            'c': _statespace.creorder_missing_vector,
            'z': _statespace.zreorder_missing_vector
        })
        prefix_copy_missing_matrix_map.update({
            's': _statespace.scopy_missing_matrix,
            'd': _statespace.dcopy_missing_matrix,
            'c': _statespace.ccopy_missing_matrix,
            'z': _statespace.zcopy_missing_matrix
        })
        prefix_copy_missing_vector_map.update({
            's': _statespace.scopy_missing_vector,
            'd': _statespace.dcopy_missing_vector,
            'c': _statespace.ccopy_missing_vector,
            'z': _statespace.zcopy_missing_vector
        })
        prefix_copy_index_matrix_map.update({
            's': _statespace.scopy_index_matrix,
            'd': _statespace.dcopy_index_matrix,
            'c': _statespace.ccopy_index_matrix,
            'z': _statespace.zcopy_index_matrix
        })
        prefix_copy_index_vector_map.update({
            's': _statespace.scopy_index_vector,
            'd': _statespace.dcopy_index_vector,
            'c': _statespace.ccopy_index_vector,
            'z': _statespace.zcopy_index_vector
        })
set_mode(compatibility=None)


try:
    from scipy.linalg.blas import find_best_blas_type
except ImportError:  # pragma: no cover
    # Shim for SciPy 0.11, derived from tag=0.11 scipy.linalg.blas
    _type_conv = {'f': 's', 'd': 'd', 'F': 'c', 'D': 'z', 'G': 'z'}

    def find_best_blas_type(arrays):
        dtype, index = max(
            [(ar.dtype, i) for i, ar in enumerate(arrays)])
        prefix = _type_conv.get(dtype.char, 'd')
        return prefix, dtype, None


def companion_matrix(polynomial):
    r"""
    Create a companion matrix
    Parameters
    ----------
    polynomial : array_like or list
        If an iterable, interpreted as the coefficients of the polynomial from
        which to form the companion matrix. Polynomial coefficients are in
        order of increasing degree, and may be either scalars (as in an AR(p)
        model) or coefficient matrices (as in a VAR(p) model). If an integer,
        it is interpereted as the size of a companion matrix of a scalar
        polynomial, where the polynomial coefficients are initialized to zeros.
        If a matrix polynomial is passed, :math:`C_0` may be set to the scalar
        value 1 to indicate an identity matrix (doing so will improve the speed
        of the companion matrix creation).
    Returns
    -------
    companion_matrix : array
    Notes
    -----
    Given coefficients of a lag polynomial of the form:
    .. math::
        c(L) = c_0 + c_1 L + \dots + c_p L^p
    returns a matrix of the form
    .. math::
        \begin{bmatrix}
            \phi_1 & 1      & 0 & \cdots & 0 \\
            \phi_2 & 0      & 1 &        & 0 \\
            \vdots &        &   & \ddots & 0 \\
                   &        &   &        & 1 \\
            \phi_n & 0      & 0 & \cdots & 0 \\
        \end{bmatrix}
    where some or all of the :math:`\phi_i` may be non-zero (if `polynomial` is
    None, then all are equal to zero).
    If the coefficients provided are scalars :math:`(c_0, c_1, \dots, c_p)`,
    then the companion matrix is an :math:`n \times n` matrix formed with the
    elements in the first column defined as
    :math:`\phi_i = -\frac{c_i}{c_0}, i \in 1, \dots, p`.
    If the coefficients provided are matrices :math:`(C_0, C_1, \dots, C_p)`,
    each of shape :math:`(m, m)`, then the companion matrix is an
    :math:`nm \times nm` matrix formed with the elements in the first column
    defined as :math:`\phi_i = -C_0^{-1} C_i', i \in 1, \dots, p`.
    It is important to understand the expected signs of the coefficients. A
    typical AR(p) model is written as:
    .. math::
        y_t = a_1 y_{t-1} + \dots + a_p y_{t-p} + \varepsilon_t
    This can be rewritten as:
    .. math::
        (1 - a_1 L - \dots - a_p L^p )y_t = \varepsilon_t \\
        (1 + c_1 L + \dots + c_p L^p )y_t = \varepsilon_t \\
        c(L) y_t = \varepsilon_t
    The coefficients from this form are defined to be :math:`c_i = - a_i`, and
    it is the :math:`c_i` coefficients that this function expects to be
    provided.
    """
    identity_matrix = False
    if isinstance(polynomial, int):
        n = polynomial
        m = 1
        polynomial = None
    else:
        n = len(polynomial) - 1

        if n < 1:
            raise ValueError("Companion matrix polynomials must include at"
                             " least two terms.")

        if isinstance(polynomial, list) or isinstance(polynomial, tuple):
            try:
                # Note: can't use polynomial[0] because of the special behavior
                # associated with matrix polynomials and the constant 1, see
                # below.
                m = len(polynomial[1])
            except TypeError:
                m = 1

            # Check if we just have a scalar polynomial
            if m == 1:
                polynomial = np.asanyarray(polynomial)
            # Check if 1 was passed as the first argument (indicating an
            # identity matrix)
            elif polynomial[0] == 1:
                polynomial[0] = np.eye(m)
                identity_matrix = True
        else:
            m = 1
            polynomial = np.asanyarray(polynomial)

    matrix = np.zeros((n * m, n * m), dtype=np.asanyarray(polynomial).dtype)
    idx = np.diag_indices((n - 1) * m)
    idx = (idx[0], idx[1] + m)
    matrix[idx] = 1
    if polynomial is not None and n > 0:
        if m == 1:
            matrix[:, 0] = -polynomial[1:] / polynomial[0]
        elif identity_matrix:
            for i in range(n):
                matrix[i * m:(i + 1) * m, :m] = -polynomial[i+1].T
        else:
            inv = np.linalg.inv(polynomial[0])
            for i in range(n):
                matrix[i * m:(i + 1) * m, :m] = -np.dot(inv, polynomial[i+1]).T
    return matrix


def diff(series, k_diff=1, k_seasonal_diff=None, k_seasons=1):
    r"""
    Difference a series simply and/or seasonally along the zero-th axis.
    Given a series (denoted :math:`y_t`), performs the differencing operation
    .. math::
        \Delta^d \Delta_s^D y_t
    where :math:`d =` `diff`, :math:`s =` `k_seasons`,
    :math:`D =` `seasonal\_diff`, and :math:`\Delta` is the difference
    operator.
    Parameters
    ----------
    series : array_like
        The series to be differenced.
    diff : int, optional
        The number of simple differences to perform. Default is 1.
    seasonal_diff : int or None, optional
        The number of seasonal differences to perform. Default is no seasonal
        differencing.
    k_seasons : int, optional
        The seasonal lag. Default is 1. Unused if there is no seasonal
        differencing.
    Returns
    -------
    differenced : array
        The differenced array.
    """
    pandas = _is_using_pandas(series, None)
    differenced = np.asanyarray(series) if not pandas else series

    # Seasonal differencing
    if k_seasonal_diff is not None:
        while k_seasonal_diff > 0:
            if not pandas:
                differenced = (
                    differenced[k_seasons:] - differenced[:-k_seasons]
                )
            else:
                differenced = differenced.diff(k_seasons)[k_seasons:]
            k_seasonal_diff -= 1

    # Simple differencing
    if not pandas:
        differenced = np.diff(differenced, k_diff, axis=0)
    else:
        while k_diff > 0:
            differenced = differenced.diff()[1:]
            k_diff -= 1
    return differenced


def concat(series, axis=0, allow_mix=False):
    """
    Concatenate a set of series.
    Parameters
    ----------
    series : iterable
        An iterable of series to be concatenated
    axis : int, optional
        The axis along which to concatenate. Default is 1 (columns).
    allow_mix : bool
        Whether or not to allow a mix of pandas and non-pandas objects. Default
        is False. If true, the returned object is an ndarray, and additional
        pandas metadata (e.g. column names, indices, etc) is lost.
    Returns
    -------
    concatenated : array or pd.DataFrame
        The concatenated array. Will be a DataFrame if series are pandas
        objects.
    """
    is_pandas = np.r_[[_is_using_pandas(s, None) for s in series]]

    if np.all(is_pandas):
        concatenated = pd.concat(series, axis=axis)
    elif np.all(~is_pandas) or allow_mix:
        concatenated = np.concatenate(series, axis=axis)
    else:
        raise ValueError('Attempted to concatenate Pandas objects with'
                         ' non-Pandas objects with `allow_mix=False`.')

    return concatenated


def is_invertible(polynomial, threshold=1.):
    r"""
    Determine if a polynomial is invertible.
    Requires all roots of the polynomial lie inside the unit circle.
    Parameters
    ----------
    polynomial : array_like or tuple, list
        Coefficients of a polynomial, in order of increasing degree.
        For example, `polynomial=[1, -0.5]` corresponds to the polynomial
        :math:`1 - 0.5x` which has root :math:`2`. If it is a matrix
        polynomial (in which case the coefficients are coefficient matrices),
        a tuple or list of matrices should be passed.
    threshold : number
        Allowed threshold for `is_invertible` to return True. Default is 1.
    Notes
    -----
    If the coefficients provided are scalars :math:`(c_0, c_1, \dots, c_n)`,
    then the corresponding polynomial is :math:`c_0 + c_1 L + \dots + c_n L^n`.
    If the coefficients provided are matrices :math:`(C_0, C_1, \dots, C_n)`,
    then the corresponding polynomial is :math:`C_0 + C_1 L + \dots + C_n L^n`.
    There are three equivalent methods of determining if the polynomial
    represented by the coefficients is invertible:
    The first method factorizes the polynomial into:
    .. math::
        C(L) & = c_0 + c_1 L + \dots + c_n L^n \\
             & = constant (1 - \lambda_1 L)
                 (1 - \lambda_2 L) \dots (1 - \lambda_n L)
    In order for :math:`C(L)` to be invertible, it must be that each factor
    :math:`(1 - \lambda_i L)` is invertible; the condition is then that
    :math:`|\lambda_i| < 1`, where :math:`\lambda_i` is a root of the
    polynomial.
    The second method factorizes the polynomial into:
    .. math::
        C(L) & = c_0 + c_1 L + \dots + c_n L^n \\
             & = constant (L - \zeta_1) (L - \zeta_2) \dots (L - \zeta_3)
    The condition is now :math:`|\zeta_i| > 1`, where :math:`\zeta_i` is a root
    of the polynomial with reversed coefficients and
    :math:`\lambda_i = \frac{1}{\zeta_i}`.
    Finally, a companion matrix can be formed using the coefficients of the
    polynomial. Then the eigenvalues of that matrix give the roots of the
    polynomial. This last method is the one actually used.
    See Also
    --------
    companion_matrix
    """
    # First method:
    # np.all(np.abs(np.roots(np.r_[1, params])) < 1)
    # Second method:
    # np.all(np.abs(np.roots(np.r_[1, params][::-1])) > 1)
    # Final method:
    eigvals = np.linalg.eigvals(companion_matrix(polynomial))
    return np.all(np.abs(eigvals) < threshold)


def solve_discrete_lyapunov(a, q, complex_step=False):
    r"""
    Solves the discrete Lyapunov equation using a bilinear transformation.
    Notes
    -----
    This is a modification of the version in Scipy (see
    https://github.com/scipy/scipy/blob/master/scipy/linalg/_solvers.py)
    which allows passing through the complex numbers in the matrix a
    (usually the transition matrix) in order to allow complex step
    differentiation.
    """
    eye = np.eye(a.shape[0])
    if not complex_step:
        aH = a.conj().transpose()
        aHI_inv = np.linalg.inv(aH + eye)
        b = np.dot(aH - eye, aHI_inv)
        c = 2*np.dot(np.dot(np.linalg.inv(a + eye), q), aHI_inv)
        return solve_sylvester(b.conj().transpose(), b, -c)
    else:
        aH = a.transpose()
        aHI_inv = np.linalg.inv(aH + eye)
        b = np.dot(aH - eye, aHI_inv)
        c = 2*np.dot(np.dot(np.linalg.inv(a + eye), q), aHI_inv)
        return solve_sylvester(b.transpose(), b, -c)


def constrain_stationary_univariate(unconstrained):
    """
    Transform unconstrained parameters used by the optimizer to constrained
    parameters used in likelihood evaluation
    Parameters
    ----------
    unconstrained : array
        Unconstrained parameters used by the optimizer, to be transformed to
        stationary coefficients of, e.g., an autoregressive or moving average
        component.
    Returns
    -------
    constrained : array
        Constrained parameters of, e.g., an autoregressive or moving average
        component, to be transformed to arbitrary parameters used by the
        optimizer.
    References
    ----------
    .. [1] Monahan, John F. 1984.
       "A Note on Enforcing Stationarity in
       Autoregressive-moving Average Models."
       Biometrika 71 (2) (August 1): 403-404.
    """

    n = unconstrained.shape[0]
    y = np.zeros((n, n), dtype=unconstrained.dtype)
    r = unconstrained/((1 + unconstrained**2)**0.5)
    for k in range(n):
        for i in range(k):
            y[k, i] = y[k - 1, i] + r[k] * y[k - 1, k - i - 1]
        y[k, k] = r[k]
    return -y[n - 1, :]


def unconstrain_stationary_univariate(constrained):
    """
    Transform constrained parameters used in likelihood evaluation
    to unconstrained parameters used by the optimizer
    Parameters
    ----------
    constrained : array
        Constrained parameters of, e.g., an autoregressive or moving average
        component, to be transformed to arbitrary parameters used by the
        optimizer.
    Returns
    -------
    unconstrained : array
        Unconstrained parameters used by the optimizer, to be transformed to
        stationary coefficients of, e.g., an autoregressive or moving average
        component.
    References
    ----------
    .. [1] Monahan, John F. 1984.
       "A Note on Enforcing Stationarity in
       Autoregressive-moving Average Models."
       Biometrika 71 (2) (August 1): 403-404.
    """
    n = constrained.shape[0]
    y = np.zeros((n, n), dtype=constrained.dtype)
    y[n-1:] = -constrained
    for k in range(n-1, 0, -1):
        for i in range(k):
            y[k-1, i] = (y[k, i] - y[k, k]*y[k, k-i-1]) / (1 - y[k, k]**2)
    r = y.diagonal()
    x = r / ((1 - r**2)**0.5)
    return x


def _constrain_sv_less_than_one_python(unconstrained, order=None,
                                       k_endog=None):
    """
    Transform arbitrary matrices to matrices with singular values less than
    one.
    Parameters
    ----------
    unconstrained : list
        Arbitrary matrices. Should be a list of length `order`, where each
        element is an array sized `k_endog` x `k_endog`.
    order : integer, optional
        The order of the autoregression.
    k_endog : integer, optional
        The dimension of the data vector.
    Returns
    -------
    constrained : list
        Partial autocorrelation matrices. Should be a list of length
        `order`, where each element is an array sized `k_endog` x `k_endog`.
    Notes
    -----
    Corresponds to Lemma 2.2 in Ansley and Kohn (1986). See
    `constrain_stationary_multivariate` for more details.
    There is a Cython implementation of this function that can be much faster,
    but which requires SciPy 0.14.0 or greater. See
    `constrain_stationary_multivariate` for details.
    """

    from scipy import linalg

    constrained = []  # P_s,  s = 1, ..., p
    if order is None:
        order = len(unconstrained)
    if k_endog is None:
        k_endog = unconstrained[0].shape[0]

    eye = np.eye(k_endog)
    for i in range(order):
        A = unconstrained[i]
        B, lower = linalg.cho_factor(eye + np.dot(A, A.T), lower=True)
        constrained.append(linalg.solve_triangular(B, A, lower=lower))
    return constrained


def _compute_coefficients_from_multivariate_pacf_python(
        partial_autocorrelations, error_variance, transform_variance=False,
        order=None, k_endog=None):
    """
    Transform matrices with singular values less than one to matrices
    corresponding to a stationary (or invertible) process.
    Parameters
    ----------
    partial_autocorrelations : list
        Partial autocorrelation matrices. Should be a list of length `order`,
        where each element is an array sized `k_endog` x `k_endog`.
    error_variance : array
        The variance / covariance matrix of the error term. Should be sized
        `k_endog` x `k_endog`. This is used as input in the algorithm even if
        is not transformed by it (when `transform_variance` is False). The
        error term variance is required input when transformation is used
        either to force an autoregressive component to be stationary or to
        force a moving average component to be invertible.
    transform_variance : boolean, optional
        Whether or not to transform the error variance term. This option is
        not typically used, and the default is False.
    order : integer, optional
        The order of the autoregression.
    k_endog : integer, optional
        The dimension of the data vector.
    Returns
    -------
    coefficient_matrices : list
        Transformed coefficient matrices leading to a stationary VAR
        representation.
    Notes
    -----
    Corresponds to Lemma 2.1 in Ansley and Kohn (1986). See
    `constrain_stationary_multivariate` for more details.
    There is a Cython implementation of this function that can be much faster,
    but which requires SciPy 0.14.0 or greater. See
    `constrain_stationary_multivariate` for details.
    """
    from scipy import linalg

    if order is None:
        order = len(partial_autocorrelations)
    if k_endog is None:
        k_endog = partial_autocorrelations[0].shape[0]

    # If we want to keep the provided variance but with the constrained
    # coefficient matrices, we need to make a copy here, and then after the
    # main loop we will transform the coefficients to match the passed variance
    if not transform_variance:
        initial_variance = error_variance
        # Need to make the input variance large enough that the recursions
        # don't lead to zero-matrices due to roundoff error, which would case
        # exceptions from the Cholesky decompositions.
        # Note that this will still not always ensure positive definiteness,
        # and for k_endog, order large enough an exception may still be raised
        error_variance = np.eye(k_endog) * (order + k_endog)**10

    forward_variances = [error_variance]   # \Sigma_s
    backward_variances = [error_variance]  # \Sigma_s^*,  s = 0, ..., p
    autocovariances = [error_variance]     # \Gamma_s
    # \phi_{s,k}, s = 1, ..., p
    #             k = 1, ..., s+1
    forwards = []
    # \phi_{s,k}^*
    backwards = []

    error_variance_factor = linalg.cholesky(error_variance, lower=True)

    forward_factors = [error_variance_factor]
    backward_factors = [error_variance_factor]

    # We fill in the entries as follows:
    # [1,1]
    # [2,2], [2,1]
    # [3,3], [3,1], [3,2]
    # ...
    # [p,p], [p,1], ..., [p,p-1]
    # the last row, correctly ordered, is then used as the coefficients
    for s in range(order):  # s = 0, ..., p-1
        prev_forwards = forwards
        prev_backwards = backwards
        forwards = []
        backwards = []

        # Create the "last" (k = s+1) matrix
        # Note: this is for k = s+1. However, below we then have to fill
        # in for k = 1, ..., s in order.
        # P L*^{-1} = x
        # x L* = P
        # L*' x' = P'
        forwards.append(
            linalg.solve_triangular(
                backward_factors[s], partial_autocorrelations[s].T,
                lower=True, trans='T'))
        forwards[0] = np.dot(forward_factors[s], forwards[0].T)

        # P' L^{-1} = x
        # x L = P'
        # L' x' = P
        backwards.append(
            linalg.solve_triangular(
                forward_factors[s], partial_autocorrelations[s],
                lower=True, trans='T'))
        backwards[0] = np.dot(backward_factors[s], backwards[0].T)

        # Update the variance
        # Note: if s >= 1, this will be further updated in the for loop
        # below
        # Also, this calculation will be re-used in the forward variance
        tmp = np.dot(forwards[0], backward_variances[s])
        autocovariances.append(tmp.copy().T)

        # Create the remaining k = 1, ..., s matrices,
        # only has an effect if s >= 1
        for k in range(s):
            forwards.insert(k, prev_forwards[k] - np.dot(
                forwards[-1], prev_backwards[s-(k+1)]))

            backwards.insert(k, prev_backwards[k] - np.dot(
                backwards[-1], prev_forwards[s-(k+1)]))

            autocovariances[s+1] += np.dot(autocovariances[k+1],
                                           prev_forwards[s-(k+1)].T)

        # Create forward and backwards variances
        forward_variances.append(
            forward_variances[s] - np.dot(tmp, forwards[s].T)
        )
        backward_variances.append(
            backward_variances[s] -
            np.dot(
                np.dot(backwards[s], forward_variances[s]),
                backwards[s].T
            )
        )

        # Cholesky factors
        forward_factors.append(
            linalg.cholesky(forward_variances[s+1], lower=True)
        )
        backward_factors.append(
            linalg.cholesky(backward_variances[s+1], lower=True)
        )

    # If we do not want to use the transformed variance, we need to
    # adjust the constrained matrices, as presented in Lemma 2.3, see above
    variance = forward_variances[-1]
    if not transform_variance:
        # Here, we need to construct T such that:
        # variance = T * initial_variance * T'
        # To do that, consider the Cholesky of variance (L) and
        # input_variance (M) to get:
        # L L' = T M M' T' = (TM) (TM)'
        # => L = T M
        # => L M^{-1} = T
        initial_variance_factor = np.linalg.cholesky(initial_variance)
        transformed_variance_factor = np.linalg.cholesky(variance)
        transform = np.dot(initial_variance_factor,
                           np.linalg.inv(transformed_variance_factor))
        inv_transform = np.linalg.inv(transform)

        for i in range(order):
            forwards[i] = (
                np.dot(np.dot(transform, forwards[i]), inv_transform)
            )

    return forwards, variance


def constrain_stationary_multivariate_python(unconstrained, error_variance,
                                             transform_variance=False,
                                             prefix=None):
    """
    Transform unconstrained parameters used by the optimizer to constrained
    parameters used in likelihood evaluation for a vector autoregression.
    Parameters
    ----------
    unconstrained : array or list
        Arbitrary matrices to be transformed to stationary coefficient matrices
        of the VAR. If a list, should be a list of length `order`, where each
        element is an array sized `k_endog` x `k_endog`. If an array, should be
        the matrices horizontally concatenated and sized
        `k_endog` x `k_endog * order`.
    error_variance : array
        The variance / covariance matrix of the error term. Should be sized
        `k_endog` x `k_endog`. This is used as input in the algorithm even if
        is not transformed by it (when `transform_variance` is False). The
        error term variance is required input when transformation is used
        either to force an autoregressive component to be stationary or to
        force a moving average component to be invertible.
    transform_variance : boolean, optional
        Whether or not to transform the error variance term. This option is
        not typically used, and the default is False.
    prefix : {'s','d','c','z'}, optional
        The appropriate BLAS prefix to use for the passed datatypes. Only
        use if absolutely sure that the prefix is correct or an error will
        result.
    Returns
    -------
    constrained : array or list
        Transformed coefficient matrices leading to a stationary VAR
        representation. Will match the type of the passed `unconstrained`
        variable (so if a list was passed, a list will be returned).
    Notes
    -----
    In the notation of [1]_, the arguments `(variance, unconstrained)` are
    written as :math:`(\Sigma, A_1, \dots, A_p)`, where :math:`p` is the order
    of the vector autoregression, and is here determined by the length of
    the `unconstrained` argument.
    There are two steps in the constraining algorithm.
    First, :math:`(A_1, \dots, A_p)` are transformed into
    :math:`(P_1, \dots, P_p)` via Lemma 2.2 of [1]_.
    Second, :math:`(\Sigma, P_1, \dots, P_p)` are transformed into
    :math:`(\Sigma, \phi_1, \dots, \phi_p)` via Lemmas 2.1 and 2.3 of [1]_.
    If `transform_variance=True`, then only Lemma 2.1 is applied in the second
    step.
    While this function can be used even in the univariate case, it is much
    slower, so in that case `constrain_stationary_univariate` is preferred.
    References
    ----------
    .. [1] Ansley, Craig F., and Robert Kohn. 1986.
       "A Note on Reparameterizing a Vector Autoregressive Moving Average Model
       to Enforce Stationarity."
       Journal of Statistical Computation and Simulation 24 (2): 99-106.
    .. [2] Ansley, Craig F, and Paul Newbold. 1979.
       "Multivariate Partial Autocorrelations."
       In Proceedings of the Business and Economic Statistics Section, 349-53.
       American Statistical Association
    """

    use_list = type(unconstrained) == list
    if not use_list:
        k_endog, order = unconstrained.shape
        order //= k_endog

        unconstrained = [
            unconstrained[:k_endog, i*k_endog:(i+1)*k_endog]
            for i in range(order)
        ]

    order = len(unconstrained)
    k_endog = unconstrained[0].shape[0]

    # Step 1: convert from arbitrary matrices to those with singular values
    # less than one.
    sv_constrained = _constrain_sv_less_than_one_python(
        unconstrained, order, k_endog)

    # Step 2: convert matrices from our "partial autocorrelation matrix" space
    # (matrices with singular values less than one) to the space of stationary
    # coefficient matrices
    constrained, var = _compute_coefficients_from_multivariate_pacf_python(
        sv_constrained, error_variance, transform_variance, order, k_endog)

    if not use_list:
        constrained = np.concatenate(constrained, axis=1).reshape(
            k_endog, k_endog * order)

    return constrained, var


# Conditionally use the Cython versions of the multivariate constraint if
# possible (i.e. if Scipy >= 0.14.0 is available.)
if has_trmm:

    def constrain_stationary_multivariate(unconstrained, variance,
                                          transform_variance=False,
                                          prefix=None):

        use_list = type(unconstrained) == list
        if use_list:
            unconstrained = np.concatenate(unconstrained, axis=1)

        k_endog, order = unconstrained.shape
        order //= k_endog

        if order < 1:
            raise ValueError('Must have order at least 1')
        if k_endog < 1:
            raise ValueError('Must have at least 1 endogenous variable')

        if prefix is None:
            prefix, dtype, _ = find_best_blas_type(
                [unconstrained, variance])
        dtype = prefix_dtype_map[prefix]

        unconstrained = np.asfortranarray(unconstrained, dtype=dtype)
        variance = np.asfortranarray(variance, dtype=dtype)

        # Step 1: convert from arbitrary matrices to those with singular values
        # less than one.
        # sv_constrained = _constrain_sv_less_than_one(unconstrained, order,
        #                                              k_endog, prefix)
        sv_constrained = prefix_sv_map[prefix](unconstrained, order, k_endog)

        # Step 2: convert matrices from our "partial autocorrelation matrix"
        # space (matrices with singular values less than one) to the space of
        # stationary coefficient matrices
        constrained, variance = prefix_pacf_map[prefix](
            sv_constrained, variance, transform_variance, order, k_endog)

        constrained = np.array(constrained, dtype=dtype)
        variance = np.array(variance, dtype=dtype)

        if use_list:
            constrained = [
                constrained[:k_endog, i*k_endog:(i+1)*k_endog]
                for i in range(order)
            ]

        return constrained, variance
    constrain_stationary_multivariate.__doc__ = (
        constrain_stationary_multivariate_python.__doc__)

else:
    constrain_stationary_multivariate = (
        constrain_stationary_multivariate_python)


def _unconstrain_sv_less_than_one(constrained, order=None, k_endog=None):
    """
    Transform matrices with singular values less than one to arbitrary
    matrices.
    Parameters
    ----------
    constrained : list
        The partial autocorrelation matrices. Should be a list of length
        `order`, where each element is an array sized `k_endog` x `k_endog`.
    order : integer, optional
        The order of the autoregression.
    k_endog : integer, optional
        The dimension of the data vector.
    Returns
    -------
    unconstrained : list
        Unconstrained matrices. A list of length `order`, where each element is
        an array sized `k_endog` x `k_endog`.
    Notes
    -----
    Corresponds to the inverse of Lemma 2.2 in Ansley and Kohn (1986). See
    `unconstrain_stationary_multivariate` for more details.
    """
    from scipy import linalg

    unconstrained = []  # A_s,  s = 1, ..., p
    if order is None:
        order = len(constrained)
    if k_endog is None:
        k_endog = constrained[0].shape[0]

    eye = np.eye(k_endog)
    for i in range(order):
        P = constrained[i]
        # B^{-1} B^{-1}' = I - P P'
        B_inv, lower = linalg.cho_factor(eye - np.dot(P, P.T), lower=True)
        # A = BP
        # B^{-1} A = P
        unconstrained.append(linalg.solve_triangular(B_inv, P, lower=lower))
    return unconstrained


def _compute_multivariate_sample_acovf(endog, maxlag):
    """
    Computer multivariate sample autocovariances
    Parameters
    ----------
    endog : array_like
        Sample data on which to compute sample autocovariances. Shaped
        `nobs` x `k_endog`.
    Returns
    -------
    sample_autocovariances : list
        A list of the first `maxlag` sample autocovariance matrices. Each
        matrix is shaped `k_endog` x `k_endog`.
    Notes
    -----
    This function computes the forward sample autocovariances:
    .. math::
        \hat \Gamma(s) = \frac{1}{n} \sum_{t=1}^{n-s}
        (Z_t - \bar Z) (Z_{t+s} - \bar Z)'
    See page 353 of Wei (1990). This function is primarily implemented for
    checking the partial autocorrelation functions below, and so is quite slow.
    References
    ----------
    .. [1] Wei, William. 1990.
        Time Series Analysis : Univariate and Multivariate Methods.
       Boston: Pearson.
    """
    # Get the (demeaned) data as an array
    endog = np.array(endog)
    if endog.ndim == 1:
        endog = endog[:, np.newaxis]
    endog -= np.mean(endog, axis=0)

    # Dimensions
    nobs, k_endog = endog.shape

    sample_autocovariances = []
    for s in range(maxlag + 1):
        sample_autocovariances.append(np.zeros((k_endog, k_endog)))
        for t in range(nobs - s):
            sample_autocovariances[s] += np.outer(endog[t], endog[t+s])
        sample_autocovariances[s] /= nobs

    return sample_autocovariances


def _compute_multivariate_acovf_from_coefficients(
        coefficients, error_variance, maxlag=None,
        forward_autocovariances=False):
    """
    Compute multivariate autocovariances from vector autoregression coefficient
    matrices
    Parameters
    ----------
    coefficients : array or list
        The coefficients matrices. If a list, should be a list of length
        `order`, where each element is an array sized `k_endog` x `k_endog`. If
        an array, should be the coefficient matrices horizontally concatenated
        and sized `k_endog` x `k_endog * order`.
    error_variance : array
        The variance / covariance matrix of the error term. Should be sized
        `k_endog` x `k_endog`.
    maxlag : integer, optional
        The maximum autocovariance to compute. Default is `order`-1. Can be
        zero, in which case it returns the variance.
    forward_autocovariances : boolean, optional
        Whether or not to compute forward autocovariances
        :math:`E(y_t y_{t+j}')`. Default is False, so that backward
        autocovariances :math:`E(y_t y_{t-j}')` are returned.
    Returns
    -------
    autocovariances : list
        A list of the first `maxlag` autocovariance matrices. Each matrix is
        shaped `k_endog` x `k_endog`.
    Notes
    -----
    Computes
    ..math::
        \Gamma(j) = E(y_t y_{t-j}')
    for j = 1, ..., `maxlag`, unless `forward_autocovariances` is specified,
    in which case it computes:
    ..math::
        E(y_t y_{t+j}') = \Gamma(j)'
    Coefficients are assumed to be provided from the VAR model:
    .. math::
        y_t = A_1 y_{t-1} + \dots + A_p y_{t-p} + \varepsilon_t
    Autocovariances are calculated by solving the associated discrete Lyapunov
    equation of the state space representation of the VAR process.
    """
    from scipy import linalg

    # Convert coefficients to a list of matrices, for use in
    # `companion_matrix`; get dimensions
    if type(coefficients) == list:
        order = len(coefficients)
        k_endog = coefficients[0].shape[0]
    else:
        k_endog, order = coefficients.shape
        order //= k_endog

        coefficients = [
            coefficients[:k_endog, i*k_endog:(i+1)*k_endog]
            for i in range(order)
        ]

    if maxlag is None:
        maxlag = order-1

    # Start with VAR(p): w_{t+1} = phi_1 w_t + ... + phi_p w_{t-p+1} + u_{t+1}
    # Then stack the VAR(p) into a VAR(1) in companion matrix form:
    # z_{t+1} = F z_t + v_t
    companion = companion_matrix(
        [1] + [-coefficients[i] for i in range(order)]
    ).T

    # Compute the error variance matrix for the stacked form: E v_t v_t'
    selected_variance = np.zeros(companion.shape)
    selected_variance[:k_endog, :k_endog] = error_variance

    # Compute the unconditional variance of z_t: E z_t z_t'
    stacked_cov = linalg.solve_discrete_lyapunov(companion, selected_variance)

    # The first (block) row of the variance of z_t gives the first p-1
    # autocovariances of w_t: \Gamma_i = E w_t w_t+i with \Gamma_0 = Var(w_t)
    # Note: these are okay, checked against ArmaProcess
    autocovariances = [
        stacked_cov[:k_endog, i*k_endog:(i+1)*k_endog]
        for i in range(min(order, maxlag+1))
    ]

    for i in range(maxlag - (order-1)):
        stacked_cov = np.dot(companion, stacked_cov)
        autocovariances += [
            stacked_cov[:k_endog, -k_endog:]
        ]

    if forward_autocovariances:
        for i in range(len(autocovariances)):
            autocovariances[i] = autocovariances[i].T

    return autocovariances


def _compute_multivariate_sample_pacf(endog, maxlag):
    """
    Computer multivariate sample partial autocorrelations
    Parameters
    ----------
    endog : array_like
        Sample data on which to compute sample autocovariances. Shaped
        `nobs` x `k_endog`.
    maxlag : integer
        Maximum lag for which to calculate sample partial autocorrelations.
    Returns
    -------
    sample_pacf : list
        A list of the first `maxlag` sample partial autocorrelation matrices.
        Each matrix is shaped `k_endog` x `k_endog`.
    """
    sample_autocovariances = _compute_multivariate_sample_acovf(endog, maxlag)

    return _compute_multivariate_pacf_from_autocovariances(
        sample_autocovariances)


def _compute_multivariate_pacf_from_autocovariances(autocovariances,
                                                    order=None, k_endog=None):
    """
    Compute multivariate partial autocorrelations from autocovariances.
    Parameters
    ----------
    autocovariances : list
        Autocorrelations matrices. Should be a list of length `order` + 1,
        where each element is an array sized `k_endog` x `k_endog`.
    order : integer, optional
        The order of the autoregression.
    k_endog : integer, optional
        The dimension of the data vector.
    Returns
    -------
    pacf : list
        List of first `order` multivariate partial autocorrelations.
    Notes
    -----
    Note that this computes multivariate partial autocorrelations.
    Corresponds to the inverse of Lemma 2.1 in Ansley and Kohn (1986). See
    `unconstrain_stationary_multivariate` for more details.
    Notes
    -----
    Computes sample partial autocorrelations if sample autocovariances are
    given.
    """
    from scipy import linalg

    if order is None:
        order = len(autocovariances)-1
    if k_endog is None:
        k_endog = autocovariances[0].shape[0]

    # Now apply the Ansley and Kohn (1986) algorithm, except that instead of
    # calculating phi_{s+1, s+1} = L_s P_{s+1} {L_s^*}^{-1} (which requires
    # the partial autocorrelation P_{s+1} which is what we're trying to
    # calculate here), we calculate it as in Ansley and Newbold (1979), using
    # the autocovariances \Gamma_s and the forwards and backwards residual
    # variances \Sigma_s, \Sigma_s^*:
    # phi_{s+1, s+1} = [ \Gamma_{s+1}' - \phi_{s,1} \Gamma_s' - ... -
    #                    \phi_{s,s} \Gamma_1' ] {\Sigma_s^*}^{-1}

    # Forward and backward variances
    forward_variances = []   # \Sigma_s
    backward_variances = []  # \Sigma_s^*,  s = 0, ..., p
    # \phi_{s,k}, s = 1, ..., p
    #             k = 1, ..., s+1
    forwards = []
    # \phi_{s,k}^*
    backwards = []

    forward_factors = []   # L_s
    backward_factors = []  # L_s^*,  s = 0, ..., p

    # Ultimately we want to construct the partial autocorrelation matrices
    # Note that this is "1-indexed" in the sense that it stores P_1, ... P_p
    # rather than starting with P_0.
    partial_autocorrelations = []

    # We fill in the entries of phi_{s,k} as follows:
    # [1,1]
    # [2,2], [2,1]
    # [3,3], [3,1], [3,2]
    # ...
    # [p,p], [p,1], ..., [p,p-1]
    # the last row, correctly ordered, should be the same as the coefficient
    # matrices provided in the argument `constrained`
    for s in range(order):  # s = 0, ..., p-1
        prev_forwards = list(forwards)
        prev_backwards = list(backwards)
        forwards = []
        backwards = []

        # Create forward and backwards variances Sigma_s, Sigma*_s
        forward_variance = autocovariances[0].copy()
        backward_variance = autocovariances[0].T.copy()

        for k in range(s):
            forward_variance -= np.dot(prev_forwards[k],
                                       autocovariances[k+1])
            backward_variance -= np.dot(prev_backwards[k],
                                        autocovariances[k+1].T)

        forward_variances.append(forward_variance)
        backward_variances.append(backward_variance)

        # Cholesky factors
        forward_factors.append(
            linalg.cholesky(forward_variances[s], lower=True)
        )
        backward_factors.append(
            linalg.cholesky(backward_variances[s], lower=True)
        )

        # Create the intermediate sum term
        if s == 0:
            # phi_11 = \Gamma_1' \Gamma_0^{-1}
            # phi_11 \Gamma_0 = \Gamma_1'
            # \Gamma_0 phi_11' = \Gamma_1
            forwards.append(linalg.cho_solve(
                (forward_factors[0], True), autocovariances[1]).T)
            # backwards.append(forwards[-1])
            # phi_11_star = \Gamma_1 \Gamma_0^{-1}
            # phi_11_star \Gamma_0 = \Gamma_1
            # \Gamma_0 phi_11_star' = \Gamma_1'
            backwards.append(linalg.cho_solve(
                (backward_factors[0], True), autocovariances[1].T).T)
        else:
            # G := \Gamma_{s+1}' -
            #      \phi_{s,1} \Gamma_s' - .. - \phi_{s,s} \Gamma_1'
            tmp_sum = autocovariances[s+1].T.copy()

            for k in range(s):
                tmp_sum -= np.dot(prev_forwards[k], autocovariances[s-k].T)

            # Create the "last" (k = s+1) matrix
            # Note: this is for k = s+1. However, below we then have to
            # fill in for k = 1, ..., s in order.
            # phi = G Sigma*^{-1}
            # phi Sigma* = G
            # Sigma*' phi' = G'
            # Sigma* phi' = G'
            # (because Sigma* is symmetric)
            forwards.append(linalg.cho_solve(
                (backward_factors[s], True), tmp_sum.T).T)

            # phi = G' Sigma^{-1}
            # phi Sigma = G'
            # Sigma' phi' = G
            # Sigma phi' = G
            # (because Sigma is symmetric)
            backwards.append(linalg.cho_solve(
                (forward_factors[s], True), tmp_sum).T)

        # Create the remaining k = 1, ..., s matrices,
        # only has an effect if s >= 1
        for k in range(s):
            forwards.insert(k, prev_forwards[k] - np.dot(
                forwards[-1], prev_backwards[s-(k+1)]))
            backwards.insert(k, prev_backwards[k] - np.dot(
                backwards[-1], prev_forwards[s-(k+1)]))

        # Partial autocorrelation matrix: P_{s+1}
        # P = L^{-1} phi L*
        # L P = (phi L*)
        partial_autocorrelations.append(linalg.solve_triangular(
            forward_factors[s], np.dot(forwards[s], backward_factors[s]),
            lower=True))

    return partial_autocorrelations


def _compute_multivariate_pacf_from_coefficients(constrained, error_variance,
                                                 order=None, k_endog=None):
    """
    Transform matrices corresponding to a stationary (or invertible) process
    to matrices with singular values less than one.
    Parameters
    ----------
    constrained : array or list
        The coefficients matrices. If a list, should be a list of length
        `order`, where each element is an array sized `k_endog` x `k_endog`. If
        an array, should be the coefficient matrices horizontally concatenated
        and sized `k_endog` x `k_endog * order`.
    error_variance : array
        The variance / covariance matrix of the error term. Should be sized
        `k_endog` x `k_endog`.
    order : integer, optional
        The order of the autoregression.
    k_endog : integer, optional
        The dimension of the data vector.
    Returns
    -------
    pacf : list
        List of first `order` multivariate partial autocorrelations.
    Notes
    -----
    Note that this computes multivariate partial autocorrelations.
    Corresponds to the inverse of Lemma 2.1 in Ansley and Kohn (1986). See
    `unconstrain_stationary_multivariate` for more details.
    Notes
    -----
    Coefficients are assumed to be provided from the VAR model:
    .. math::
        y_t = A_1 y_{t-1} + \dots + A_p y_{t-p} + \varepsilon_t
    """

    if type(constrained) == list:
        order = len(constrained)
        k_endog = constrained[0].shape[0]
    else:
        k_endog, order = constrained.shape
        order //= k_endog

    # Get autocovariances for the process; these are defined to be
    # E z_t z_{t-j}'
    # However, we want E z_t z_{t+j}' = (E z_t z_{t-j}')'
    _acovf = _compute_multivariate_acovf_from_coefficients

    autocovariances = [
        autocovariance.T for autocovariance in
        _acovf(constrained, error_variance, maxlag=order)]

    return _compute_multivariate_pacf_from_autocovariances(autocovariances)


def unconstrain_stationary_multivariate(constrained, error_variance):
    """
    Transform constrained parameters used in likelihood evaluation
    to unconstrained parameters used by the optimizer
    Parameters
    ----------
    constrained : array or list
        Constrained parameters of, e.g., an autoregressive or moving average
        component, to be transformed to arbitrary parameters used by the
        optimizer. If a list, should be a list of length `order`, where each
        element is an array sized `k_endog` x `k_endog`. If an array, should be
        the coefficient matrices horizontally concatenated and sized
        `k_endog` x `k_endog * order`.
    error_variance : array
        The variance / covariance matrix of the error term. Should be sized
        `k_endog` x `k_endog`. This is used as input in the algorithm even if
        is not transformed by it (when `transform_variance` is False).
    Returns
    -------
    unconstrained : array
        Unconstrained parameters used by the optimizer, to be transformed to
        stationary coefficients of, e.g., an autoregressive or moving average
        component. Will match the type of the passed `constrained`
        variable (so if a list was passed, a list will be returned).
    Notes
    -----
    Uses the list representation internally, even if an array is passed.
    References
    ----------
    .. [1] Ansley, Craig F., and Robert Kohn. 1986.
       "A Note on Reparameterizing a Vector Autoregressive Moving Average Model
       to Enforce Stationarity."
       Journal of Statistical Computation and Simulation 24 (2): 99-106.
    """

    from scipy import linalg

    use_list = type(constrained) == list
    if not use_list:
        k_endog, order = constrained.shape
        order //= k_endog

        constrained = [
            constrained[:k_endog, i*k_endog:(i+1)*k_endog]
            for i in range(order)
        ]
    else:
        order = len(constrained)
        k_endog = constrained[0].shape[0]

    # Step 1: convert matrices from the space of stationary
    # coefficient matrices to our "partial autocorrelation matrix" space
    # (matrices with singular values less than one)
    partial_autocorrelations = _compute_multivariate_pacf_from_coefficients(
        constrained, error_variance, order, k_endog)

    # Step 2: convert from arbitrary matrices to those with singular values
    # less than one.
    unconstrained = _unconstrain_sv_less_than_one(
        partial_autocorrelations, order, k_endog)

    if not use_list:
        unconstrained = np.concatenate(unconstrained, axis=1)

    return unconstrained, error_variance


def validate_matrix_shape(name, shape, nrows, ncols, nobs):
    """
    Validate the shape of a possibly time-varying matrix, or raise an exception
    Parameters
    ----------
    name : str
        The name of the matrix being validated (used in exception messages)
    shape : array_like
        The shape of the matrix to be validated. May be of size 2 or (if
        the matrix is time-varying) 3.
    nrows : int
        The expected number of rows.
    ncols : int
        The expected number of columns.
    nobs : int
        The number of observations (used to validate the last dimension of a
        time-varying matrix)
    Raises
    ------
    ValueError
        If the matrix is not of the desired shape.
    """
    ndim = len(shape)

    # Enforce dimension
    if ndim not in [2, 3]:
        raise ValueError('Invalid value for %s matrix. Requires a'
                         ' 2- or 3-dimensional array, got %d dimensions' %
                         (name, ndim))
    # Enforce the shape of the matrix
    if not shape[0] == nrows:
        raise ValueError('Invalid dimensions for %s matrix: requires %d'
                         ' rows, got %d' % (name, nrows, shape[0]))
    if not shape[1] == ncols:
        raise ValueError('Invalid dimensions for %s matrix: requires %d'
                         ' columns, got %d' % (name, ncols, shape[1]))

    # If we don't yet know `nobs`, don't allow time-varying arrays
    if nobs is None and not (ndim == 2 or shape[-1] == 1):
        raise ValueError('Invalid dimensions for %s matrix: time-varying'
                         ' matrices cannot be given unless `nobs` is specified'
                         ' (implicitly when a dataset is bound or else set'
                         ' explicity)' % name)

    # Enforce time-varying array size
    if ndim == 3 and nobs is not None and not shape[-1] in [1, nobs]:
        raise ValueError('Invalid dimensions for time-varying %s'
                         ' matrix. Requires shape (*,*,%d), got %s' %
                         (name, nobs, str(shape)))


def validate_vector_shape(name, shape, nrows, nobs):
    """
    Validate the shape of a possibly time-varying vector, or raise an exception
    Parameters
    ----------
    name : str
        The name of the vector being validated (used in exception messages)
    shape : array_like
        The shape of the vector to be validated. May be of size 1 or (if
        the vector is time-varying) 2.
    nrows : int
        The expected number of rows (elements of the vector).
    nobs : int
        The number of observations (used to validate the last dimension of a
        time-varying vector)
    Raises
    ------
    ValueError
        If the vector is not of the desired shape.
    """
    ndim = len(shape)
    # Enforce dimension
    if ndim not in [1, 2]:
        raise ValueError('Invalid value for %s vector. Requires a'
                         ' 1- or 2-dimensional array, got %d dimensions' %
                         (name, ndim))
    # Enforce the shape of the vector
    if not shape[0] == nrows:
        raise ValueError('Invalid dimensions for %s vector: requires %d'
                         ' rows, got %d' % (name, nrows, shape[0]))

    # If we don't yet know `nobs`, don't allow time-varying arrays
    if nobs is None and not (ndim == 1 or shape[-1] == 1):
        raise ValueError('Invalid dimensions for %s vector: time-varying'
                         ' vectors cannot be given unless `nobs` is specified'
                         ' (implicitly when a dataset is bound or else set'
                         ' explicity)' % name)

    # Enforce time-varying array size
    if ndim == 2 and not shape[1] in [1, nobs]:
        raise ValueError('Invalid dimensions for time-varying %s'
                         ' vector. Requires shape (*,%d), got %s' %
                         (name, nobs, str(shape)))


def reorder_missing_matrix(matrix, missing, reorder_rows=False,
                           reorder_cols=False, is_diagonal=False,
                           inplace=False, prefix=None):
    """
    Reorder the rows or columns of a time-varying matrix where all non-missing
    values are in the upper left corner of the matrix.
    Parameters
    ----------
    matrix : array_like
        The matrix to be reordered. Must have shape (n, m, nobs).
    missing : array_like of bool
        The vector of missing indices. Must have shape (k, nobs) where `k = n`
        if `reorder_rows is True` and `k = m` if `reorder_cols is True`.
    reorder_rows : bool, optional
        Whether or not the rows of the matrix should be re-ordered. Default
        is False.
    reorder_cols : bool, optional
        Whether or not the columns of the matrix should be re-ordered. Default
        is False.
    is_diagonal : bool, optional
        Whether or not the matrix is diagonal. If this is True, must also have
        `n = m`. Default is False.
    inplace : bool, optional
        Whether or not to reorder the matrix in-place.
    prefix : {'s', 'd', 'c', 'z'}, optional
        The Fortran prefix of the vector. Default is to automatically detect
        the dtype. This parameter should only be used with caution.
    Returns
    -------
    reordered_matrix : array_like
        The reordered matrix.
    """
    if prefix is None:
        prefix = find_best_blas_type((matrix,))[0]
    reorder = prefix_reorder_missing_matrix_map[prefix]

    if not inplace:
        matrix = np.copy(matrix, order='F')

    reorder(matrix, np.asfortranarray(missing), reorder_rows, reorder_cols,
            is_diagonal)

    return matrix


def reorder_missing_vector(vector, missing, inplace=False, prefix=None):
    """
    Reorder the elements of a time-varying vector where all non-missing
    values are in the first elements of the vector.
    Parameters
    ----------
    vector : array_like
        The vector to be reordered. Must have shape (n, nobs).
    missing : array_like of bool
        The vector of missing indices. Must have shape (n, nobs).
    inplace : bool, optional
        Whether or not to reorder the matrix in-place. Default is False.
    prefix : {'s', 'd', 'c', 'z'}, optional
        The Fortran prefix of the vector. Default is to automatically detect
        the dtype. This parameter should only be used with caution.
    Returns
    -------
    reordered_vector : array_like
        The reordered vector.
    """
    if prefix is None:
        prefix = find_best_blas_type((vector,))[0]
    reorder = prefix_reorder_missing_vector_map[prefix]

    if not inplace:
        vector = np.copy(vector, order='F')

    reorder(vector, np.asfortranarray(missing))

    return vector


def copy_missing_matrix(A, B, missing, missing_rows=False, missing_cols=False,
                        is_diagonal=False, inplace=False, prefix=None):
    """
    Copy the rows or columns of a time-varying matrix where all non-missing
    values are in the upper left corner of the matrix.
    Parameters
    ----------
    A : array_like
        The matrix from which to copy. Must have shape (n, m, nobs) or
        (n, m, 1).
    B : array_like
        The matrix to copy to. Must have shape (n, m, nobs).
    missing : array_like of bool
        The vector of missing indices. Must have shape (k, nobs) where `k = n`
        if `reorder_rows is True` and `k = m` if `reorder_cols is True`.
    missing_rows : bool, optional
        Whether or not the rows of the matrix are a missing dimension. Default
        is False.
    missing_cols : bool, optional
        Whether or not the columns of the matrix are a missing dimension.
        Default is False.
    is_diagonal : bool, optional
        Whether or not the matrix is diagonal. If this is True, must also have
        `n = m`. Default is False.
    inplace : bool, optional
        Whether or not to copy to B in-place. Default is False.
    prefix : {'s', 'd', 'c', 'z'}, optional
        The Fortran prefix of the vector. Default is to automatically detect
        the dtype. This parameter should only be used with caution.
    Returns
    -------
    copied_matrix : array_like
        The matrix B with the non-missing submatrix of A copied onto it.
    """
    if prefix is None:
        prefix = find_best_blas_type((A, B))[0]
    copy = prefix_copy_missing_matrix_map[prefix]

    if not inplace:
        B = np.copy(B, order='F')

    # We may have been given an F-contiguous memoryview; in that case, we don't
    # want to alter it or convert it to a numpy array
    try:
        if not A.is_f_contig():
            raise ValueError()
    except:
        A = np.asfortranarray(A)

    copy(A, B, np.asfortranarray(missing), missing_rows, missing_cols,
         is_diagonal)

    return B


def copy_missing_vector(a, b, missing, inplace=False, prefix=None):
    """
    Reorder the elements of a time-varying vector where all non-missing
    values are in the first elements of the vector.
    Parameters
    ----------
    a : array_like
        The vector from which to copy. Must have shape (n, nobs) or (n, 1).
    b : array_like
        The vector to copy to. Must have shape (n, nobs).
    missing : array_like of bool
        The vector of missing indices. Must have shape (n, nobs).
    inplace : bool, optional
        Whether or not to copy to b in-place. Default is False.
    prefix : {'s', 'd', 'c', 'z'}, optional
        The Fortran prefix of the vector. Default is to automatically detect
        the dtype. This parameter should only be used with caution.
    Returns
    -------
    copied_vector : array_like
        The vector b with the non-missing subvector of b copied onto it.
    """
    if prefix is None:
        prefix = find_best_blas_type((a, b))[0]
    copy = prefix_copy_missing_vector_map[prefix]

    if not inplace:
        b = np.copy(b, order='F')

    # We may have been given an F-contiguous memoryview; in that case, we don't
    # want to alter it or convert it to a numpy array
    try:
        if not a.is_f_contig():
            raise ValueError()
    except:
        a = np.asfortranarray(a)

    copy(a, b, np.asfortranarray(missing))

    return b


def copy_index_matrix(A, B, index, index_rows=False, index_cols=False,
                      is_diagonal=False, inplace=False, prefix=None):
    """
    Copy the rows or columns of a time-varying matrix where all non-index
    values are in the upper left corner of the matrix.
    Parameters
    ----------
    A : array_like
        The matrix from which to copy. Must have shape (n, m, nobs) or
        (n, m, 1).
    B : array_like
        The matrix to copy to. Must have shape (n, m, nobs).
    index : array_like of bool
        The vector of index indices. Must have shape (k, nobs) where `k = n`
        if `reorder_rows is True` and `k = m` if `reorder_cols is True`.
    index_rows : bool, optional
        Whether or not the rows of the matrix are a index dimension. Default
        is False.
    index_cols : bool, optional
        Whether or not the columns of the matrix are a index dimension.
        Default is False.
    is_diagonal : bool, optional
        Whether or not the matrix is diagonal. If this is True, must also have
        `n = m`. Default is False.
    inplace : bool, optional
        Whether or not to copy to B in-place. Default is False.
    prefix : {'s', 'd', 'c', 'z'}, optional
        The Fortran prefix of the vector. Default is to automatically detect
        the dtype. This parameter should only be used with caution.
    Returns
    -------
    copied_matrix : array_like
        The matrix B with the non-index submatrix of A copied onto it.
    """
    if prefix is None:
        prefix = find_best_blas_type((A, B))[0]
    copy = prefix_copy_index_matrix_map[prefix]

    if not inplace:
        B = np.copy(B, order='F')

    # We may have been given an F-contiguous memoryview; in that case, we don't
    # want to alter it or convert it to a numpy array
    try:
        if not A.is_f_contig():
            raise ValueError()
    except:
        A = np.asfortranarray(A)

    copy(A, B, np.asfortranarray(index), index_rows, index_cols,
         is_diagonal)

    return B


def copy_index_vector(a, b, index, inplace=False, prefix=None):
    """
    Reorder the elements of a time-varying vector where all non-index
    values are in the first elements of the vector.
    Parameters
    ----------
    a : array_like
        The vector from which to copy. Must have shape (n, nobs) or (n, 1).
    b : array_like
        The vector to copy to. Must have shape (n, nobs).
    index : array_like of bool
        The vector of index indices. Must have shape (n, nobs).
    inplace : bool, optional
        Whether or not to copy to b in-place. Default is False.
    prefix : {'s', 'd', 'c', 'z'}, optional
        The Fortran prefix of the vector. Default is to automatically detect
        the dtype. This parameter should only be used with caution.
    Returns
    -------
    copied_vector : array_like
        The vector b with the non-index subvector of b copied onto it.
    """
    if prefix is None:
        prefix = find_best_blas_type((a, b))[0]
    copy = prefix_copy_index_vector_map[prefix]

    if not inplace:
        b = np.copy(b, order='F')

    # We may have been given an F-contiguous memoryview; in that case, we don't
    # want to alter it or convert it to a numpy array
    try:
        if not a.is_f_contig():
            raise ValueError()
    except:
        a = np.asfortranarray(a)

    copy(a, b, np.asfortranarray(index))

    return b




def _marginalize_vector(event_probs, event_conditional_vectors,
        weighted_vectors, marginal_vector, vector_biases, vector_bias_sqrs,
        event_conditional_covs, covs_plus_bias_sqrs,
        weighted_covs_plus_bias_sqrs, marginal_cov):
    r"""
    Generic method, marginalizing random vector's expectation and covariance
    matrix
    Parameters
    ----------
    event_probs : array_like
        Probabilities of the set of the collectively exhaustive events
        :math:`{ A_1, ..., A_n }`. In Kim filter these events are determined by
        Markov switching model regime value.
    event_conditional_vectors : array_like
        (Hereafter let's denote random vector as :math:`\alpha`)
        Vector expectations, conditional on events: :math:`E[ \alpha | A_i ]`.
    weighted_vectors : array_like
        Buffer to store :math:`Pr[ A_i ] * E[ \alpha | A_i ]`.
    marginal_vector : array_like
        Buffer to store the result: :math:`E[ \alpha ]`.
    vector_biases : array_like
        Buffer to store :math:`E[ \alpha ] - E[ \alpha | A_i ]`.
    vector_bias_sqrs : array_like
        Buffer to store :math:`( E[ \alpha ] - E[ \alpha | A_i ]) *
        ( E[ \alpha ] - E[ \alpha | A_i ] )^T`.
    event_conditional_covs : array_like
        Vector covariance matrices, conditional on events:
        :math:`Var[ \alpha | A_i ]`.
    covs_plus_bias_sqrs : array_like
        Buffer to store :math:` Var[ \alpha ] ( E[ \alpha ] -
        E[ \alpha | A_i ]) * ( E[ \alpha ] - E[ \alpha | A_i ] )^T`.
    weighted_covs_plus_bias_sqrs : array_like
        Buffer to store :math:`Pr[ A_i ] * ( Var[ \alpha ] ( E[ \alpha ] -
        E[ \alpha | A_i ]) * ( E[ \alpha ] - E[ \alpha | A_i ] )^T )`.
    marginal_cov : array_like
        Buffer to store the result: :math:`Var[ \alpha ]`.
    """

    # Pr[ A_i ] * E[ \alpha | A_i ]
    np.multiply(event_probs.reshape(-1, 1), event_conditional_vectors,
            out=weighted_vectors)

    # E[ \alpha ] = \sum_{i} Pr[ A_i ] * E[ \alpha | A_i ]
    np.sum(weighted_vectors, axis=0, out=marginal_vector)

    # E[ \alpha ] - E[ \alpha | A_i ]
    np.subtract(marginal_vector, event_conditional_vectors,
            out=vector_biases)

    # ( E[ \alpha ] - E[ \alpha | A_i ] ) *
    # * ( E[ \alpha ] - E[ \alpha | A_i ] )^T
    for i in range(event_probs.shape[0]):
        np.outer(vector_biases[i], vector_biases[i], out=vector_bias_sqrs[i])

    # Var[ \alpha | A_i ] + ( E[ \alpha ] - E[ \alpha | A_i ] ) *
    # * ( E[ \alpha ] - E[ \alpha | A_i ] )^T
    np.add(event_conditional_covs, vector_bias_sqrs, out=covs_plus_bias_sqrs)

    # Pr[ A_i ] * ( Var[ \alpha | A_i ] + ( E[ \alpha ] - E[ \alpha | A_i ] ) *
    # * ( E[ \alpha ] - E[ \alpha | A_i ] )^T )
    np.multiply(event_probs.reshape(-1, 1, 1), covs_plus_bias_sqrs,
            out=weighted_covs_plus_bias_sqrs)

    # Var[ \alpha ] = \sum_{i}
    # Pr[ A_i ] * ( Var[ \alpha | A_i ] + ( E[ \alpha ] - E[ \alpha | A_i ] ) *
    # * ( E[ \alpha ] - E[ \alpha | A_i ] )^T )
    np.sum(weighted_covs_plus_bias_sqrs, axis=0,
            out=marginal_cov)

class SwitchingRepresentation(object):
    r"""
    Markov switching state space representation of a time series process.
    Parameters
    ----------
    k_endog : int
        The number of variables in the process.
    k_states : int
        The dimension of the unobserved state process.
    k_regimes : int
        The number of switching regimes.
    dtype : dtype, optional
        Default datatype of the state space matrices. Default is `np.float64`.
    design : array_like, optional
        The design matrix, :math:`Z`. Default is set to zeros.
    obs_intercept : array_like, optional
        The intercept for the observation equation, :math:`d`. Default is set
        to zeros.
    obs_cov : array_like, optional
        The covariance matrix for the observation equation :math:`H`. Default
        is set to zeros.
    transition : array_like, optional
        The transition matrix, :math:`T`. Default is set to zeros.
    state_intercept : array_like, optional
        The intercept for the transition equation, :math:`c`. Default is set to
        zeros.
    selection : array_like, optional
        The selection matrix, :math:`R`. Default is set to zeros.
    state_cov : array_like, optional
        The covariance matrix for the state equation :math:`Q`. Default is set
        to zeros.
    regime_transition : array_like, optional
        Left stochastic matrix of regime transition probabilities. Default are
        equal switch probabilites.
    **kwargs
        Additional keyword arguments, passed to each KalmanFilter instance,
        representing corresponding regime.
    Attributes
    ----------
    nobs : int
        The number of observations. Initialized after data binding.
    k_endog : int
        The dimension of the observation series.
    k_states : int
        The dimension of the unobserved state process.
    k_regimes : int
        The number of switching regimes.
    regime_filters: array
        `KalmanFilter` instances, corresponding to each regime.
    regime_transition: array
        Left stochastic matrix of regime transition probabilities.
    Notes
    -----
    This class stores `k_regimes` `KalmanFilter` instances and left stochastic
    `regime_transition` matrix. Each Kalman Filter stores representation of its
    regime. Interface is partially copied from `Representation` class.
    A general Markov switching state space model is of the form
    .. math::
        y_t & = Z_{S_t} \alpha_t + d_{S_t} + \varepsilon_t \\
        \alpha_t & = T_{S_t} \alpha_{t-1} + c_{S_t} + R_{S_t} \eta_t \\
    where :math:`y_t` refers to the observation vector at time :math:`t`,
    :math:`\alpha_t` refers to the (unobserved) state vector at time
    :math:`t`, :math:`S_t` refers to the (unobserved) regime at time :math:`t`
    and where the irregular components are defined as
    .. math::
        \varepsilon_t \sim N(0, H_{S_t}) \\
        \eta_t \sim N(0, Q_{S_t}) \\
    The remaining variables (:math:`Z_{S_t}, d_{S_t}, H_{S_t}, T_{S_t},
    c_{S_t}, R_{S_t}, Q_{S_t}`) in the equations are matrices describing
    the process. Their variable names and dimensions are as follows
    Z : `design`          :math:`(k\_regimes \times k\_endog \times k\_states
        \times nobs)`
    d : `obs_intercept`   :math:`(k\_regimes \times k\_endog \times nobs)`
    H : `obs_cov`         :math:`(k\_regimes \times k\_endog \times k\_endog
        \times nobs)`
    T : `transition`      :math:`(k\_regimes \times k\_states \times k\_states
        \times nobs)`
    c : `state_intercept` :math:`(k\_regimes \times k\_states \times nobs)`
    R : `selection`       :math:`(k\_regimes \times k\_states \times k\_posdef
        \times nobs)`
    Q : `state_cov`       :math:`(k\_regimes \times k\_posdef \times k\_posdef
        \times nobs)`
    In the case that one of the matrices is time-invariant (so that, for
    example, :math:`Z_t = Z_{t+1} ~ \forall ~ t`), its last dimension may
    be of size :math:`1` rather than size `nobs`.
    In the case that one of the matrices is the same for all regimes, it
    shouldn't have the first `k_regimes` dimension.
    Unobserved process of switching regimes is described by Markovian law:
    .. math::
        Pr[S_t = i | S_{t-1} = j] = p_{ij} \\
    where :math:`p` is a left stochastic matrix (that is, all its elements are
    non-negative and the sum of elements of each row is 1), referenced as
    `regime_transition`.
    References
    ----------
    .. [1] Kim, Chang-Jin, and Charles R. Nelson. 1999.
        "State-Space Models with Regime Switching:
        Classical and Gibbs-Sampling Approaches with Applications".
        MIT Press Books. The MIT Press.
    """

    # Dimensions of state space matrices in KalmanFilter class (without first
    # `k_regimes` dimension). This is used to detect whether provided matrix is
    # not switching and has to be broadcasted to every regime.
    _per_regime_dims = {
            'design': 3,
            'obs_intercept': 2,
            'obs_cov': 3,
            'transition': 3,
            'state_intercept': 2,
            'selection': 3,
            'state_cov': 3
    }

    def __init__(self, k_endog, k_states, k_regimes, dtype=np.float64,
            design=None, obs_intercept=None, obs_cov=None, transition=None,
            state_intercept=None, selection=None, state_cov=None,
            regime_transition=None, **kwargs):

        # All checks for correct types and dimensions, etc. are delegated to
        # KalmanFilter instances. This class is supposed to accomplish only
        # the high-level logic.

        if k_regimes < 1:
            raise ValueError('Only multiple regimes are available.'
                    'Consider using regular KalmanFilter.')

        self._k_endog = k_endog
        self._k_states = k_states
        self._k_regimes = k_regimes
        self._dtype = dtype

        # Broadcasting matrices, if they are provided in non-switching shape
        design = self._broadcast_per_regime(design,
                self._per_regime_dims['design'])
        obs_intercept = self._broadcast_per_regime(obs_intercept,
                self._per_regime_dims['obs_intercept'])
        obs_cov = self._broadcast_per_regime(obs_cov,
                self._per_regime_dims['obs_cov'])
        transition = self._broadcast_per_regime(transition,
                self._per_regime_dims['transition'])
        state_intercept = self._broadcast_per_regime(state_intercept,
                self._per_regime_dims['state_intercept'])
        selection = self._broadcast_per_regime(selection,
                self._per_regime_dims['selection'])
        state_cov = self._broadcast_per_regime(state_cov,
                self._per_regime_dims['state_cov'])

        # Using Kim and Nelson timing convention is required for Kim filter
        kwargs['alternate_timing'] = True

        # Kalman filters for each regime
        # Also, all low-level checks are carried in these initializations
        self._regime_kalman_filters = [KalmanFilter(k_endog, k_states,
                dtype=dtype, design=design[i], obs_intercept=obs_intercept[i],
                obs_cov=obs_cov[i], transition=transition[i],
                state_intercept=state_intercept[i], selection=selection[i],
                state_cov=state_cov[i], **kwargs) for i in range(k_regimes)]

        # Check and store transition matrix
        self._set_regime_transition(regime_transition)

    def _set_regime_transition(self, regime_transition):
        # This method does some checks of regime transition matrix and stores
        # its value.

        dtype = self._dtype
        k_regimes = self._k_regimes

        # All probabilites are stored and treated by their logarithms
        if regime_transition is not None:

            # If regime transition is provided as list
            regime_transition = np.asarray(regime_transition, dtype=dtype)

            if regime_transition.shape != (k_regimes, k_regimes):
                raise ValueError('Regime transition matrix should have shape'
                        ' (k_regimes, k_regimes)')

            # Regime transition matrix is required to be left stochastic
            if not _is_left_stochastic(regime_transition):
                raise ValueError(
                        'Provided regime transition matrix is not stochastic')

            self._log_regime_transition = np.log(regime_transition)
        else:
            # If regime transition matrix is not provided by user, set all
            # probabilites to one value - `1/k_regimes`.
            self._log_regime_transition = -np.log(k_regimes)

    def _broadcast_per_regime(self, matrix, per_regime_dims):
        # This method checks if provided state space representation matrix is in
        # a non-switching shape and then duplicates matrix for every regime.
        # After that, a version of a matrix for the corresponding regime is
        # passed to `KalmanFilter` initializer.

        k_regimes = self._k_regimes

        # If matrix is not provided, pass `None` to every `KalmanFilter`
        if matrix is None:
            return [None for _ in range(k_regimes)]

        matrix = np.asarray(matrix, dtype=self._dtype)

        # `per_regime_dims` contains a number of dimensions for corresponding
        # matrix in a non-switching case
        if len(matrix.shape) == per_regime_dims:
            return [matrix for _ in range(k_regimes)]

        # In the switching case, matrix has one more (first) dimension of size
        # `k_regimes`
        if matrix.shape[0] != k_regimes:
            raise ValueError('First dimension is not k_regimes')

        return matrix

    def __getitem__(self, key):

        # Check if matrix name or slice is provided
        if type(key) == str:
            get_slice = False
            matrix_name = key
        elif type(key) == tuple:
            get_slice = True
            matrix_name = key[0]
            slice_ = key[1:]
        else:
            raise ValueError('First index must be the name of a valid state' \
                    'space matrix.')

        # Regime transition matrix is stored in this class, so it is treated
        # differently from other matrices.
        if matrix_name == 'regime_transition':
            if get_slice:
                return np.exp(self._log_regime_transition)[slice_]
            else:
                return np.exp(self._log_regime_transition)

        # Check if matrix name belongs to state space matrices set
        if matrix_name not in self._per_regime_dims:
            raise IndexError('"%s" is an invalid state space matrix name.' \
                    % matrix_name)

        # Combine corresponding matrices from all regimes together and return
        # result.
        return np.asarray([regime_filter[key] for regime_filter in \
                self._regime_kalman_filters])

    def __setitem__(self, key, value):
        # When no slice is provided, value can contain a battery with values
        # for every regime. Otherwise, the same value is set to matrix slice
        # of every regime.

        # Check if matrix name or slice is provided
        if type(key) == str:
            set_slice = False
            matrix_name = key
        elif type(key) == tuple:
            set_slice = True
            matrix_name = key[0]
            slice_ = key[1:]
        else:
            raise ValueError('First index must be the name of a valid state' \
                    ' space matrix.')

        # Regime transition matrix is stored in this class, so it is treated
        # differently from other matrices.
        if matrix_name == 'regime_transition':
            if set_slice:

                self._log_regime_transition[slice_] = np.log(value)

                # Check if value doesn't violate left-stochastic feature
                if not _is_left_stochastic(
                        np.exp(self._log_regime_transition)):
                    raise ValueError('Regime transition matrix is not' \
                            ' left-stochastic anymore')
            else:
                self._set_regime_transition(value)
            return

        # Check if matrix name belongs to state space matrices set
        if matrix_name not in self._per_regime_dims:
            raise IndexError('"%s" is an invalid state space matrix name.' \
                    % matrix_name)

        if set_slice:
            # Broadcast one value to every regime
            for regime_filter in self._regime_kalman_filters:
                regime_filter[key] = value
        else:
            # Prepare data to set to every regime
            value = self._broadcast_per_regime(value,
                self._per_regime_dims[matrix_name])

            for regime_filter, regime_value in zip(self._regime_kalman_filters,
                    value):
                regime_filter[key] = regime_value

    @property
    def nobs(self):
        """
        (int) The number of observations. Initialized after data binding.
        Otherwise, will throw `AttributeError` exception.
        """
        return self._nobs

    @property
    def k_regimes(self):
        """
        (int) The number of switching regimes.
        """
        return self._k_regimes

    @property
    def k_endog(self):
        """
        (int) The dimension of the observation series.
        """
        return self._k_endog

    @property
    def k_states(self):
        """
        (int) The dimension of the unobserved state process.
        """
        return self._k_states

    @property
    def dtype(self):
        """
        (dtype) Datatype of currently active representation matrices.
        """
        return self._dtype

    @property
    def regime_filters(self):
        """
        (array) `KalmanFilter` instances, corresponding to each regime.
        """
        return self._regime_kalman_filters

    @property
    def regime_transition(self):
        """
        (array) Left stochastic matrix of regime transition probabilities.
        """
        return np.exp(self._log_regime_transition)

    @regime_transition.setter
    def regime_transition(self, value):
        """
        (array) Left stochastic matrix of regime transition probabilities.
        """
        self._set_regime_transition(value)

    @property
    def initialization(self):
        """
        (str) Kalman filter initalization method. This property handling is
        delegated to `KalmanFilter`. Since diffuse approximation is not
        available for Kim Filter, `initialization` can be `None`, 'stationary'
        or 'known'.
        """
        return self.regime_filters[0].initialization

    @property
    def _complex_endog(self):
        # A flag for complex data, handled by `Representation` class.
        # This is used internally in `MLEModel` class.
        return self.regime_filters[0]._complex_endog

    def bind(self, endog):
        """
        This method propagates the action of the same name among regime filters.
        See `Representation.bind` documentation for details.
        """

        # Copying for the case
        for regime_filter in self._regime_kalman_filters:
            regime_filter.bind(endog)

        # `endog` is the same for every regime filter.
        self.endog = self._regime_kalman_filters[0].endog
        # `nobs` is the same for every regime filter.
        self._nobs = self._regime_kalman_filters[0].nobs

    def initialize_known(self, initial_state, initial_state_cov):
        """
        Initialize the statespace model with known distribution for initial
        state.
        These values are assumed to be known with certainty or else
        filled with parameters during, for example, maximum likelihood
        estimation.
        Parameters
        ----------
        initial_state : array_like
            Known mean of the initial state vector.
            If its shape is `(k_states,)`, then this value is broadcasted to
            all regimes.
            If its shape is `(k_regimes, k_states)`, then it is assumed that
            every regime has its own `initial_state`.
        initial_state_cov : array_like
            Known covariance matrix of the initial state vector.
            If its shape is `(k_states, k_states)`, then this value is
            broadcasted to all regimes.
            If its shape is `(k_regimes, k_states, k_states)`, then it is
            assumed that every regime has its own `initial_state_cov`.
        """

        k_regimes = self._k_regimes
        regime_filters = self._regime_kalman_filters

        # Broadcast matrices, if they are provided in non-switching shape
        initial_state = self._broadcast_per_regime(initial_state, 1)
        initial_state_cov = self._broadcast_per_regime(initial_state_cov, 2)

        # Delegating initialization to `KalmanFilter`
        for i in range(k_regimes):
            regime_filters[i].initialize_known(initial_state[i],
                    initial_state_cov[i])

    def initialize_stationary(self):
        """
        This method propagates the action of the same name among regime filters.
        See `Representation.initialize_stationary` documentation for details.
        """

        for regime_filter in self._regime_kalman_filters:
            regime_filter.initialize_stationary()

    def initialize_known_regime_probs(self, initial_regime_probs):
        """
        Initialize marginal regime distribution at t=0.
        Parameters
        ----------
        initial_regime_probs : array_like
            Array of shape `(k_regimes,)`, representating marginal regime
            distribution at t=0.
        """

        self._initial_regime_logprobs = np.log(initial_regime_probs)

    def initialize_uniform_regime_probs(self):
        """
        Initialize marginal regime distribution at t=0 with uniform
        distribution.
        """

        k_regimes = self._k_regimes
        # All probabilities are set to `1/k_regimes`
        self._initial_regime_logprobs = -np.log(k_regimes)

    def initialize_stationary_regime_probs(self):
        """
        Initialize marginal regime distribution at t=0 with stationary
        distribution of the Markov chain, described by `regime_transition`
        matrix.
        """

        k_regimes = self._k_regimes
        dtype = self._dtype

        # Switch from logarithms to actual values
        regime_transition = np.exp(self._log_regime_transition)

        # Stationary distribution is calculated by solving the system of linear
        # equations.

        # This matrix, multiplied by stationary distribution (in a column
        # shape), gives a column of `k_regimes` zeros and one on the bottom.
        coefficient_matrix = np.vstack((regime_transition - \
                np.identity(k_regimes, dtype=dtype),
                np.ones((1, k_regimes), dtype=dtype)))

        # Solve the system by OLS approach. Multiplication by "dependent
        # variable" column is performed implicitly by selecting the last column.
        candidate = np.linalg.pinv(coefficient_matrix)[:, -1]

        eps = 1e-8

        # Check whether all values are non-negative (by eps)
        if np.any(candidate < -eps):
            raise RuntimeError('Regime switching chain doesn\'t have ' \
                    'a stationary distribution')

        # Set very small negative values to zero
        candidate[candidate < 0] = 0

        # This is required to perform a normalization
        if candidate.sum() < eps:
            raise RuntimeError('Regime switching chain doesn\'t have ' \
                    'a stationary distribution')

        # Normalization, since OLS solution can be imprecise
        candidate /= candidate.sum()

        # Store distribution in the class
        self._initial_regime_logprobs = np.log(candidate)

    @property
    def initial_regime_probs(self):
        """
        Marginal regime distribution at t=0.
        """
        return np.exp(self._initial_regime_logprobs)

class FrozenSwitchingRepresentation(object):
    """
    Frozen Markov switching state space model
    Takes a snapshot of a Markov switching state space model
    Parameters
    ----------
    model : SwitchingRepresentation
        A Markov switching state space representation
    Attributes
    ----------
    dtype : dtype
        Datatype of representation matrices
    nobs : int
        The number of observations. Initialized after data binding.
    k_endog : int
        The dimension of the observation series.
    k_states : int
        The dimension of the unobserved state process.
    k_regimes : int
        The number of switching regimes.
    endog : array
        The observation vector.
    log_regime_transition: array
        Square `k_regimes` x `k_regimes` matrix of regime transition
        log-probabilities.
    regime_representations: array
        `FrozenRepresentation` instances, corresponding to each regime.
    """
    _model_attributes = [
        'model', 'dtype', 'nobs', 'k_endog', 'k_states', 'k_regimes',
        'endog', 'log_regime_transition', 'regime_representations'
    ]
    _attributes = _model_attributes

    def __init__(self, model):

        # Initialize all attributes to None
        for name in self._attributes:
            setattr(self, name, None)

    def update_representation(self, model):
        # Model
        self.model = model

        # Data type
        self.dtype = model.dtype

        # Copy the model dimensions
        self.nobs = model._nobs
        self.k_endog = model._k_endog
        self.k_states = model._k_states
        self.k_regimes = model._k_regimes

        # Save endog data
        self.endog = model.endog

        # Save regime transition matrix
        self.log_regime_transition = model._log_regime_transition

        # Create and store frozen representations of each regime
        self.regime_representations = \
                [FrozenRepresentation(regime_kalman_filter) for \
                regime_kalman_filter in model._regime_kalman_filters]

    def __getitem__(self, key):

        # regime transition matrix is treated differently from other matrices
        if key == 'regime_transition':
            return np.exp(self.log_regime_transition)

        # Check if matrix name belongs to state space matrices set
        if key not in SwitchingRepresentation._per_regime_dims:
            raise IndexError('"%s" is an invalid state space matrix name.' \
                    % key)

        # Combine corresponding matrices from all regimes together and return
        # result.
        return np.asarray([getattr(representation, key) for representation in \
                self.regime_representations])

class _KimFilter(object):

    def __init__(self, model, filter_method=None, inversion_method=None,
            stability_method=None, conserve_memory=None, tolerance=None,
            complex_step=False, nobs=None):

        # This class does all hard filtering work

        self.model = model

        # Check if endogenous data is binded to switching representation
        if not hasattr(model, '_nobs'):
            raise RuntimeError(
                    'No endog data binded. Consider using bind() first.')

        # Initialize filters and save some useful low-level references
        model._initialize_filters(filter_method=filter_method,
            inversion_method=inversion_method,
            stability_method=stability_method, conserve_memory=conserve_memory,
            tolerance=tolerance, complex_step=complex_step)

        # Check if filter initialization is provided
        if any((regime_filter.initialization is None for regime_filter in \
                model._regime_kalman_filters)):
            raise RuntimeError('Statespace model not initialized.')

        if nobs is None:
            self.nobs = model.nobs
        else:
            self.nobs = nobs

    def _hamilton_prediction_step(self, t,
            predicted_prev_and_curr_regime_logprobs):

        # The goal of this step is to calculate Pr[ S_t, S_{t-1} | \psi_{t-1} ]

        model = self.model

        # Filtered regime log-probabilities at previous moment
        if t == 0:
            regime_logprobs = model._initial_regime_logprobs
        else:
            regime_logprobs = self.filtered_regime_logprobs[:, t - 1]

        # Pr[ S_t, S_{t-1} | \psi_{t-1} ] = Pr[ S_t | S_{t-1} ] *
        # * Pr[ S_{t-1} | \psi_{t-1} ]
        np.add(model._log_regime_transition.transpose(),
                regime_logprobs.reshape(-1, 1),
                out=predicted_prev_and_curr_regime_logprobs)

        # Pr[ S_t | \psi_{t-1} ] =
        # = \sum_{S_{t-1}} Pr[ S_t, S_{t-1} | \psi_{t-1} ]
        self.predicted_regime_logprobs[:, t] = logsumexp(
                predicted_prev_and_curr_regime_logprobs, axis=0)

    def _kalman_filter_step(self, t, prev_regime, curr_regime, state_buffer,
            state_cov_buffer, state_batteries, state_cov_batteries,
            prev_and_curr_regime_cond_obs_logprobs, forecast_error_batteries,
            forecast_error_cov_batteries):

        # `SwitchingRepresentation` aggregates `k_regimes` `KalmanFilter`
        # instances. During this step, for every combination (i, j)
        # \beta_{t-1|t-1}^{i} (and P_{t-1|t-1}^{i} respectively) is passed
        # through j-th Kalman filter to produce \beta_{t|t}^{(i,j)},
        # P_{t|t}^{(i,j)} and f( y_t | S_{t-1} = i, S_t = j, \psi_{t-1} ).

        # All heavy-weight computations of this step are delegated to
        # `KalmanFilter` class

        model = self.model

        # Saving useful references of high and low level Kalman filters

        # Low-level Cython filters
        curr_kfilter = model._kfilters[curr_regime]
        prev_kfilter = model._kfilters[prev_regime]
        # Corresponding high-level Python filters
        curr_regime_filter = model._regime_kalman_filters[curr_regime]
        prev_regime_filter = model._regime_kalman_filters[prev_regime]

        # Every `curr_kfilter` does the filtering iteration `k_regimes` times
        # from one time position, so its time pointer has to be corrected every
        # time.
        curr_kfilter.seek(t)

        # `Feeding` \beta_{t-1|t-1}^{i} and P_{t-1|t-1}^{i} to j-th Kalman
        # Filter input.
        if t == 0:
            # Saving previous initialization in temporary buffer
            state_buffer = curr_regime_filter._initial_state
            state_cov_buffer = curr_regime_filter._initial_state_cov
            # High level initialization
            curr_regime_filter.initialize_known(
                    prev_regime_filter._initial_state,
                    prev_regime_filter._initial_state_cov)
            # Low level initialization - this method "pushes" new
            # initialization to Cython Kalman filter
            curr_regime_filter._initialize_state(
                    **model._state_init_kwargs[curr_regime])
        else:
            # Saving previous moment filter data to temporary buffer
            np.copyto(state_buffer, curr_kfilter.filtered_state[:, t - 1])
            np.copyto(state_cov_buffer,
                    curr_kfilter.filtered_state_cov[:, :, t - 1])
            # Subtitution j-th filter filtered data at previous moment by
            # \beta_{t-1|t-1}^{i} and P_{t-1|t-1}^{i}
            np.copyto(np.asarray(curr_kfilter.filtered_state[:, t - 1]),
                    prev_kfilter.filtered_state[:, t - 1])
            np.copyto(np.asarray(curr_kfilter.filtered_state_cov[:, :, t - 1]),
                    prev_kfilter.filtered_state_cov[:, :, t - 1])

        # Do the Kalman filter iteration
        next(curr_kfilter)

        # Putting filtered data from previous moment from temporary buffer back
        # to filter's matrices
        if t == 0:
            curr_regime_filter.initialize_known(state_buffer, state_cov_buffer)
        else:
            np.copyto(np.asarray(curr_kfilter.filtered_state[:, t - 1]),
                    state_buffer)
            np.copyto(np.asarray(curr_kfilter.filtered_state_cov[:, :, t - 1]),
                    state_cov_buffer)

        # Saving \beta_{t|t}^{(i,j)} and P_{t|t}^{(i,j)} in batteries
        np.copyto(state_batteries[prev_regime, curr_regime, :],
                curr_kfilter.filtered_state[:, t])
        np.copyto(state_cov_batteries[prev_regime, curr_regime, :, :],
                curr_kfilter.filtered_state_cov[:, :, t])

        # Saving f( y_t | S_{t-1} = i, S_t = j, \psi_{t-1} )
        prev_and_curr_regime_cond_obs_logprobs[prev_regime, curr_regime] = \
                curr_kfilter.loglikelihood[t]

        np.copyto(forecast_error_batteries[prev_regime, curr_regime, :],
                curr_kfilter.forecast_error[:, t])
        np.copyto(forecast_error_cov_batteries[prev_regime, curr_regime, :, :],
                curr_kfilter.forecast_error_cov[:, :, t])

    def _collapse_forecasts(self, t, predicted_prev_and_curr_regime_logprobs,
            predicted_prev_and_curr_regime_probs, forecast_error_batteries,
            weighted_error_batteries, error_biases, error_bias_sqrs,
            forecast_error_cov_batteries, error_covs_plus_bias_sqrs,
            weighted_error_covs_plus_bias_sqrs):

        # This method calculates \eta_{t|t-1} and f_{t|t-1}, which are primarily
        # used in `FilterResults` and `SwitchingMLEResults` for hypothesis
        # testing.

        model = self.model

        k_endog = model._k_endog

        # Switching from logprobs to probs
        np.exp(predicted_prev_and_curr_regime_logprobs,
                out=predicted_prev_and_curr_regime_probs)

        # Calculate \eta_{t|t-1}, f_{t|t-1}, collapsing \eta_{t|t-1}^{(i,j)} and
        # f_{t|t-1}^{(i,j)} with probability weights
        _marginalize_vector(predicted_prev_and_curr_regime_probs,
                forecast_error_batteries.reshape(-1, k_endog),
                weighted_error_batteries, self.forecast_error[:, t],
                error_biases, error_bias_sqrs,
                forecast_error_cov_batteries.reshape(-1, k_endog, k_endog),
                error_covs_plus_bias_sqrs, weighted_error_covs_plus_bias_sqrs,
                self.forecast_error_cov[:, :, t])

    def _hamilton_filtering_step(self, t,
            predicted_prev_and_curr_regime_logprobs,
            prev_and_curr_regime_cond_obs_logprobs,
            predicted_prev_and_curr_regime_and_obs_logprobs,
            filtered_prev_and_curr_regime_logprobs):

        # This step is related to different probability inferences

        model = self.model

        k_regimes = model._k_regimes

        # f( y_t, S_t, S_{t-1} | \psi_{t-1} ) =
        # = f( y_t | S_t, S_{t-1}, \psi_{t-1} ) *
        # * Pr[ S_t, S_{t-1} | \psi_{t-1} ]
        np.add(prev_and_curr_regime_cond_obs_logprobs,
                predicted_prev_and_curr_regime_logprobs,
                out=predicted_prev_and_curr_regime_and_obs_logprobs)

        # f( y_t | \psi_{t-1} ) = \sum_{S_t, S_{t-1}}
        # f( y_t, S_t, S_{t-1} | \psi_{t-1} )
        obs_loglikelihood = \
                logsumexp(predicted_prev_and_curr_regime_and_obs_logprobs)

        # Saving f( y_t | \psi_{t-1} )
        self.obs_loglikelihoods[t] = obs_loglikelihood

        # Pr[ S_t, S_{t-1} | \psi_{t} ] = f( y_t, S_t, S_{t-1} | \psi_{t-1} ) /
        # / f( y_t | \psi_{t-1} )
        # Condition to avoid -np.inf - (-np.inf) operation
        if obs_loglikelihood != -np.inf:
            np.subtract(predicted_prev_and_curr_regime_and_obs_logprobs,
                    obs_loglikelihood,
                    out=filtered_prev_and_curr_regime_logprobs)
        else:
            filtered_prev_and_curr_regime_logprobs[:, :] = -np.inf

        # Pr[ S_t | \psi_t ] = \sum_{S_{t-1}} Pr[ S_t, S_{t-1} | \psi_{t} ]
        self.filtered_regime_logprobs[:, t] = logsumexp(
                filtered_prev_and_curr_regime_logprobs, axis=0)

    def _approximation_step(self, t, curr_regime,
            filtered_prev_and_curr_regime_logprobs, approx_states,
            filtered_prev_cond_on_curr_regime_logprobs,
            state_batteries, weighted_states, state_biases,
            state_bias_sqrs, state_cov_batteries,
            state_covs_and_state_bias_sqrs,
            weighted_state_covs_and_state_bias_sqrs, approx_state_covs):

        # During this step approximate \beta_{t|t}^{i} and P_{t|t}^{i}
        # are calculated and stored inside Kalman filters' `filtered_state` and
        # `filtered_state_cov` matrices (using these matrices allows not to
        # allocate another heavy arrays but seems to conform name semantics)

        model = self.model

        # Reference to Cython KalmanFilter
        curr_filter = model._kfilters[curr_regime]

        # Zero joint probability indicates that this pair of current and
        # previous regimes is impossible and no need to do further calculations
        if self.filtered_regime_logprobs[curr_regime, t] == -np.inf:
            # Filling \beta_{t-1|t-1}^{i} and P_{t-1|t-1}^{i} with zeros
            # Any value would be alright, since these data is multiplied by zero
            # weight in the next iteration
            approx_states[curr_regime, :] = 0
            approx_state_covs[curr_regime, :, :] = 0

            # Copying data form temporary buffer to Kalman filter matrices
            # There is no need to do it here, but just for consistency
            np.copyto(np.asarray(curr_filter.filtered_state[:, t]),
                    approx_states[curr_regime, :])
            np.copyto(np.asarray(curr_filter.filtered_state_cov[:, :, t]),
                    approx_state_covs[curr_regime, :, :])
            return

        # Pr[ S_{t-1} | S_t, \psi_t ] = Pr[ S_t, S_{t-1} | \psi_t ] /
        # / Pr[ S_t | \psi_t ]
        np.subtract(filtered_prev_and_curr_regime_logprobs[:, curr_regime],
                self.filtered_regime_logprobs[curr_regime, t],
                out=filtered_prev_cond_on_curr_regime_logprobs)

        # Switching from logprobs to probs
        filtered_prev_cond_on_curr_regime_probs = \
                filtered_prev_cond_on_curr_regime_logprobs
        np.exp(filtered_prev_cond_on_curr_regime_logprobs,
                out=filtered_prev_cond_on_curr_regime_probs)

        # Calculate \beta_{t|t}^j, P_{t|t}^j, collapsing \beta_{t|t}^{(i,j)} and
        # P_{t|t}^{(i,j)} with probability weights
        _marginalize_vector(filtered_prev_cond_on_curr_regime_logprobs,
                state_batteries[:, curr_regime, :], weighted_states,
                approx_states[curr_regime, :], state_biases, state_bias_sqrs,
                state_cov_batteries[:, curr_regime, :, :],
                state_covs_and_state_bias_sqrs,
                weighted_state_covs_and_state_bias_sqrs,
                approx_state_covs[curr_regime, :, :])

        # Copying data form temporary buffer to Kalman filter matrices
        np.copyto(np.asarray(curr_filter.filtered_state[:, t]),
                approx_states[curr_regime, :])
        np.copyto(np.asarray(curr_filter.filtered_state_cov[:, :, t]),
                approx_state_covs[curr_regime, :, :])

    def _collapse_states(self, t, filtered_regime_probs,
            approx_states, weighted_states, state_biases, state_bias_sqrs,
            approx_state_covs, state_covs_and_state_bias_sqrs,
            weighted_state_covs_and_state_bias_sqrs):

        # This method calculates \beta_{t|t} and P_{t|t} using \beta_{t|t}^j
        # and P_{t|t}^j.

        # Switching from logprobs to probs
        np.exp(self.filtered_regime_logprobs[:, t], out=filtered_regime_probs)

        # Collapsing \beta_{t|t}^i and P_{t|t}^i to get the result
        _marginalize_vector(filtered_regime_probs, approx_states,
                weighted_states, self.filtered_state[:, t], state_biases,
                state_bias_sqrs, approx_state_covs,
                state_covs_and_state_bias_sqrs,
                weighted_state_covs_and_state_bias_sqrs,
                self.filtered_state_cov[:, :, t])

    def __call__(self):

        # This method is based on section 5 of
        # Kim, Chang-Jin, and Charles R. Nelson. 1999.
        # "State-Space Models with Regime Switching:
        # Classical and Gibbs-Sampling Approaches with Applications".
        # MIT Press Books. The MIT Press.

        # Also, notation in comments follows this section

        model = self.model

        k_endog = model._k_endog
        k_states = model._k_states
        k_regimes = model._k_regimes
        dtype = model._dtype
        nobs = self.nobs

        # Array, storing \ln( f( y_t | \psi_{t-1} ) )
        self.obs_loglikelihoods = np.zeros((nobs,), dtype=dtype)

        # Array, storing \ln( Pr[ S_t | \psi_t ] )
        self.filtered_regime_logprobs = np.zeros((k_regimes, nobs),
                dtype=dtype)
        # Array, storing \ln( Pr[ S_t | \psi_{t-1} ] )
        self.predicted_regime_logprobs = np.zeros((k_regimes, nobs),
                dtype=dtype)

        # Array, storing \beta_{t|t}
        self.filtered_state = np.zeros((k_states, nobs), dtype=dtype)
        # Array, storing P_{t|t}
        self.filtered_state_cov = np.zeros((k_states, k_states, nobs),
                dtype=dtype)

        # Array, storing \eta_{t|t-1}
        self.forecast_error = np.zeros((k_endog, nobs), dtype=dtype)
        # Array, storing f_{t|t-1}
        self.forecast_error_cov = np.zeros((k_endog, k_endog, nobs),
                dtype=dtype)

        # If user didn't specify initialization, try to find stationary regime
        # distribution. If it is not found, use simple uniform distribution.
        if not hasattr(model, '_initial_regime_probs'):
            try:
                model.initialize_stationary_regime_probs()
            except RuntimeError:
                model.initialize_uniform_regime_probs()

        # Allocation of buffers, which are reused during numerous iterations
        # of Kim filtering routine.

        # These buffers are used during Kalman filter step as a temporary
        # location of replaced from filters states.
        state_buffer = np.zeros((k_states,), dtype=dtype)
        state_cov_buffer = np.zeros((k_states, k_states), dtype=dtype)

        # Batteries of \beta_{t|t}^{(i,j)}
        state_batteries = np.zeros((k_regimes, k_regimes, k_states),
                dtype=dtype)
        # Batteries of P_{t|t}^{(i,j)}
        state_cov_batteries = np.zeros((k_regimes, k_regimes, k_states,
                k_states), dtype=dtype)

        # Batteries of \eta_{t|t-1}^{(i,j)}
        forecast_error_batteries = np.zeros((k_regimes, k_regimes, k_endog),
                dtype=dtype)
        # Batteries of f_{t|t-1}^{(i,j)}
        forecast_error_cov_batteries = np.zeros((k_regimes, k_regimes, k_endog,
                k_endog), dtype=dtype)

        # Buffer for Pr[ S_{t-1} = i, S_t = j | \psi_{t-1} ] *
        # * \eta_{t|t-1}^{(i,j)}
        weighted_error_batteries = np.zeros((k_regimes * k_regimes, k_endog),
                dtype=dtype)
        # Buffer for \eta_{t|t-1} - \eta_{t|t-1}^{(i,j)}
        error_biases = np.zeros((k_regimes * k_regimes, k_endog), dtype=dtype)
        # Buffer for ( \eta_{t|t-1} - \eta_{t|t-1}^{(i,j)} ) *
        # * ( \eta_{t|t-1} - \eta_{t|t-1}^{(i,j)} )^T
        error_bias_sqrs = np.zeros((k_regimes * k_regimes, k_endog, k_endog),
                dtype=dtype)
        # Buffer for f_{t|t-1}^{(i,j)} +
        # + ( \eta_{t|t-1} - \eta_{t|t-1}^{(i,j)} ) *
        # * ( \eta_{t|t-1} - \eta_{t|t-1}^{(i,j)} )^T
        error_covs_plus_bias_sqrs = np.zeros((k_regimes * k_regimes, k_endog,
                k_endog), dtype=dtype)
        # Buffer for Pr[ S_{t-1} = i, S_t = j | \psi_{t-1} ] *
        # * ( f_{t|t-1}^{(i,j)} + ( \eta_{t|t-1} - \eta_{t|t-1}^{(i,j)} ) *
        # * ( \eta_{t|t-1} - \eta_{t|t-1}^{(i,j)} )^T )
        weighted_error_covs_plus_bias_sqrs = np.zeros((k_regimes * k_regimes,
                k_endog, k_endog), dtype=dtype)

        # Buffer for \ln( Pr[ S_t, S_{t-1} | \psi_{t-1} ] )
        predicted_prev_and_curr_regime_logprobs = np.zeros((k_regimes,
                k_regimes), dtype=dtype)
        # Buffer for Pr[ S_t, S_{t-1} | \psi_{t-1} ]
        predicted_prev_and_curr_regime_probs = np.zeros((k_regimes, k_regimes),
                dtype=dtype)
        # Buffer for \ln( f( y_t | S_t, S_{t-1}, \psi_{t-1} ) )
        prev_and_curr_regime_cond_obs_logprobs = np.zeros((k_regimes,
                k_regimes), dtype=dtype)
        # Buffer for \ln( f( y_t, S_t, S_{t-1} | \psi_{t-1} ) )
        predicted_prev_and_curr_regime_and_obs_logprobs = np.zeros((k_regimes,
                k_regimes), dtype=dtype)
        # Buffer for \ln( Pr[ S_t, S_{t-1} | \psi_t ] )
        filtered_prev_and_curr_regime_logprobs = np.zeros((k_regimes,
                k_regimes), dtype=dtype)

        # Buffer for \ln( Pr[ S_{t-1} | S_t, \psi_t ] )
        filtered_prev_cond_on_curr_regime_logprobs = np.zeros((k_regimes,),
                dtype=dtype)

        # Buffer for \beta_{t|t}^j
        approx_states = np.zeros((k_regimes, k_states), dtype=dtype)
        # Buffer for P_{t|t}^j
        approx_state_covs = np.zeros((k_regimes, k_states, k_states),
                dtype=dtype)

        # Buffer for Pr[ S_{t-1} = i | S_t = j, \psi_t ] * \beta_{t|t}^{(i,j)}
        # (in `_approximation_step`) and for
        # Pr[ S_t = j | \psi_t ] * \beta_{t|t}^j (in `_collapse_states`)
        weighted_states = np.zeros((k_regimes, k_states), dtype=dtype)

        # Buffer for \beta_{t|t}^j - \beta_{t|t}^{(i,j)}
        # (in `_approximation_step`) and for \beta_{t|t} - \beta_{t|t}^j
        # (in `_collapse_states`)
        state_biases = np.zeros((k_regimes, k_states), dtype=dtype)

        # Buffer for ( \beta_{t|t}^j - \beta_{t|t}^{(i,j)} ) *
        # * ( \beta_{t|t}^j - \beta_{t|t}^{(i,j)} )^T (in `_approximation_step`)
        # and for ( \beta_{t|t} - \beta_{t|t}^j ) *
        # * ( \beta_{t|t} - \beta_{t|t}^j )^T (in `_collapse_states`)
        state_bias_sqrs = np.zeros((k_regimes, k_states, k_states),
                dtype=dtype)

        # Buffer for P_{t|t}^{(i,j)} + ( \beta_{t|t}^j - \beta_{t|t}^{(i,j)} ) *
        # * ( \beta_{t|t}^j - \beta_{t|t}^{(i,j)} )^T (in `_approximation_step`)
        # and for P_{t|t}^j + ( \beta_{t|t} - \beta_{t|t}^j ) *
        # * ( \beta_{t|t} - \beta_{t|t}^j )^T (in `_collapse_states`)
        state_covs_and_state_bias_sqrs = np.zeros((k_regimes, k_states,
                k_states), dtype=dtype)

        # Buffer for Pr[ S_{t-1} = i | S_t = j, \psi_t ] * ( P_{t|t}^{(i,j)} +
        # + ( \beta_{t|t}^j - \beta_{t|t}^{(i,j)} ) *
        # * ( \beta_{t|t}^j - \beta_{t|t}^{(i,j)} )^T )
        # (in `_approximation_step`) and for
        # Pr[ S_t = j | \psi_t ] * ( P_{t|t}^j +
        # + ( \beta_{t|t} - \beta_{t|t}^j ) *
        # * ( \beta_{t|t} - \beta_{t|t}^j )^T ) (in `_collapse_states`)
        weighted_state_covs_and_state_bias_sqrs = np.zeros((k_regimes,
                k_states, k_states), dtype=dtype)

        # Buffer for Pr[ S_t = j | \psi_t ]
        filtered_regime_probs = np.zeros((k_regimes,), dtype=dtype)

        # Iterating over observation period
        for t in range(nobs):
            # Kim filter iteration consists of three consecutive steps: Kalman
            # filter step, Hamilton filter step and Approximation step.
            # Here Hamilton filter step is splitted into two parts: prediction
            # and filtering.

            # To optimize computation time, several buffers are reused over
            # iterations, so no array reallocations required. That's why
            # methods, corresponding to Kim filter phases, have a lot of
            # buffers in their argument lists.

            # Hamilton prediction
            self._hamilton_prediction_step(t,
                    predicted_prev_and_curr_regime_logprobs)

            # Kalman filter
            for prev_regime in range(k_regimes):
                for curr_regime in range(k_regimes):
                    # This condition optimizes calculation time in case of
                    # sparse regime transition  matrix (e.g. for MS AR)
                    if predicted_prev_and_curr_regime_logprobs[prev_regime, \
                            curr_regime] != -np.inf:
                        self._kalman_filter_step(t, prev_regime, curr_regime,
                                state_buffer, state_cov_buffer, state_batteries,
                                state_cov_batteries,
                                prev_and_curr_regime_cond_obs_logprobs,
                                forecast_error_batteries,
                                forecast_error_cov_batteries)

            # Collecting forecast errors
            self._collapse_forecasts(t, predicted_prev_and_curr_regime_logprobs,
                    predicted_prev_and_curr_regime_probs,
                    forecast_error_batteries, weighted_error_batteries,
                    error_biases, error_bias_sqrs, forecast_error_cov_batteries,
                    error_covs_plus_bias_sqrs,
                    weighted_error_covs_plus_bias_sqrs)

            # Hamilton filter
            self._hamilton_filtering_step(t,
                    predicted_prev_and_curr_regime_logprobs,
                    prev_and_curr_regime_cond_obs_logprobs,
                    predicted_prev_and_curr_regime_and_obs_logprobs,
                    filtered_prev_and_curr_regime_logprobs)

            # Approximation
            for curr_regime in range(k_regimes):
                self._approximation_step(t, curr_regime,
                        filtered_prev_and_curr_regime_logprobs, approx_states,
                        filtered_prev_cond_on_curr_regime_logprobs,
                        state_batteries, weighted_states, state_biases,
                        state_bias_sqrs, state_cov_batteries,
                        state_covs_and_state_bias_sqrs,
                        weighted_state_covs_and_state_bias_sqrs,
                        approx_state_covs)

            # Collecting filtering results
            self._collapse_states(t, filtered_regime_probs,
                    approx_states, weighted_states, state_biases,
                    state_bias_sqrs, approx_state_covs,
                    state_covs_and_state_bias_sqrs,
                    weighted_state_covs_and_state_bias_sqrs)

class KimFilterResults(FrozenSwitchingRepresentation):
    """
    Results from applying the Kim filter to a Markov switching state space
    model.
    Parameters
    ----------
    model : SwitchingRepresentation
        A Markov switching state space representation
    Attributes
    ----------
    loglikelihood_burn : int
        The number of initial periods during which
        the loglikelihood is not recorded.
    initial_regime_logprobs : array
        Marginal regime distribution at t=0, array of shape `(k_regimes,)`.
    obs_loglikelihoods : array
        Loglikelihood for each observation associated with the
        statespace model (ignoring loglikelihood_burn).
    filtered_state : array
        State mean at the moment t, conditional on all observations measured
        till the moment t. A `(nobs, k_states)` shaped array.
    filtered_state_cov : array
        State covariance at the moment t, conditional on all observations
        measured till the moment t. A `(nobs, k_states, k_states)` shaped
        array.
    filtered_regime_logprobs : array
        Log-probability of a given regime being active at the moment t,
        conditional on all observations measured till the moment t. A
        `(nobs, k_regimes)` shaped array.
    predicted_regime_logprobs : array
        Log-probability of a given regime being active at the moment t,
        conditional on all observations measured until the moment t. A
        `(nobs, k_regimes)` shaped array.
    filter_method : int
        Bitmask representing the Kalman filtering method (for underlying Kalman
        filters).
    inversion_method : int
        Bitmask representing the method used to invert the forecast error
        covariance matrix (for underlying Kalman filters).
    stability_method : int
        Bitmask representing the methods used to promote numerical stability in
        the Kalman filter recursions (for underlying Kalman filters).
    conserve_memory : int
        Bitmask representing the selected memory conservation method (for
        underlying Kalman filters).
    filter_timing : int
        Whether or not to use the alternate timing convention (for underlying
        Kalman filters).
    tolerance : float
        The tolerance at which the Kalman filter determines convergence to
        steady-state (for underlying Kalman filters).
    """

    _filter_attributes = ['loglikelihood_burn', 'initial_regime_logprobs',
            'obs_loglikelihoods', 'filtered_state', 'filtered_state_cov',
            'filtered_regime_logprobs', 'predicted_regime_logprobs',
            'filter_method', 'inversion_method', 'stability_method',
            'conserve_method', 'filter_timing', 'tolerance']

    _attributes = FrozenSwitchingRepresentation._attributes + \
            _filter_attributes

    def update_representation(self, model):
        """
        Update the results to match a given model
        Parameters
        ----------
        model : SwitchingRepresentation
            The model object from which to take the updated values.
        Notes
        -----
        This method is rarely required except for internal usage.
        """

        super(KimFilterResults, self).update_representation(model)

        # When user didn't define initial regime distribution himself, these
        # values are initialized only during filtering.
        self.initial_regime_logprobs = model._initial_regime_logprobs

    def update_filter(self, kfilter, model):
        """
        Update the filter results
        Parameters
        ----------
        kfilter : _KimFilter
            Object, handling filtering, which to take the updated values from.
        model : KimFilter
            `KimFilter` object with metadata.
        Notes
        -----
        This method is rarely required except for internal usage.
        """

        # Save Kalman filter parameters
        self.filter_method = model.filter_method
        self.inversion_method = model.inversion_method
        self.stability_method = model.stability_method
        self.conserve_memory = model.conserve_memory
        self.filter_timing = model.filter_timing
        self.tolerance = model.tolerance
        self.loglikelihood_burn = model.loglikelihood_burn

        # Save filtering result matrices

        self.obs_loglikelihoods = kfilter.obs_loglikelihoods

        self.filtered_state = kfilter.filtered_state
        self.filtered_state_cov = kfilter.filtered_state_cov

        self.forecasts_error = np.array(kfilter.forecast_error, copy=True)
        self.forecasts = self.forecasts_error + \
                kfilter.model.endog[:, :self.forecasts_error.shape[1]]
        self.forecasts_error_cov = np.array(kfilter.forecast_error_cov,
                copy=True)

        self.filtered_regime_logprobs = kfilter.filtered_regime_logprobs
        self.predicted_regime_logprobs = kfilter.predicted_regime_logprobs

        self._standardized_forecasts_error = None

    @property
    def standardized_forecasts_error(self):
        """
        (array) Standardized forecast errors
        """
        if self._standardized_forecasts_error is None:
            # Logic partially copied from `kalman_filter.FilterResults` class
            from scipy import linalg
            self._standardized_forecasts_error = \
                    np.zeros_like(self.forecasts_error)
            for t in range(self.forecasts_error_cov.shape[2]):
                upper, _ = linalg.cho_factor(self.forecasts_error_cov[:, :, t])
                self._standardized_forecasts_error[:, t] = \
                        linalg.solve_triangular(upper,
                        self.forecasts_error[:, t])

        return self._standardized_forecasts_error

    @property
    def filtered_regime_probs(self):
        """
        (array) Probability of a given regime being active at the moment t,
        conditional on all observations measured till the moment t. A
        `(k_regimes, nobs)` shaped array.
        """
        return np.exp(self.filtered_regime_logprobs)

    @property
    def predicted_regime_probs(self):
        """
        (array) Probability of a given regime being active at the moment t,
        conditional on all observations measured until the moment t. A
        `(k_regimes, nobs)` shaped array.
        """
        return np.exp(self.predicted_regime_logprobs)

    def loglikeobs(self, loglikelihood_burn=0):
        """
        Calculate the loglikelihood for each observation associated with the
        statespace model.
        Parameters
        ----------
        loglikelihood_burn : int
            The number of initial periods during which the loglikelihood is
            not recorded.
        Notes
        -----
        If `loglikelihood_burn` is positive, then the entries in the returned
        loglikelihood vector are set to be zero for those initial time periods.
        Returns
        -------
        loglike : array of float
            Array of loglikelihood values for each observation.
        """

        # If loglikelihood is provided in both `loglikeobs` arguments and
        # constructor, choose bigger one
        loglikelihood_burn = max(loglikelihood_burn, self.loglikelihood_burn)

        loglikelihoods = np.array(self.obs_loglikelihoods)
        loglikelihoods[:loglikelihood_burn] = 0

        return loglikelihoods

    def loglike(self, loglikelihood_burn=0):
        """
        Calculate the loglikelihood associated with the statespace model.
        Parameters
        ----------
        loglikelihood_burn : int
            The number of initial periods during which the loglikelihood is
            not recorded.
        Returns
        -------
        loglike : float
            The joint loglikelihood.
        """

        # If loglikelihood is provided in both `loglike` arguments and
        # constructor, choose bigger one
        loglikelihood_burn = max(loglikelihood_burn, self.loglikelihood_burn)

        # Summarize observation loglikelihoods and return results
        return self.obs_loglikelihoods[loglikelihood_burn:].sum()

    def predict(self, start=None, end=None, dynamic=None, **kwargs):
        """
        In-sample and out-of-sample prediction for state space models generally
        Parameters
        ----------
        start : int, optional
            Zero-indexed observation number at which to start forecasting,
            i.e., the first forecast will be at start.
        end : int, optional
            Zero-indexed observation number at which to end forecasting, i.e.,
            the last forecast will be at end.
        dynamic : int, optional
            This option is added for compatibility, using it will cause
            `NotImplementedError`.
        **kwargs
            If the prediction range is outside of the sample range, any
            of the state space representation matrices that are time-varying
            must have updated values provided for the out-of-sample range.
            For example, of `obs_intercept` is a time-varying component and
            the prediction range extends 10 periods beyond the end of the
            sample, a (`k_regimes` x `k_endog` x 10) matrix must be provided
            with the new intercept values.
        Returns
        -------
        results : PredictionResults
            A PredictionResults object.
        Notes
        -----
        Only one-step-ahead prediction and forecasting are available.
        All prediction is performed by applying the deterministic part of the
        measurement equation using the predicted state variables and ignoring
        the Hamilton filtering step.
        See Also
        --------
        statsmodels.tsa.statespace.kalman_filter.FilterResults.predict
        """

        nobs = self.nobs

        # Raise error, if `dynamic` argument is provided
        if dynamic is not None and dynamic != 0:
            raise NotImplementedError

        if end is None:
            end = nobs

        # Length of prediction periods
        nstatic = nobs
        ndynamic = 0
        nforecast = max(0, end - nobs)

        # If no forecasting needed, the existing result object can be used
        if nforecast == 0:
            return PredictionResults(self, start, end, nstatic, ndynamic,
                    nforecast)

        # Collect representation matrices
        representation = {}
        for name in SwitchingRepresentation._per_regime_dims.keys():
            representation[name] = np.asarray([getattr(x, name) for x in \
                    self.regime_representations])

        warning = ('Model has time-invariant {0} matrix, so the {0}'
                ' argument to `predict` has been ignored.')
        exception = ('Forecasting for models with time-varying {0} matrix'
                ' requires an updated time-varying matrix for the'
                ' period to be forecasted.')

        # Extending representation matrices, if they are not time-invariant
        for name in SwitchingRepresentation._per_regime_dims.keys():
            # If matrix is time invariant, no extension is required
            if representation[name].shape[-1] == 1:
                if name in kwargs:
                    warn(warning.format(name))
            # If it's not, and no extension provided, raise an error
            elif name not in kwargs:
                raise ValueError(exception.format(name))
            # Extend matrix with out-of-sample range
            else:
                mat = self.model._broadcast_per_regime(kwargs[name],
                        SwitchingRepresentation._per_regime_dims[name])
                if mat.shape[:-1] != representation[name].shape[:-1]:
                    raise ValueError(exception.format(name))

                representation = np.c_[representation, mat]

        # Add regime transition matrix to arguments
        representation['regime_transition'] = np.exp(self.log_regime_transition)

        # Extend `endog` array with NaNs
        endog = np.empty((self.k_endog, nforecast))
        endog.fill(np.nan)
        endog = np.asfortranarray(np.c_[self.endog, endog])

        # Other options
        model_kwargs = {
                'filter_method': self.filter_method,
                'inversion_method': self.inversion_method,
                'stability_method': self.stability_method,
                'conserve_memory': self.conserve_memory,
                'filter_timing': self.filter_timing,
                'tolerance': self.tolerance,
                'loglikelihoood_burn': self.loglikelihood_burn
        }

        model_kwargs.update(representation)

        # The size of state covariance matrix is the same for all regimes
        k_posdef = self.regime_representations[0].k_posdef

        # Instantiate the model for forecasting
        model = KimFilter(self.k_endog, self.k_states, self.k_regimes,
                k_posdef=k_posdef, **model_kwargs)

        # Bind the endogenous data
        model.bind(endog)

        # Provide state space initialization
        if self.model.initialization is not None:
            model.initialize_known(
                    [x.initial_state for x in self.regime_representations],
                    [x.initial_state_cov for x in self.regime_representations])

        # Do actual prediction
        results = self._predict(nstatic, nforecast, model)

        return PredictionResults(results, start, end, nstatic, ndynamic,
                nforecast)

    def _predict(self, nstatic, nforecast, model):
        # This method performs actual prediction

        # Notation in comments follows Kim and Nelson book, although the
        # prediction algorithm is not described there.

        dtype = self.dtype
        k_regimes = self.k_regimes
        k_endog = self.k_endog
        k_states = self.k_states

        # One-step-ahead prediction is usual filtering of non-NaN prefix of the
        # data
        filter_results = model.filter(nobs=nstatic)

        # Allocating some useful arrays for prediction

        log_regime_transition = self.log_regime_transition

        # Low-level Kalman filters
        kfilters = model._kfilters

        # High-level Kalman filters
        regime_filters = model.regime_filters

        # Predicted probabilities
        predicted_regime_logprobs = np.zeros((k_regimes, nstatic + nforecast),
                dtype=dtype)

        # First `nstatic` columns of the matrix contain
        # \ln( Pr[ S_t | \psi_{t-1} ] ), t <= T, - one-step-ahead prediction of
        # regime. The remaining part is fulfilled with
        # \ln( Pr[ S_t | \psi_T ] ), where t > T.
        predicted_regime_logprobs[:, :nstatic] = \
                filter_results.predicted_regime_logprobs

        # Batteries of \beta_{t|T}^{(i,j)}, t > T
        predicted_state_batteries = np.zeros((k_regimes, k_regimes, k_states),
                dtype=dtype)

        # Batteries of P_{t|T}^{(i,j)}, t > T
        predicted_state_cov_batteries = np.zeros((k_regimes, k_regimes,
                k_states, k_states), dtype=dtype)

        # Forecasts array
        # First `nstatic` columns of the matrix contain E[ y_t | \psi_{t-1} ],
        # t <= T, - one-step-ahead prediction of observation. The remaining part
        # is fulfilled with E[ y_t | \psi_T ], t > T.
        forecasts = np.zeros((k_endog, nstatic + nforecast), dtype=dtype)
        forecasts[:, :nstatic] = filter_results.forecasts

        # Forecast covariance
        # First `nstatic` columns of the matrix contain f_{t|t-1}, t <= T, -
        # one-step-ahead prediction error covariance. The remaining part is
        # fulfilled with f_{t|T}, t > T - forecast error covariance.
        forecasts_error_cov = np.zeros((k_endog, k_endog, nstatic + nforecast),
                dtype=dtype)
        forecasts_error_cov[:, :, :nstatic] = filter_results.forecasts_error_cov

        # Batteries of E[ y_t | S_t, S_{t-1}, \psi_T ], t > T
        forecast_batteries = np.zeros((k_regimes, k_regimes, k_endog),
                dtype=dtype)

        # Batteries of f_{t|T}^{(i,j)}, t > T
        forecast_cov_batteries = np.zeros((k_regimes, k_regimes, k_endog,
                k_endog), dtype=dtype)

        # These are helping buffers, passed to `_marginalize_vector` function.
        # See `_marginalize_vector` documentation for details.
        weighted_forecasts = np.zeros((k_regimes * k_regimes, k_endog),
                dtype=dtype)
        forecast_biases = np.zeros((k_regimes * k_regimes, k_endog),
                dtype=dtype)
        forecast_bias_sqrs = np.zeros((k_regimes * k_regimes, k_endog, k_endog),
                dtype=dtype)
        forecast_covs_and_bias_sqrs = np.zeros((k_regimes * k_regimes, k_endog,
                k_endog), dtype=dtype)
        weighted_forecast_covs_and_bias_sqrs = np.zeros((k_regimes * k_regimes,
                k_endog, k_endog), dtype=dtype)

        # Buffers for \beta_{t|T}^{j}, t > T
        approx_predicted_states = np.zeros((k_regimes, k_states), dtype=dtype)

        # Buffers for P_{t|T}^{j}, t > T
        approx_predicted_state_covs = np.zeros((k_regimes, k_states, k_states),
                dtype=dtype)

        # Initialize approximate predicted state with \beta_{T|T}^{j} and
        # P_{T|T}^{j}
        for i in range(k_regimes):
            # Scenario, when `nstatic` = `nobs` = 0 is strange, but is handled
            # here
            if nstatic != 0:
                # \beta_{T|T}^{j} and P_{T|T}^{j} are stored in `filtered_state`
                # and `filtered_state_cov` arrays
                np.copyto(approx_predicted_states[i],
                        np.asarray(kfilters[i].filtered_state[:, nstatic - 1]))
                np.copyto(approx_predicted_state_covs[i],
                        np.asarray(kfilters[i].filtered_state_cov[:, :,
                        nstatic - 1]))
            else:
                # In this case T = 0, \beta_{0|0}^{j} and P_{0|0}^{j} is an
                # initialization of the model
                np.copyto(approx_predicted_states[i],
                        regime_filters[i]._initial_state)
                np.copyto(approx_predicted_state_covs[i],
                        regime_filters[i]._initial_state_cov)

        # These are helping buffers, passed to `_marginalize_vector` function.
        # See `_marginalize_vector` documentation for details.
        weighted_states = np.zeros((k_regimes, k_states), dtype=dtype)
        state_biases = np.zeros((k_regimes, k_states), dtype=dtype)
        state_bias_sqrs = np.zeros((k_regimes, k_states, k_states),
                dtype=dtype)
        state_covs_and_state_bias_sqrs = np.zeros((k_regimes, k_states,
                k_states), dtype=dtype)
        weighted_state_covs_and_state_bias_sqrs = np.zeros((k_regimes,
                k_states, k_states), dtype=dtype)

        # Forecasting
        for t in range(nstatic, nstatic + nforecast):

            # Pr[ S_t, S_{t-1} | \psi_T ] = Pr[ S_t | S_{t-1} ] *
            # * Pr[ S_{t-1} | \psi_T ]
            predicted_prev_and_curr_regime_logprobs = \
                    log_regime_transition.transpose() + \
                    predicted_regime_logprobs[:, t - 1].reshape(-1, 1)

            # Pr[ S_t | \psi_T ] = \sum_{S_{t-1}} Pr[ S_t, S_{t-1} | \psi_T ]
            predicted_regime_logprobs[:, t] = logsumexp(
                    predicted_prev_and_curr_regime_logprobs, axis=0)

            # Performing Kalman prediction in underlying Kalman filters
            # Remember, that NaNs in endog data force Kalman filter to do
            # prediction in the `__next__` calls
            for prev_regime in range(k_regimes):
                for curr_regime in range(k_regimes):
                    # This condition optimizes calculation time in case of
                    # sparse regime transition matrix (e.g. for MS AR)
                    if predicted_prev_and_curr_regime_logprobs[prev_regime, \
                            curr_regime] != -np.inf:

                        # References to current Kalman filter
                        curr_regime_filter = regime_filters[curr_regime]
                        curr_kfilter = kfilters[curr_regime]

                        # Pass \beta_{t|T}^{i} and P_{t|T}^{i} to j-th filter
                        # Again, the case of t == 0 is strange and rare
                        if t != 0:
                            np.copyto(np.asarray(curr_kfilter.filtered_state[:,
                                    t - 1]), approx_predicted_states[
                                    prev_regime])
                            np.copyto(np.asarray(
                                    curr_kfilter.filtered_state_cov[:,
                                    :, t - 1]), approx_predicted_state_covs[
                                    prev_regime])
                        else:
                            # If t == 0, use approximate state as initialization
                            curr_regime_filter.initialize_known(
                                    approx_predicted_states[prev_regime],
                                    approx_predicted_state_covs[prev_regime])
                            # Low level initialization - this method "pushes"
                            # new initialization to Cython Kalman filter
                            curr_regime_filter._initialize_state(
                                    **model._state_init_kwargs[curr_regime])

                        # We use every regime filter `k_regimes` times in the
                        # given moment t, so it needs to be moved one step back
                        curr_kfilter.seek(t)

                        # Iteration of prediction
                        next(curr_kfilter)

                        # Kalman filter produces \beta_{t|T}^{(i,j)} and
                        # P_{t|T}^{(i,j)}

                        np.copyto(predicted_state_batteries[prev_regime,
                                curr_regime], np.asarray(
                                curr_kfilter.predicted_state[:, t]))

                        np.copyto(predicted_state_cov_batteries[prev_regime,
                                curr_regime],
                                curr_kfilter.predicted_state_cov[:, :, t])

                        # Use convenient reference
                        predicted_state = predicted_state_batteries[prev_regime,
                                curr_regime].reshape(-1, 1)
                        predicted_state_cov = predicted_state_cov_batteries[
                                prev_regime, curr_regime]

                        # Get some state-space matrices of current regime and
                        # moment

                        if curr_regime_filter.design.shape[-1] == 1:
                            design = curr_regime_filter.design[:, :, 0]
                        else:
                            design = curr_regime_filter.design[:, :, t]

                        if curr_regime_filter.obs_intercept.shape[-1] == 1:
                            obs_intercept = curr_regime_filter.obs_intercept[:,
                                    0]
                        else:
                            obs_intercept = curr_regime_filter.obs_intercept[:,
                                    t]

                        if curr_regime_filter.obs_cov.shape[-1] == 1:
                            obs_cov = curr_regime_filter.obs_cov[:, :, 0]
                        else:
                            obs_cov = curr_regime_filter.obs_cov[:, :, t]

                        # Manually calculate forecast
                        # E[ y_t | S_t, S_{t-1}, \psi_T ] and its covariance
                        # f_{t|T}^{(i,j)}, since Kalman prediction fulfills
                        # corresponding arrays with zero

                        forecast_batteries[prev_regime, curr_regime] = \
                                design.dot(predicted_state) + obs_intercept

                        forecast_cov_batteries[prev_regime, curr_regime] = \
                                design.dot(predicted_state_cov).dot(
                                design.T) + obs_cov

            # Switch from logprobs to probs
            predicted_prev_and_curr_regime_probs = np.exp(
                    predicted_prev_and_curr_regime_logprobs)

            # Collapse E[ y_t | S_t, S_{t-1}, \psi_T ] and f_{t|T}^{(i,j)}
            # to obtain E[ y_t | \psi_T ] and f_{t|T}
            _marginalize_vector(predicted_prev_and_curr_regime_probs.ravel(),
                    forecast_batteries.reshape(-1, k_endog), weighted_forecasts,
                    forecasts[:, t], forecast_biases, forecast_bias_sqrs,
                    forecast_cov_batteries.reshape(-1, k_endog, k_endog),
                    forecast_covs_and_bias_sqrs,
                    weighted_forecast_covs_and_bias_sqrs, forecasts_error_cov[:,
                    :, t])

            # Pr[ S_{t-1} | S_t, \psi_T ] = Pr[ S_t, S_{t-1} | \psi_T ] /
            # / Pr[ S_t | \psi_T ]
            predicted_prev_cond_on_curr_regime_logprobs = \
                    predicted_prev_and_curr_regime_logprobs - \
                    predicted_regime_logprobs[:, t].reshape(1, -1)

            # Switch from logprobs to probs
            predicted_prev_cond_on_curr_regime_probs = np.exp(
                    predicted_prev_cond_on_curr_regime_logprobs)

            # Collapse \beta_{t|T}^{(i,j)} and P_{t|T}^{(i,j)}
            # to obtain \beta_{t|T}^{j} and P_{t|T}^{j}
            for curr_regime in range(k_regimes):
                _marginalize_vector(predicted_prev_cond_on_curr_regime_probs[:,
                        curr_regime], predicted_state_batteries[:, curr_regime,
                        :], weighted_states, approx_predicted_states[
                        curr_regime], state_biases, state_bias_sqrs,
                        predicted_state_cov_batteries[:, curr_regime, :, :],
                        state_covs_and_state_bias_sqrs,
                        weighted_state_covs_and_state_bias_sqrs,
                        approx_predicted_state_covs[curr_regime])

        # Use existing `KimFilterResults` object to substitute corresponding
        # fields with forecasted data
        filter_results.forecasts = forecasts
        filter_results.forecasts_error_cov = forecasts_error_cov
        filter_results.predicted_regime_logprobs = predicted_regime_logprobs

        return filter_results

class KimFilter(SwitchingRepresentation):
    """
    Markov switching state space representation of a time series process, with
    Kim filter
    Parameters
    ----------
    k_endog : int
        The number of variables in the process.
    k_states : int
        The dimension of the unobserved state process.
    k_regimes : int
        The number of switching regimes.
    loglikelihood_burn : int, optional
        The number of initial periods during which the loglikelihood is not
        recorded. Default is 0.
    results_class : class, optional
        Default results class to use to save filtering output. Default is
        `KimFilterResults`. If specified, class must extend from
        `KimFilterResults`.
    **kwargs
        Additional keyword arguments, passed to `SwitchingRepresentation`
        initializer.
    Notes
    -----
    This class extends `SwitchingRepresentation` and performs filtering and
    likelihood estimation.
    See Also
    --------
    _KimFilter
    KimFilterResults
    statsmodels.tsa.statespace.regime_switching.switching_representation. \
    SwitchingRepresentation
    statsmodels.tsa.statespace.kalman_filter.KalmanFilter
    """

    def __init__(self, k_endog, k_states, k_regimes, loglikelihood_burn=0,
            results_class=None, **kwargs):

        # Filter options
        self._loglikelihood_burn = loglikelihood_burn

        # Set results class
        if results_class is not None:
            self._results_class = results_class
        else:
            self._results_class = KimFilterResults

        # Initialize representation
        super(KimFilter, self).__init__(k_endog, k_states, k_regimes, **kwargs)

    def filter(self, results=None, filter_method=None, inversion_method=None,
            stability_method=None, conserve_memory=None, tolerance=None,
            complex_step=False, nobs=None):
        """
        Apply the Kim filter to the Markov switching statespace model
        Parameters
        ----------
        results : class or object, optional
            If a class, then that class is instantiated and returned with the
            result of filtering. It must be a subclass of `KimFilterResults`.
            If an object, then that object is updated with the filtering data.
            Its class should extend `KimFilterResults`.
            If `None`, then a `KimFilterResults` object is returned.
        nobs : int
            If specified, filtering is performed on data prefix of length
            `nobs`. This parameter is usually used internally for prediction.
        Notes
        -----
        `filter_method`, `inversion_method`, `stability_method`,
        `conserve_memory` and `tolerance` keyword arguments are passed to
        `k_regimes` `KalmanFilter` instances, used in Kim filtering routine.
        `filter_timing` doesn't allow modification, because Kim filter
        requires Kim and Nelson timing convention.
        See keyword arguments documentation in
        `statsmodels.tsa.statespace.kalman_filter.KalmanFilter.filter`.
        See Also
        --------
        statsmodels.tsa.statespace.kalman_filter.KalmanFilter.filter
        """

        # Actual calculations are done by `_KimFilter` class. See this class
        # for details.
        kfilter = _KimFilter(self, filter_method=filter_method,
            inversion_method=inversion_method,
            stability_method=stability_method, conserve_memory=conserve_memory,
            tolerance=tolerance, complex_step=complex_step, nobs=nobs)
        kfilter()

        # If results are not provided, use the class from initializer
        # (or default `KimFilterResults` class)
        if results is None:
            results = self._results_class

        # If results is a class, create an instance of this class
        if isinstance(results, type):
            if not issubclass(results, KimFilterResults):
                raise ValueError('Invalid results type.')
            results = results(self)

        # save representation data in results
        results.update_representation(self)
        # Save filtering data in results
        results.update_filter(kfilter, self)

        return results

    def _initialize_filters(self, filter_method=None, inversion_method=None,
            stability_method=None, conserve_memory=None, tolerance=None,
            complex_step=False):

        # This method is used before filtering, see `_KimFilter.__init__` method

        kfilters = []
        state_init_kwargs = []

        # Using Kim and Nelson timing convention is required for Kim filter
        filter_timing = 1

        for regime_filter in self._regime_kalman_filters:
            # Initializating
            prefix = regime_filter._initialize_filter(
                    filter_method=filter_method,
                    inversion_method=inversion_method,
                    stability_method=stability_method,
                    conserve_memory=conserve_memory, tolerance=tolerance,
                    filter_timing=filter_timing)[0]
            kfilters.append(regime_filter._kalman_filters[prefix])

            state_init_kwargs.append({'prefix': prefix,
                    'complex_step': complex_step})
            #regime_filter._initialize_state(prefix=prefix,
            #        complex_step=complex_step)

        # Store Cython filter references in the class
        self._kfilters = kfilters

        # These arguments are stored for `KalmanFilter.initialize_state`
        # method call later. See `_KimFilter._kalman_filter_step` method
        self._state_init_kwargs = state_init_kwargs

    @property
    def loglikelihood_burn(self):
        """
        (int) The number of initial periods during which the loglikelihood is
        not recorded. Default is 0.
        """
        return self._loglikelihood_burn

    @loglikelihood_burn.setter
    def loglikelihood_burn(self, value):
        """
        (int) The number of initial periods during which the loglikelihood is
        not recorded. Default is 0.
        """
        self._loglikelihood_burn = value

    @property
    def initialization(self):
        return self._regime_kalman_filters[0].initialization

    def loglikeobs(self, loglikelihood_burn=0, **kwargs):
        """
        Calculate the loglikelihood for each observation associated with the
        statespace model.
        Parameters
        ----------
        loglikelihood_burn : int
            The number of initial periods during which the loglikelihood is
            not recorded.
        **kwargs
            Additional keyword arguments to pass to the Kim filter. See
            `KimFilter.filter` for more details.
        Notes
        -----
        If `loglikelihood_burn` is positive, then the entries in the returned
        loglikelihood vector are set to be zero for those initial time periods.
        Returns
        -------
        loglike : array of float
            Array of loglikelihood values for each observation.
        """

        # Perform filtering
        kfilter = _KimFilter(self, **kwargs)
        kfilter()

        # If loglikelihood is provided in both `loglikeobs` arguments and
        # constructor (or setter), choose bigger one
        loglikelihood_burn = max(loglikelihood_burn, self._loglikelihood_burn)

        # Copying observations loglikelihoods from `_KimFilter`
        loglikelihoods = np.array(kfilter.obs_loglikelihoods)
        loglikelihoods[:loglikelihood_burn] = 0

        return loglikelihoods

    def loglike(self, loglikelihood_burn=0, **kwargs):
        """
        Calculate the loglikelihood associated with the statespace model.
        Parameters
        ----------
        loglikelihood_burn : int
            The number of initial periods during which the loglikelihood is
            not recorded.
        **kwargs
            Additional keyword arguments to pass to the Kim filter. See
            `KimFilter.filter` for more details.
        Returns
        -------
        loglike : float
            The joint loglikelihood.
        """

        # Perform filtering
        kfilter = _KimFilter(self, **kwargs)
        kfilter()

        # If loglikelihood is provided in both `loglike` arguments and
        # constructor (or setter), choose bigger one
        loglikelihood_burn = max(loglikelihood_burn, self._loglikelihood_burn)

        # Summarize observation loglikelihoods and return results
        return kfilter.obs_loglikelihoods[loglikelihood_burn:].sum()

    @property
    def tolerance(self):
        """
        (float) The tolerance at which the Kalman filter determines convergence
        to steady-state. Handling is delegated to `KalmanFilter`.
        """
        return self.regime_filters[0].tolerance

    @tolerance.setter
    def tolerance(self, value):
        """
        (float) The tolerance at which the Kalman filter determines convergence
        to steady-state. Handling is delegated to `KalmanFilter`.
        """
        for regime_filter in self.regime_filters:
            regime_filter.tolerance = value

    def set_filter_method(self, **kwargs):
        """
        This method propagates the action of the same name among regime filters.
        See `KalmanFilter.set_filter_method` documentation for details.
        """

        for regime_filter in self._regime_kalman_filters:
            regime_filter.set_filter_method(**kwargs)

    @property
    def filter_method(self):
        """
        (int) Filter method, used in underlying Kalman filters
        """
        return self.regime_filters[0].filter_method

    def set_inversion_method(self, **kwargs):
        """
        This method propagates the action of the same name among regime filters.
        See `KalmanFilter.set_inversion_method` documentation for details.
        """

        for regime_filter in self._regime_kalman_filters:
            regime_filter.set_inversion_method(**kwargs)

    @property
    def inversion_method(self):
        """
        (int) Inversion method, used in underlying Kalman filters
        """
        return self.regime_filters[0].inversion_method

    def set_stability_method(self, **kwargs):
        """
        This method propagates the action of the same name among regime filters.
        See `KalmanFilter.set_stability_method` documentation for details.
        """

        for regime_filter in self._regime_kalman_filters:
            regime_filter.set_stability_method(**kwargs)

    @property
    def stability_method(self):
        """
        (int) Stability method, used in underlying Kalman filters
        """
        return self.regime_filters[0].stability_method

    def set_conserve_memory(self, **kwargs):
        """
        This method propagates the action of the same name among regime filters.
        See `KalmanFilter.set_conserve_method` documentation for details.
        """

        for regime_filter in self._regime_kalman_filters:
            regime_filter.set_conserve_memory(**kwargs)

    @property
    def conserve_memory(self):
        """
        (int) Conserve memory property, used in underlying Kalman filters
        """
        return self.regime_filters[0].conserve_memory

    @property
    def filter_timing(self):
        """
        (int) Filter timing, used in underlying Kalman filters
        """
        return self.regime_filters[0].filter_timing

class _KimSmoother(object):

    def __init__(self, model, filtered_regime_logprobs,
            predicted_regime_logprobs, regime_partition):

        # This class does smoothing work

        # Save reference to state space representation
        self.model = model

        # Save probabilities which are used during smoothing
        self.filtered_regime_logprobs = filtered_regime_logprobs
        self.predicted_regime_logprobs = predicted_regime_logprobs

        # Save partition
        self.regime_partition = regime_partition

    def __call__(self):

        # This method is based on chapter 5.3 of
        # Kim, Chang-Jin, and Charles R. Nelson. 1999.
        # "State-Space Models with Regime Switching:
        # Classical and Gibbs-Sampling Approaches with Applications".
        # MIT Press Books. The MIT Press.

        # Notation in comments follows this chapter

        # Method calculates Pr[ S_t | \psi_T ] and Pr[ S_t, S_{t+1} | \psi_T ]

        model = self.model
        regime_partition = self.regime_partition

        dtype = model._dtype
        nobs = model._nobs
        k_regimes = model._k_regimes

        # If partition is provided, its elements are treated as regimes.
        if regime_partition is None:
            # Create references, used during smoothing
            log_transition = model._log_regime_transition
            filtered_regime_logprobs = self.filtered_regime_logprobs
            predicted_regime_logprobs = self.predicted_regime_logprobs
            partition_size = k_regimes
        else:
            # Get transition probability matrix for partition (or raise error,
            # if partition doesn't form Markov chain)
            transition_probs = regime_partition.get_transition_probabilities(
                    np.exp(model._log_regime_transition))
            log_transition = np.log(transition_probs)

            # Allocating partition probabilities array
            filtered_regime_logprobs = np.zeros((regime_partition.size, nobs),
                    dtype=dtype)
            predicted_regime_logprobs = np.zeros((regime_partition.size, nobs),
                    dtype=dtype)

            # Collapsing values to get probabilities of subsets, forming
            # partition
            for i in range(regime_partition.size):
                mask = regime_partition.get_mask(i)
                filtered_regime_logprobs[i, :] = logsumexp(
                        self.filtered_regime_logprobs[mask, :], axis=0)
                predicted_regime_logprobs[i, :] = logsumexp(
                        self.predicted_regime_logprobs[mask, :], axis=0)

            partition_size = regime_partition.size

        # Allocation of result

        # Pr[ S_t | \psi_T ]
        self.smoothed_regime_logprobs = np.zeros((partition_size, nobs),
                dtype=dtype)
        # Pr[ S_t, S_{t+1} | \psi_T ]
        self.smoothed_curr_and_next_regime_logprobs = np.zeros((partition_size,
                partition_size, nobs - 1), dtype=dtype)

        # Allocation of buffers, reused during iterations of smoothing

        # Pr[ S_t, S_{t+1} | \psi_t ]
        predicted_curr_and_next_regime_logprobs = np.zeros((partition_size,
                partition_size), dtype=dtype)
        # Pr[ S_t | S_{t+1}, \psi_t ]
        filtered_curr_regime_cond_on_next_logprobs = np.zeros((partition_size,
                partition_size), dtype=dtype)

        # Initialization of smoothing
        self.smoothed_regime_logprobs[:, -1] = filtered_regime_logprobs[:, -1]

        # Backward pass iterations
        for t in range(nobs - 2, -1, -1):

            # Pr[ S_t, S_{t+1} | \psi_t ] = Pr[ S_t | \psi_t ] *
            # Pr[ S_{t+1} | S_t ]
            np.add(log_transition.transpose(),
                    filtered_regime_logprobs[:, t].reshape(-1, 1),
                    out=predicted_curr_and_next_regime_logprobs)

            # Pr[ S_t | S_{t+1}, \psi_t ] = Pr[ S_t, S_{t+1} | \psi_t ] /
            # Pr[ S_{t+1} | \psi_t ]
            for i in range(partition_size):
                # Condition to avoid -np.inf - (-np.inf) operation
                if predicted_regime_logprobs[i, t + 1] != -np.inf:
                    np.subtract(predicted_curr_and_next_regime_logprobs[:, i],
                            predicted_regime_logprobs[i, t + 1],
                            out=filtered_curr_regime_cond_on_next_logprobs[:,
                            i])
                else:
                    filtered_curr_regime_cond_on_next_logprobs[:, i] = -np.inf

            # Pr[ S_t, S_{t+1} | \psi_T ] \approx Pr[ S_{t+1} | \psi_T ] * \
            # Pr[ S_t | S_{t+1}, \psi_t ]
            np.add(self.smoothed_regime_logprobs[:, t + 1].reshape(1, -1),
                    filtered_curr_regime_cond_on_next_logprobs,
                    out=self.smoothed_curr_and_next_regime_logprobs[:, :, t])

            # Pr[ S_t | \psi_T ] = \sum_{S_{t+1}} Pr[ S_t, S_{t+1} | \psi_T ]
            self.smoothed_regime_logprobs[:, t] = logsumexp(
                    self.smoothed_curr_and_next_regime_logprobs[:, :, t], axis=1)

class KimSmoother(KimFilter):
    """
    Markov switching state space representation of a time series process, with
    Kim filter and smoother
    Parameters
    ----------
    k_endog : int
        The number of variables in the process.
    k_states : int
        The dimension of the unobserved state process.
    k_regimes : int
        The number of switching regimes.
    results_class : class, optional
        Default results class to use to save filtering output. Default is
        `KimSmootherResults`. If specified, class must extend from
        `KimSmootherResults`.
    **kwargs
        Additional keyword arguments, passed to superclass initializer.
    Notes
    -----
    This class extends `KimFilter` and performs Kim smoothing.
    See Also
    --------
    _KimSmoother
    KimSmootherResults
    statsmodels.tsa.statespace.regime_switching.kim_filter.KimFilter
    """

    def __init__(self, k_endog, k_states, k_regimes, results_class=None, **kwargs):

        # If no `results_class` provided, set it to default value
        if results_class is None:
            results_class = KimSmootherResults

        super(KimSmoother, self).__init__(k_endog, k_states, k_regimes,
                results_class=results_class, **kwargs)

    def smooth(self, results=None, run_filter=True, regime_partition=None,
            **kwargs):
        """
        Apply the Kim smoother to the Markov switching statespace model.
        Parameters
        ----------
        results : class or object, optional
            If a class, then that class is instantiated and returned with the
            result of filtering. It must be a subclass of `KimFilterResults`.
            If an object, then that object is updated with the filtering data.
            Its class should extend `KimFilterResults`.
            If `None`, then a `KimFilterResults` object is returned.
        run_filter : bool, optional
            Whether or not to run the Kim filter prior to smoothing. Default is
            `True`.
        regime_partition : RegimePartition, optional
            Partition of regimes set, forming a Markov chain. If this is
            specified, smoothing is performed towards elements of partition as
            regimes. See `tools.RegimePartition` for details about partitions.
        **kwargs
            Additional keyword arguments, passed to `filter` method, if
            filtering happens.
        Notes
        -----
        Smoothing is impossible without preliminary filtering, so make sure
        that you've chosen `run_filter=True` option or provided `results`
        object with filtering results.
        Since smoothing is approximate, applying smoothing to regimes and than
        collapsing values to form partition smoothed probabilities gives
        different result from that achieved by treating partition probabilities
        inside smoothing iterations. This is why `regime_partition` can be
        specified.
        See example of `regime_partition` option usage in
        `tests.test_ms_ar_hamilton1989.TestHamilton1989_Smoothing`.
        Returns
        -------
        KimSmootherResults
        See Also
        --------
        statsmodels.tsa.statespace.regime_switching.tools.RegimePartition
        statsmodels.tsa.statespace.regime_switching.tests.\
        test_ms_ar_hamilton1989.TestHamilton1989_Smoothing
        """

        if run_filter:
            # Run filtering first
            results = self.filter(results=results, **kwargs)
        elif results is None or isinstance(results, type) or \
                results.filtered_regime_logprobs is None or \
                results.predicted_regime_logprobs is None:
            # Raise exception, if no filtering happened before.
            raise ValueError(
                    'Can\'t perform smoothing without filtering first')

        # Check if given `results` instance is valid
        if not isinstance(results, KimSmootherResults):
            raise ValueError('Invalid results type.')

        # Actual calculations are done by `_KimSmoother` class. See this class
        # for details.
        smoother = _KimSmoother(self, results.filtered_regime_logprobs,
                results.predicted_regime_logprobs, regime_partition)
        smoother()

        # Save smoothing data in results
        results.update_smoother(smoother)

        return results

class KimSmootherResults(KimFilterResults):
    """
    Results from applying the Kim smoother and filter to a Markov switching
    state space model.
    Parameters
    ----------
    model : SwitchingRepresentation
        A Markov switching state space representation
    Attributes
    ----------
    smoothed_regime_logprobs : array_like
        Smoothed log-probabilities of given regime being active at given moment.
        `(nobs, k_regimes)`-shaped array.
    smoothed_curr_and_next_regime_logprobs : array_like
        Smoothed log-probabilities of two given regimes being active at given
        and previous moment.
        `(nobs, k_regimes, k_regimes)`-shaped array.
    """

    _smoother_attributes = ['smoothed_regime_logprobs',
            'smoothed_curr_and_next_regime_logprobs']

    _attributes = KimFilterResults._attributes + _smoother_attributes

    def update_smoother(self, smoother):
        """
        Update the smoothing results
        Parameters
        ----------
        smoother : _KimSmoother
            Object, handling smoothing, which to take updated values from.
        Notes
        -----
        This method is rarely required except for internal usage.
        """

        self.smoothed_regime_logprobs = smoother.smoothed_regime_logprobs
        self.smoothed_curr_and_next_regime_logprobs = \
                smoother.smoothed_curr_and_next_regime_logprobs

    @property
    def smoothed_regime_probs(self):
        """
        (array) Smoothed probabilities of given regime being active at given
        moment.
        """
        return np.exp(self.smoothed_regime_logprobs)

    @property
    def smoothed_curr_and_next_regime_probs(self):
        """
        (array) Smoothed log-probabilities of two given regimes being active at
        given and previous moment.
        """
        return np.exp(self.smoothed_curr_and_next_regime_logprobs.values)

class MLEModel(tsbase.TimeSeriesModel):
    r"""
    State space model for maximum likelihood estimation
    Parameters
    ----------
    endog : array_like
        The observed time-series process :math:`y`
    k_states : int
        The dimension of the unobserved state process.
    exog : array_like, optional
        Array of exogenous regressors, shaped nobs x k. Default is no
        exogenous regressors.
    dates : array-like of datetime, optional
        An array-like object of datetime objects. If a Pandas object is given
        for endog, it is assumed to have a DateIndex.
    freq : str, optional
        The frequency of the time-series. A Pandas offset or 'B', 'D', 'W',
        'M', 'A', or 'Q'. This is optional if dates are given.
    **kwargs
        Keyword arguments may be used to provide default values for state space
        matrices or for Kalman filtering options. See `Representation`, and
        `KalmanFilter` for more details.
    Attributes
    ----------
    ssm : KalmanFilter
        Underlying state space representation.
    Notes
    -----
    This class wraps the state space model with Kalman filtering to add in
    functionality for maximum likelihood estimation. In particular, it adds
    the concept of updating the state space representation based on a defined
    set of parameters, through the `update` method or `updater` attribute (see
    below for more details on which to use when), and it adds a `fit` method
    which uses a numerical optimizer to select the parameters that maximize
    the likelihood of the model.
    The `start_params` `update` method must be overridden in the
    child class (and the `transform` and `untransform` methods, if needed).
    See Also
    --------
    MLEResults
    statsmodels.tsa.statespace.kalman_filter.KalmanFilter
    statsmodels.tsa.statespace.representation.Representation
    """

    def __init__(self, endog, k_states, exog=None, dates=None, freq=None,
                 **kwargs):
        # Initialize the model base
        super(MLEModel, self).__init__(endog=endog, exog=exog,
                                       dates=dates, freq=freq,
                                       missing='none')

        # Store kwargs to recreate model
        self._init_kwargs = kwargs

        # Prepared the endog array: C-ordered, shape=(nobs x k_endog)
        self.endog, self.exog = self.prepare_data()

        # Dimensions
        self.nobs = self.endog.shape[0]
        self.k_states = k_states

        # Initialize the state-space representation
        self.initialize_statespace(**kwargs)

    def prepare_data(self):
        """
        Prepare data for use in the state space representation
        """
        endog = np.array(self.data.orig_endog, order='C')
        exog = self.data.orig_exog
        if exog is not None:
            exog = np.array(exog)

        # Base class may allow 1-dim data, whereas we need 2-dim
        if endog.ndim == 1:
            endog.shape = (endog.shape[0], 1)  # this will be C-contiguous

        return endog, exog

    def initialize_statespace(self, **kwargs):
        """
        Initialize the state space representation
        Parameters
        ----------
        **kwargs
            Additional keyword arguments to pass to the state space class
            constructor.
        """
        # (Now self.endog is C-ordered and in long format (nobs x k_endog). To
        # get F-ordered and in wide format just need to transpose)
        endog = self.endog.T

        # Instantiate the state space object
        self.ssm = SimulationSmoother(endog.shape[0], self.k_states, **kwargs)
        # Bind the data to the model
        self.ssm.bind(endog)

        # Other dimensions, now that `ssm` is available
        self.k_endog = self.ssm.k_endog

    def __setitem__(self, key, value):
        return self.ssm.__setitem__(key, value)

    def __getitem__(self, key):
        return self.ssm.__getitem__(key)

    def set_filter_method(self, filter_method=None, **kwargs):
        """
        Set the filtering method
        The filtering method controls aspects of which Kalman filtering
        approach will be used.
        Parameters
        ----------
        filter_method : integer, optional
            Bitmask value to set the filter method to. See notes for details.
        **kwargs
            Keyword arguments may be used to influence the filter method by
            setting individual boolean flags. See notes for details.
        Notes
        -----
        This method is rarely used. See the corresponding function in the
        `KalmanFilter` class for details.
        """
        self.ssm.set_filter_method(filter_method, **kwargs)

    def set_inversion_method(self, inversion_method=None, **kwargs):
        """
        Set the inversion method
        The Kalman filter may contain one matrix inversion: that of the
        forecast error covariance matrix. The inversion method controls how and
        if that inverse is performed.
        Parameters
        ----------
        inversion_method : integer, optional
            Bitmask value to set the inversion method to. See notes for
            details.
        **kwargs
            Keyword arguments may be used to influence the inversion method by
            setting individual boolean flags. See notes for details.
        Notes
        -----
        This method is rarely used. See the corresponding function in the
        `KalmanFilter` class for details.
        """
        self.ssm.set_inversion_method(inversion_method, **kwargs)

    def set_stability_method(self, stability_method=None, **kwargs):
        """
        Set the numerical stability method
        The Kalman filter is a recursive algorithm that may in some cases
        suffer issues with numerical stability. The stability method controls
        what, if any, measures are taken to promote stability.
        Parameters
        ----------
        stability_method : integer, optional
            Bitmask value to set the stability method to. See notes for
            details.
        **kwargs
            Keyword arguments may be used to influence the stability method by
            setting individual boolean flags. See notes for details.
        Notes
        -----
        This method is rarely used. See the corresponding function in the
        `KalmanFilter` class for details.
        """
        self.ssm.set_stability_method(stability_method, **kwargs)

    def set_conserve_memory(self, conserve_memory=None, **kwargs):
        """
        Set the memory conservation method
        By default, the Kalman filter computes a number of intermediate
        matrices at each iteration. The memory conservation options control
        which of those matrices are stored.
        Parameters
        ----------
        conserve_memory : integer, optional
            Bitmask value to set the memory conservation method to. See notes
            for details.
        **kwargs
            Keyword arguments may be used to influence the memory conservation
            method by setting individual boolean flags.
        Notes
        -----
        This method is rarely used. See the corresponding function in the
        `KalmanFilter` class for details.
        """
        self.ssm.set_conserve_memory(conserve_memory, **kwargs)

    def set_smoother_output(self, smoother_output=None, **kwargs):
        """
        Set the smoother output
        The smoother can produce several types of results. The smoother output
        variable controls which are calculated and returned.
        Parameters
        ----------
        smoother_output : integer, optional
            Bitmask value to set the smoother output to. See notes for details.
        **kwargs
            Keyword arguments may be used to influence the smoother output by
            setting individual boolean flags.
        Notes
        -----
        This method is rarely used. See the corresponding function in the
        `KalmanSmoother` class for details.
        """
        self.ssm.set_smoother_output(smoother_output, **kwargs)

    def initialize_known(self, initial_state, initial_state_cov):
        self.ssm.initialize_known(initial_state, initial_state_cov)

    def initialize_approximate_diffuse(self, variance=None):
        self.ssm.initialize_approximate_diffuse(variance)

    def initialize_stationary(self):
        self.ssm.initialize_stationary()

    @property
    def initialization(self):
        return self.ssm.initialization

    @property
    def initial_variance(self):
        return self.ssm.initial_variance

    @initial_variance.setter
    def initial_variance(self, value):
        self.ssm.initial_variance = value

    @property
    def loglikelihood_burn(self):
        return self.ssm.loglikelihood_burn

    @loglikelihood_burn.setter
    def loglikelihood_burn(self, value):
        self.ssm.loglikelihood_burn = value

    @property
    def tolerance(self):
        return self.ssm.tolerance

    @tolerance.setter
    def tolerance(self, value):
        self.ssm.tolerance = value

    def fit(self, start_params=None, transformed=True,
            cov_type='opg', cov_kwds=None, method='lbfgs', maxiter=5,
            full_output=1, disp=5, callback=None, return_params=False,
            optim_score=None, optim_complex_step=None, optim_hessian=None,
            flags=None, **kwargs):
        """
        Fits the model by maximum likelihood via Kalman filter.
        Parameters
        ----------
        start_params : array_like, optional
            Initial guess of the solution for the loglikelihood maximization.
            If None, the default is given by Model.start_params.
        transformed : boolean, optional
            Whether or not `start_params` is already transformed. Default is
            True.
        cov_type : str, optional
            The `cov_type` keyword governs the method for calculating the
            covariance matrix of parameter estimates. Can be one of:
            - 'opg' for the outer product of gradient estimator
            - 'oim' for the observed information matrix estimator, calculated
              using the method of Harvey (1989)
            - 'approx' for the observed information matrix estimator,
              calculated using a numerical approximation of the Hessian matrix.
            - 'robust' for an approximate (quasi-maximum likelihood) covariance
              matrix that may be valid even in the presense of some
              misspecifications. Intermediate calculations use the 'oim'
              method.
            - 'robust_approx' is the same as 'robust' except that the
              intermediate calculations use the 'approx' method.
            - 'none' for no covariance matrix calculation.
        cov_kwds : dict or None, optional
            A dictionary of arguments affecting covariance matrix computation.
            **opg, oim, approx, robust, robust_approx**
            - 'approx_complex_step' : boolean, optional - If True, numerical
              approximations are computed using complex-step methods. If False,
              numerical approximations are computed using finite difference
              methods. Default is True.
            - 'approx_centered' : boolean, optional - If True, numerical
              approximations computed using finite difference methods use a
              centered approximation. Default is False.
        method : str, optional
            The `method` determines which solver from `scipy.optimize`
            is used, and it can be chosen from among the following strings:
            - 'newton' for Newton-Raphson, 'nm' for Nelder-Mead
            - 'bfgs' for Broyden-Fletcher-Goldfarb-Shanno (BFGS)
            - 'lbfgs' for limited-memory BFGS with optional box constraints
            - 'powell' for modified Powell's method
            - 'cg' for conjugate gradient
            - 'ncg' for Newton-conjugate gradient
            - 'basinhopping' for global basin-hopping solver
            The explicit arguments in `fit` are passed to the solver,
            with the exception of the basin-hopping solver. Each
            solver has several optional arguments that are not the same across
            solvers. See the notes section below (or scipy.optimize) for the
            available arguments and for the list of explicit arguments that the
            basin-hopping solver supports.
        maxiter : int, optional
            The maximum number of iterations to perform.
        full_output : boolean, optional
            Set to True to have all available output in the Results object's
            mle_retvals attribute. The output is dependent on the solver.
            See LikelihoodModelResults notes section for more information.
        disp : boolean, optional
            Set to True to print convergence messages.
        callback : callable callback(xk), optional
            Called after each iteration, as callback(xk), where xk is the
            current parameter vector.
        return_params : boolean, optional
            Whether or not to return only the array of maximizing parameters.
            Default is False.
        optim_score : {'harvey', 'approx'} or None, optional
            The method by which the score vector is calculated. 'harvey' uses
            the method from Harvey (1989), 'approx' uses either finite
            difference or complex step differentiation depending upon the
            value of `optim_complex_step`, and None uses the built-in gradient
            approximation of the optimizer. Default is None. This keyword is
            only relevant if the optimization method uses the score.
        optim_complex_step : bool, optional
            Whether or not to use complex step differentiation when
            approximating the score; if False, finite difference approximation
            is used. Default is True. This keyword is only relevant if
            `optim_score` is set to 'harvey' or 'approx'.
        optim_hessian : {'opg','oim','approx'}, optional
            The method by which the Hessian is numerically approximated. 'opg'
            uses outer product of gradients, 'oim' uses the information
            matrix formula from Harvey (1989), and 'approx' uses numerical
            approximation. This keyword is only relevant if the
            optimization method uses the Hessian matrix.
        **kwargs
            Additional keyword arguments to pass to the optimizer.
        Returns
        -------
        MLEResults
        See also
        --------
        statsmodels.base.model.LikelihoodModel.fit
        MLEResults
        """
        if start_params is None:
            start_params = self.start_params
            transformed = True

        # Update the score method
        if optim_score is None and method == 'lbfgs':
            kwargs.setdefault('approx_grad', True)
            kwargs.setdefault('epsilon', 1e-5)
        elif optim_score is None:
            optim_score = 'approx'

        # Check for complex step differentiation
        if optim_complex_step is None:
            optim_complex_step = not self.ssm._complex_endog
        elif optim_complex_step and self.ssm._complex_endog:
            raise ValueError('Cannot use complex step derivatives when data'
                             ' or parameters are complex.')

        # Unconstrain the starting parameters
        if transformed:
            start_params = self.untransform_params(np.array(start_params))

        # Maximum likelihood estimation
        if flags is None:
            flags = {}
        flags.update({
            'transformed': False,
            'score_method': optim_score,
            'approx_complex_step': optim_complex_step
        })
        if optim_hessian is not None:
            flags['hessian_method'] = optim_hessian
        fargs = (flags,)
        mlefit = super(MLEModel, self).fit(start_params, method=method,
                                           fargs=fargs,
                                           maxiter=maxiter,
                                           full_output=full_output,
                                           disp=disp, callback=callback,
                                           skip_hessian=True, **kwargs)

        # Just return the fitted parameters if requested
        if return_params:
            return self.transform_params(mlefit.params)
        # Otherwise construct the results class if desired
        else:
            res = self.smooth(mlefit.params, transformed=False,
                              cov_type=cov_type, cov_kwds=cov_kwds)

            res.mlefit = mlefit
            res.mle_retvals = mlefit.mle_retvals
            res.mle_settings = mlefit.mle_settings

            return res

    def filter(self, params, transformed=True, complex_step=False,
               cov_type=None, cov_kwds=None, return_ssm=False,
               results_class=None, results_wrapper_class=None, **kwargs):
        """
        Kalman filtering
        Parameters
        ----------
        params : array_like
            Array of parameters at which to evaluate the loglikelihood
            function.
        transformed : boolean, optional
            Whether or not `params` is already transformed. Default is True.
        return_ssm : boolean,optional
            Whether or not to return only the state space output or a full
            results object. Default is to return a full results object.
        cov_type : str, optional
            See `MLEResults.fit` for a description of covariance matrix types
            for results object.
        cov_kwds : dict or None, optional
            See `MLEResults.get_robustcov_results` for a description required
            keywords for alternative covariance estimators
        **kwargs
            Additional keyword arguments to pass to the Kalman filter. See
            `KalmanFilter.filter` for more details.
        """
        params = np.array(params, ndmin=1)

        if not transformed:
            params = self.transform_params(params)
        self.update(params, transformed=True, complex_step=complex_step)

        # Save the parameter names
        self.data.param_names = self.param_names

        if complex_step:
            kwargs['inversion_method'] = INVERT_UNIVARIATE | SOLVE_LU

        # Get the state space output
        result = self.ssm.filter(complex_step=complex_step, **kwargs)

        # Wrap in a results object
        if not return_ssm:
            result_kwargs = {}
            if cov_type is not None:
                result_kwargs['cov_type'] = cov_type
            if cov_kwds is not None:
                result_kwargs['cov_kwds'] = cov_kwds

            if results_class is None:
                results_class = MLEResults
            if results_wrapper_class is None:
                results_wrapper_class = MLEResultsWrapper

            result = results_wrapper_class(
                results_class(self, params, result, **result_kwargs)
            )

        return result

    def smooth(self, params, transformed=True, complex_step=False,
               cov_type=None, cov_kwds=None, return_ssm=False,
               results_class=None, results_wrapper_class=None, **kwargs):
        """
        Kalman smoothing
        Parameters
        ----------
        params : array_like
            Array of parameters at which to evaluate the loglikelihood
            function.
        transformed : boolean, optional
            Whether or not `params` is already transformed. Default is True.
        return_ssm : boolean,optional
            Whether or not to return only the state space output or a full
            results object. Default is to return a full results object.
        cov_type : str, optional
            See `MLEResults.fit` for a description of covariance matrix types
            for results object.
        cov_kwds : dict or None, optional
            See `MLEResults.get_robustcov_results` for a description required
            keywords for alternative covariance estimators
        **kwargs
            Additional keyword arguments to pass to the Kalman filter. See
            `KalmanFilter.filter` for more details.
        """
        params = np.array(params, ndmin=1)

        if not transformed:
            params = self.transform_params(params)
        self.update(params, transformed=True, complex_step=complex_step)

        # Save the parameter names
        self.data.param_names = self.param_names

        if complex_step:
            kwargs['inversion_method'] = INVERT_UNIVARIATE | SOLVE_LU

        # Get the state space output
        result = self.ssm.smooth(complex_step=complex_step, **kwargs)

        # Wrap in a results object
        if not return_ssm:
            result_kwargs = {}
            if cov_type is not None:
                result_kwargs['cov_type'] = cov_type
            if cov_kwds is not None:
                result_kwargs['cov_kwds'] = cov_kwds

            if results_class is None:
                results_class = MLEResults
            if results_wrapper_class is None:
                results_wrapper_class = MLEResultsWrapper

            result = results_wrapper_class(
                results_class(self, params, result, **result_kwargs)
            )

        return result

    def loglike(self, params, *args, **kwargs):
        """
        Loglikelihood evaluation
        Parameters
        ----------
        params : array_like
            Array of parameters at which to evaluate the loglikelihood
            function.
        transformed : boolean, optional
            Whether or not `params` is already transformed. Default is True.
        **kwargs
            Additional keyword arguments to pass to the Kalman filter. See
            `KalmanFilter.filter` for more details.
        Notes
        -----
        [1]_ recommend maximizing the average likelihood to avoid scale issues;
        this is done automatically by the base Model fit method.
        References
        ----------
        .. [1] Koopman, Siem Jan, Neil Shephard, and Jurgen A. Doornik. 1999.
           Statistical Algorithms for Models in State Space Using SsfPack 2.2.
           Econometrics Journal 2 (1): 107-60. doi:10.1111/1368-423X.00023.
        See Also
        --------
        update : modifies the internal state of the state space model to
                 reflect new params
        """
        # We need to handle positional arguments in two ways, in case this was
        # called by a Scipy optimization routine
        if len(args) > 0:
            argnames = ['transformed', 'complex_step']
            # the fit() method will pass a dictionary
            if isinstance(args[0], dict):
                flags = args[0]
            # otherwise, a user may have just used positional arguments...
            else:
                flags = dict(zip(argnames, args))
            transformed = flags.get('transformed', True)
            complex_step = flags.get('complex_step', True)

            for name, value in flags.items():
                if name in kwargs:
                    raise TypeError("loglike() got multiple values for keyword"
                                    " argument '%s'" % name)
        else:
            transformed = kwargs.pop('transformed', True)
            complex_step = kwargs.pop('complex_step', True)

        if not transformed:
            params = self.transform_params(params)

        self.update(params, transformed= True, complex_step=complex_step)

        if complex_step:
            kwargs['inversion_method'] = INVERT_UNIVARIATE | SOLVE_LU

        loglike = self.ssm.loglike(complex_step=complex_step, **kwargs)

        # Koopman, Shephard, and Doornik recommend maximizing the average
        # likelihood to avoid scale issues, but the averaging is done
        # automatically in the base model `fit` method
        return loglike

    def loglikeobs(self, params, transformed=True, complex_step=False,
                   **kwargs):
        """
        Loglikelihood evaluation
        Parameters
        ----------
        params : array_like
            Array of parameters at which to evaluate the loglikelihood
            function.
        transformed : boolean, optional
            Whether or not `params` is already transformed. Default is True.
        **kwargs
            Additional keyword arguments to pass to the Kalman filter. See
            `KalmanFilter.filter` for more details.
        Notes
        -----
        [1]_ recommend maximizing the average likelihood to avoid scale issues;
        this is done automatically by the base Model fit method.
        References
        ----------
        .. [1] Koopman, Siem Jan, Neil Shephard, and Jurgen A. Doornik. 1999.
           Statistical Algorithms for Models in State Space Using SsfPack 2.2.
           Econometrics Journal 2 (1): 107-60. doi:10.1111/1368-423X.00023.
        See Also
        --------
        update : modifies the internal state of the Model to reflect new params
        """
        if not transformed:
            params = self.transform_params(params)

        # If we're using complex-step differentiation, then we can't use
        # Cholesky factorization
        if complex_step:
            kwargs['inversion_method'] = INVERT_UNIVARIATE | SOLVE_LU

        self.update(params, transformed=True, complex_step=complex_step)

        return self.ssm.loglikeobs(complex_step=complex_step, **kwargs)

    def simulation_smoother(self, simulation_output=None, **kwargs):
        r"""
        Retrieve a simulation smoother for the state space model.
        Parameters
        ----------
        simulation_output : int, optional
            Determines which simulation smoother output is calculated.
            Default is all (including state and disturbances).
        **kwargs
            Additional keyword arguments, used to set the simulation output.
            See `set_simulation_output` for more details.
        Returns
        -------
        SimulationSmoothResults
        """
        return self.ssm.simulation_smoother(
            simulation_output=simulation_output, **kwargs)

    def _forecasts_error_partial_derivatives(self, params, transformed=True,
                                             approx_complex_step=None,
                                             approx_centered=False,
                                             res=None, **kwargs):
        params = np.array(params, ndmin=1)

        # We can't use complex-step differentiation with non-transformed
        # parameters
        if approx_complex_step is None:
            approx_complex_step = transformed
        if not transformed and approx_complex_step:
            raise ValueError("Cannot use complex-step approximations to"
                             " calculate the observed_information_matrix"
                             " with untransformed parameters.")

        # If we're using complex-step differentiation, then we can't use
        # Cholesky factorization
        if approx_complex_step:
            kwargs['inversion_method'] = INVERT_UNIVARIATE | SOLVE_LU

        # Get values at the params themselves
        if res is None:
            self.update(params, transformed=transformed,
                        complex_step=approx_complex_step)
            res = self.ssm.filter(complex_step=approx_complex_step, **kwargs)

        # Setup
        n = len(params)

        # Compute partial derivatives w.r.t. forecast error and forecast
        # error covariance
        partials_forecasts_error = (
            np.zeros((self.k_endog, self.nobs, n))
        )
        partials_forecasts_error_cov = (
            np.zeros((self.k_endog, self.k_endog, self.nobs, n))
        )
        if approx_complex_step:
            epsilon = _get_epsilon(params, 2, None, n)
            increments = np.identity(n) * 1j * epsilon

            for i, ih in enumerate(increments):
                self.update(params + ih, transformed=transformed,
                            complex_step=True)
                _res = self.ssm.filter(complex_step=True, **kwargs)

                partials_forecasts_error[:, :, i] = (
                    _res.forecasts_error.imag / epsilon[i]
                )

                partials_forecasts_error_cov[:, :, :, i] = (
                    _res.forecasts_error_cov.imag / epsilon[i]
                )
        elif not approx_centered:
            epsilon = _get_epsilon(params, 2, None, n)
            ei = np.zeros((n,), float)
            for i in range(n):
                ei[i] = epsilon[i]
                self.update(params + ei, transformed=transformed,
                            complex_step=False)
                _res = self.ssm.filter(complex_step=False, **kwargs)

                partials_forecasts_error[:, :, i] = (
                    _res.forecasts_error - res.forecasts_error) / epsilon[i]

                partials_forecasts_error_cov[:, :, :, i] = (
                    _res.forecasts_error_cov -
                    res.forecasts_error_cov) / epsilon[i]
                ei[i] = 0.0
        else:
            epsilon = _get_epsilon(params, 3, None, n) / 2.
            ei = np.zeros((n,), float)
            for i in range(n):
                ei[i] = epsilon[i]

                self.update(params + ei, transformed=transformed,
                            complex_step=False)
                _res1 = self.ssm.filter(complex_step=False, **kwargs)

                self.update(params - ei, transformed=transformed,
                            complex_step=False)
                _res2 = self.ssm.filter(complex_step=False, **kwargs)

                partials_forecasts_error[:, :, i] = (
                    (_res1.forecasts_error - _res2.forecasts_error) /
                    (2 * epsilon[i]))

                partials_forecasts_error_cov[:, :, :, i] = (
                    (_res1.forecasts_error_cov - _res2.forecasts_error_cov) /
                    (2 * epsilon[i]))

                ei[i] = 0.0

        return partials_forecasts_error, partials_forecasts_error_cov

    def observed_information_matrix(self, params, transformed=True,
                                    approx_complex_step=None,
                                    approx_centered=False, **kwargs):
        """
        Observed information matrix
        Parameters
        ----------
        params : array_like, optional
            Array of parameters at which to evaluate the loglikelihood
            function.
        **kwargs
            Additional keyword arguments to pass to the Kalman filter. See
            `KalmanFilter.filter` for more details.
        Notes
        -----
        This method is from Harvey (1989), which shows that the information
        matrix only depends on terms from the gradient. This implementation is
        partially analytic and partially numeric approximation, therefore,
        because it uses the analytic formula for the information matrix, with
        numerically computed elements of the gradient.
        References
        ----------
        Harvey, Andrew C. 1990.
        Forecasting, Structural Time Series Models and the Kalman Filter.
        Cambridge University Press.
        """
        params = np.array(params, ndmin=1)

        # Setup
        n = len(params)

        # We can't use complex-step differentiation with non-transformed
        # parameters
        if approx_complex_step is None:
            approx_complex_step = transformed
        if not transformed and approx_complex_step:
            raise ValueError("Cannot use complex-step approximations to"
                             " calculate the observed_information_matrix"
                             " with untransformed parameters.")

        # Get values at the params themselves
        self.update(params, transformed=transformed,
                    complex_step=approx_complex_step)
        # If we're using complex-step differentiation, then we can't use
        # Cholesky factorization
        if approx_complex_step:
            kwargs['inversion_method'] = INVERT_UNIVARIATE | SOLVE_LU
        res = self.ssm.filter(complex_step=approx_complex_step, **kwargs)
        dtype = self.ssm.dtype

        # Save this for inversion later
        inv_forecasts_error_cov = res.forecasts_error_cov.copy()

        partials_forecasts_error, partials_forecasts_error_cov = (
            self._forecasts_error_partial_derivatives(
                params, transformed=transformed,
                approx_complex_step=approx_complex_step,
                approx_centered=approx_centered, res=res, **kwargs))

        # Compute the information matrix
        tmp = np.zeros((self.k_endog, self.k_endog, self.nobs, n), dtype=dtype)

        information_matrix = np.zeros((n, n), dtype=dtype)
        for t in range(self.ssm.loglikelihood_burn, self.nobs):
            inv_forecasts_error_cov[:, :, t] = (
                np.linalg.inv(res.forecasts_error_cov[:, :, t])
            )
            for i in range(n):
                tmp[:, :, t, i] = np.dot(
                    inv_forecasts_error_cov[:, :, t],
                    partials_forecasts_error_cov[:, :, t, i]
                )
            for i in range(n):
                for j in range(n):
                    information_matrix[i, j] += (
                        0.5 * np.trace(np.dot(tmp[:, :, t, i],
                                              tmp[:, :, t, j]))
                    )
                    information_matrix[i, j] += np.inner(
                        partials_forecasts_error[:, t, i],
                        np.dot(inv_forecasts_error_cov[:, :, t],
                               partials_forecasts_error[:, t, j])
                    )
        return information_matrix / (self.nobs - self.ssm.loglikelihood_burn)

    def opg_information_matrix(self, params, transformed=True,
                               approx_complex_step=None, **kwargs):
        """
        Outer product of gradients information matrix
        Parameters
        ----------
        params : array_like, optional
            Array of parameters at which to evaluate the loglikelihood
            function.
        **kwargs
            Additional arguments to the `loglikeobs` method.
        References
        ----------
        Berndt, Ernst R., Bronwyn Hall, Robert Hall, and Jerry Hausman. 1974.
        Estimation and Inference in Nonlinear Structural Models.
        NBER Chapters. National Bureau of Economic Research, Inc.
        """
        # We can't use complex-step differentiation with non-transformed
        # parameters
        if approx_complex_step is None:
            approx_complex_step = transformed
        if not transformed and approx_complex_step:
            raise ValueError("Cannot use complex-step approximations to"
                             " calculate the observed_information_matrix"
                             " with untransformed parameters.")

        score_obs = self.score_obs(params, transformed=transformed,
                                   approx_complex_step=approx_complex_step,
                                   **kwargs).transpose()
        return (
            np.inner(score_obs, score_obs) /
            (self.nobs - self.ssm.loglikelihood_burn)
        )

    def _score_complex_step(self, params, **kwargs):
        # the default epsilon can be too small
        # inversion_method = INVERT_UNIVARIATE | SOLVE_LU
        epsilon = _get_epsilon(params, 2., None, len(params))
        kwargs['transformed'] = True
        kwargs['complex_step'] = True
        return approx_fprime_cs(params, self.loglike, epsilon=epsilon,
                                kwargs=kwargs)

    def _score_finite_difference(self, params, approx_centered=False,
                                 **kwargs):
        kwargs['transformed'] = True
        return approx_fprime(params, self.loglike, kwargs=kwargs,
                             centered=approx_centered)

    def _score_harvey(self, params, approx_complex_step=True, **kwargs):
        score_obs = self._score_obs_harvey(
            params, approx_complex_step=approx_complex_step, **kwargs)
        return np.sum(score_obs, axis=0)

    def _score_obs_harvey(self, params, approx_complex_step=True,
                          approx_centered=False, **kwargs):
        """
        Score
        Parameters
        ----------
        params : array_like, optional
            Array of parameters at which to evaluate the loglikelihood
            function.
        **kwargs
            Additional keyword arguments to pass to the Kalman filter. See
            `KalmanFilter.filter` for more details.
        Notes
        -----
        This method is from Harvey (1989), section 3.4.5
        References
        ----------
        Harvey, Andrew C. 1990.
        Forecasting, Structural Time Series Models and the Kalman Filter.
        Cambridge University Press.
        """
        params = np.array(params, ndmin=1)
        n = len(params)

        # Get values at the params themselves
        self.update(params, transformed=True, complex_step=approx_complex_step)
        if approx_complex_step:
            kwargs['inversion_method'] = INVERT_UNIVARIATE | SOLVE_LU
        res = self.ssm.filter(complex_step=approx_complex_step, **kwargs)
        dtype = self.ssm.dtype

        # Get forecasts error partials
        partials_forecasts_error, partials_forecasts_error_cov = (
            self._forecasts_error_partial_derivatives(
                params, transformed=True,
                approx_complex_step=approx_complex_step,
                approx_centered=approx_centered, res=res, **kwargs))

        # Compute partial derivatives w.r.t. likelihood function
        partials = np.zeros((self.nobs, n))
        k_endog = self.k_endog
        for t in range(self.nobs):
            for i in range(n):
                inv_forecasts_error_cov = np.linalg.inv(
                    res.forecasts_error_cov[:, :, t])
                partials[t, i] += np.trace(np.dot(
                    np.dot(inv_forecasts_error_cov,
                           partials_forecasts_error_cov[:, :, t, i]),
                    (np.eye(k_endog) -
                     np.dot(inv_forecasts_error_cov,
                            np.outer(res.forecasts_error[:, t],
                                     res.forecasts_error[:, t])))))
                # 2 * dv / di * F^{-1} v_t
                # where x = F^{-1} v_t or F x = v
                partials[t, i] += 2 * np.dot(
                    partials_forecasts_error[:, t, i],
                    np.dot(inv_forecasts_error_cov, res.forecasts_error[:, t]))

        return -partials / 2.

    def score(self, params, *args, **kwargs):
        """
        Compute the score function at params.
        Parameters
        ----------
        params : array_like
            Array of parameters at which to evaluate the score.
        *args, **kwargs
            Additional arguments to the `loglike` method.
        Returns
        ----------
        score : array
            Score, evaluated at `params`.
        Notes
        -----
        This is a numerical approximation, calculated using first-order complex
        step differentiation on the `loglike` method.
        Both \*args and \*\*kwargs are necessary because the optimizer from
        `fit` must call this function and only supports passing arguments via
        \*args (for example `scipy.optimize.fmin_l_bfgs`).
        """
        params = np.array(params, ndmin=1)

        # We were given one positional argument if this was called by a Scipy
        # optimization routine
        if len(args) > 0:
            argnames = ['transformed', 'method', 'approx_complex_step',
                        'approx_centered']
            # the fit() method will pass a dictionary
            if isinstance(args[0], dict):
                flags = args[0]
                flags['method'] = flags.get('score_method', 'approx')
            # otherwise, a user may have just used positional arguments...
            else:
                flags = dict(zip(argnames, args))
            transformed = flags.get('transformed', True)
            method = flags.get('method', 'approx')
            approx_complex_step = flags.get('approx_complex_step', None)
            approx_centered = flags.get('approx_centered', True)

            for name, value in flags.items():
                if name in kwargs:
                    raise TypeError("score() got multiple values for keyword"
                                    " argument '%s'" % name)
        else:
            transformed = kwargs.pop('transformed', True)
            method = kwargs.pop('method', 'approx')
            approx_complex_step = kwargs.pop('approx_complex_step', None)
            approx_centered = kwargs.pop('approx_centered', False)

        if approx_complex_step is None:
            approx_complex_step = not self.ssm._complex_endog
        if approx_complex_step and self.ssm._complex_endog:
            raise ValueError('Cannot use complex step derivatives when data'
                             ' or parameters are complex.')

        if not transformed:
            transform_score = self.transform_jacobian(params)
            params = self.transform_params(params)

        if method == 'harvey':
            score = self._score_harvey(
                params, approx_complex_step=approx_complex_step, **kwargs)
        elif method == 'approx' and approx_complex_step:
            score = self._score_complex_step(params, **kwargs)
        elif method == 'approx':
            score = self._score_finite_difference(
                params, approx_centered=approx_centered, **kwargs)
        else:
            raise NotImplementedError('Invalid score method.')

        if not transformed:
            score = np.dot(transform_score, score)

        return score

    def score_obs(self, params, method='approx', transformed=True,
                  approx_complex_step=None, approx_centered=False, **kwargs):
        """
        Compute the score per observation, evaluated at params
        Parameters
        ----------
        params : array_like
            Array of parameters at which to evaluate the score.
        **kwargs
            Additional arguments to the `loglike` method.
        Returns
        ----------
        score : array (nobs, k_vars)
            Score per observation, evaluated at `params`.
        Notes
        -----
        This is a numerical approximation, calculated using first-order complex
        step differentiation on the `loglikeobs` method.
        """
        params = np.array(params, ndmin=1)

        if not transformed and approx_complex_step:
            raise ValueError("Cannot use complex-step approximations to"
                             " calculate the score at each observation"
                             " with untransformed parameters.")

        if approx_complex_step is None:
            approx_complex_step = not self.ssm._complex_endog
        if approx_complex_step and self.ssm._complex_endog:
            raise ValueError('Cannot use complex step derivatives when data'
                             ' or parameters are complex.')

        if method == 'harvey':
            score = self._score_obs_harvey(
                params, transformed=transformed,
                approx_complex_step=approx_complex_step, **kwargs)
        elif method == 'approx' and approx_complex_step:
            # the default epsilon can be too small
            epsilon = _get_epsilon(params, 2., None, len(params))
            kwargs['complex_step'] = True
            kwargs['transformed'] = True
            score = approx_fprime_cs(params, self.loglikeobs, epsilon=epsilon,
                                     kwargs=kwargs)
        elif method == 'approx':
            kwargs['transformed'] = transformed
            score = approx_fprime(params, self.loglikeobs, kwargs=kwargs,
                                  centered=approx_centered)
        else:
            raise NotImplementedError('Invalid scoreobs method.')

        return score

    def hessian(self, params, *args, **kwargs):
        """
        Hessian matrix of the likelihood function, evaluated at the given
        parameters
        Parameters
        ----------
        params : array_like
            Array of parameters at which to evaluate the hessian.
        *args, **kwargs
            Additional arguments to the `loglike` method.
        Returns
        -------
        hessian : array
            Hessian matrix evaluated at `params`
        Notes
        -----
        This is a numerical approximation.
        Both \*args and \*\*kwargs are necessary because the optimizer from
        `fit` must call this function and only supports passing arguments via
        \*args (for example `scipy.optimize.fmin_l_bfgs`).
        """
        # We were given one positional argument if this was called by a Scipy
        # optimization routine
        if len(args) > 0:
            argnames = ['transformed', 'method', 'approx_complex_step',
                        'approx_centered']
            # the fit() method will pass a dictionary
            if isinstance(args[0], dict):
                flags = args[0]
                flags['method'] = flags.get('hessian_method', 'approx')
            # otherwise, a user may have just used positional arguments...
            else:
                flags = dict(zip(argnames, args))
            transformed = flags.get('transformed', True)
            method = flags.get('method', 'approx')
            approx_complex_step = flags.get('approx_complex_step', None)
            approx_centered = flags.get('approx_centered', True)

            for name, value in flags.items():
                if name in kwargs:
                    raise TypeError("hessian() got multiple values for keyword"
                                    " argument '%s'" % name)
        else:
            transformed = kwargs.pop('transformed', False)
            method = kwargs.pop('method', 'approx')
            approx_complex_step = kwargs.pop('approx_complex_step', None)
            approx_centered = kwargs.pop('approx_centered', False)

        if not transformed and approx_complex_step:
            raise ValueError("Cannot use complex-step approximations to"
                             " calculate the hessian with untransformed"
                             " parameters.")

        if approx_complex_step is None:
            approx_complex_step = not self.ssm._complex_endog
        if approx_complex_step and self.ssm._complex_endog:
            raise ValueError('Cannot use complex step derivatives when data'
                             ' or parameters are complex.')

        if method == 'oim':
            hessian = self._hessian_oim(
                params, transformed=transformed,
                approx_complex_step=approx_complex_step,
                approx_centered=approx_centered, **kwargs)
        elif method == 'opg':
            hessian = self._hessian_opg(
                params, transformed=transformed,
                approx_complex_step=approx_complex_step,
                approx_centered=approx_centered, **kwargs)
        elif method == 'approx' and approx_complex_step:
            return self._hessian_complex_step(
                params, transformed=transformed, **kwargs)
        elif method == 'approx':
            return self._hessian_finite_difference(
                params, transformed=transformed,
                approx_centered=approx_centered, **kwargs)
        else:
            raise NotImplementedError('Invalid Hessian calculation method.')
        return hessian

    def _hessian_oim(self, params, **kwargs):
        """
        Hessian matrix computed using the Harvey (1989) information matrix
        """
        return -self.observed_information_matrix(params, **kwargs)

    def _hessian_opg(self, params, **kwargs):
        """
        Hessian matrix computed using the outer product of gradients
        information matrix
        """
        return -self.opg_information_matrix(params, **kwargs)

    def _hessian_finite_difference(self, params, approx_centered=False,
                                   **kwargs):
        params = np.array(params, ndmin=1)

        warnings.warn('Calculation of the Hessian using finite differences'
                      ' is usually subject to substantial approximation'
                      ' errors.')

        if not approx_centered:
            epsilon = _get_epsilon(params, 3, None, len(params))
        else:
            epsilon = _get_epsilon(params, 4, None, len(params)) / 2
        hessian = approx_fprime(params, self._score_finite_difference,
            epsilon=epsilon, kwargs=kwargs, centered=approx_centered)

        return hessian / (self.nobs - self.ssm.loglikelihood_burn)

    def _hessian_complex_step(self, params, **kwargs):
        """
        Hessian matrix computed by second-order complex-step differentiation
        on the `loglike` function.
        """
        # the default epsilon can be too small
        epsilon = _get_epsilon(params, 3., None, len(params))
        kwargs['transformed'] = True
        kwargs['complex_step'] = True
        hessian = approx_hess_cs(
            params, self.loglike, epsilon=epsilon, kwargs=kwargs)

        return hessian / (self.nobs - self.ssm.loglikelihood_burn)

    @property
    def start_params(self):
        """
        (array) Starting parameters for maximum likelihood estimation.
        """
        if hasattr(self, '_start_params'):
            return self._start_params
        else:
            raise NotImplementedError

    @property
    def param_names(self):
        """
        (list of str) List of human readable parameter names (for parameters
        actually included in the model).
        """
        if hasattr(self, '_param_names'):
            return self._param_names
        else:
            try:
                names = ['param.%d' % i for i in range(len(self.start_params))]
            except NotImplementedError:
                names = []
            return names

    def transform_jacobian(self, unconstrained, approx_centered=False):
        """
        Jacobian matrix for the parameter transformation function
        Parameters
        ----------
        unconstrained : array_like
            Array of unconstrained parameters used by the optimizer.
        Returns
        -------
        jacobian : array
            Jacobian matrix of the transformation, evaluated at `unconstrained`
        Notes
        -----
        This is a numerical approximation using finite differences. Note that
        in general complex step methods cannot be used because it is not
        guaranteed that the `transform_params` method is a real function (e.g.
        if Cholesky decomposition is used).
        See Also
        --------
        transform_params
        """
        return approx_fprime(unconstrained, self.transform_params,
                             centered=approx_centered)

    def transform_params(self, unconstrained):
        """
        Transform unconstrained parameters used by the optimizer to constrained
        parameters used in likelihood evaluation
        Parameters
        ----------
        unconstrained : array_like
            Array of unconstrained parameters used by the optimizer, to be
            transformed.
        Returns
        -------
        constrained : array_like
            Array of constrained parameters which may be used in likelihood
            evalation.
        Notes
        -----
        This is a noop in the base class, subclasses should override where
        appropriate.
        """
        return np.array(unconstrained, ndmin=1)

    def untransform_params(self, constrained):
        """
        Transform constrained parameters used in likelihood evaluation
        to unconstrained parameters used by the optimizer
        Parameters
        ----------
        constrained : array_like
            Array of constrained parameters used in likelihood evalution, to be
            transformed.
        Returns
        -------
        unconstrained : array_like
            Array of unconstrained parameters used by the optimizer.
        Notes
        -----
        This is a noop in the base class, subclasses should override where
        appropriate.
        """
        return np.array(constrained, ndmin=1)

    def update(self, params, transformed=True, complex_step=False):
        """
        Update the parameters of the model
        Parameters
        ----------
        params : array_like
            Array of new parameters.
        transformed : boolean, optional
            Whether or not `params` is already transformed. If set to False,
            `transform_params` is called. Default is True.
        Returns
        -------
        params : array_like
            Array of parameters.
        Notes
        -----
        Since Model is a base class, this method should be overridden by
        subclasses to perform actual updating steps.
        """
        params = np.array(params, ndmin=1)

        if not transformed:
            params = self.transform_params(params)

        return params

    def simulate(self, params, nsimulations, measurement_shocks=None,
                 state_shocks=None, initial_state=None):
        """
        Simulate a new time series following the state space model
        Parameters
        ----------
        params : array_like
            Array of model parameters.
        nsimulations : int
            The number of observations to simulate. If the model is
            time-invariant this can be any number. If the model is
            time-varying, then this number must be less than or equal to the
            number
        measurement_shocks : array_like, optional
            If specified, these are the shocks to the measurement equation,
            :math:`\varepsilon_t`. If unspecified, these are automatically
            generated using a pseudo-random number generator. If specified,
            must be shaped `nsimulations` x `k_endog`, where `k_endog` is the
            same as in the state space model.
        state_shocks : array_like, optional
            If specified, these are the shocks to the state equation,
            :math:`\eta_t`. If unspecified, these are automatically
            generated using a pseudo-random number generator. If specified,
            must be shaped `nsimulations` x `k_posdef` where `k_posdef` is the
            same as in the state space model.
        initial_state : array_like, optional
            If specified, this is the state vector at time zero, which should
            be shaped (`k_states` x 1), where `k_states` is the same as in the
            state space model. If unspecified, but the model has been
            initialized, then that initialization is used. If unspecified and
            the model has not been initialized, then a vector of zeros is used.
            Note that this is not included in the returned `simulated_states`
            array.
        Returns
        -------
        simulated_obs : array
            An (nsimulations x k_endog) array of simulated observations.
        """
        self.update(params)

        simulated_obs, simulated_states = self.ssm.simulate(
            nsimulations, measurement_shocks, state_shocks, initial_state)

        # Simulated obs is (k_endog x nobs); don't want to squeeze in
        # case of npredictions = 1
        if simulated_obs.shape[0] == 1:
            simulated_obs = simulated_obs[0, :]
        else:
            simulated_obs = simulated_obs.T
        return simulated_obs

    def impulse_responses(self, params, steps=1, impulse=0,
                          orthogonalized=False, cumulative=False, **kwargs):
        """
        Impulse response function
        Parameters
        ----------
        params : array_like
            Array of model parameters.
        steps : int, optional
            The number of steps for which impulse responses are calculated.
            Default is 1. Note that the initial impulse is not counted as a
            step, so if `steps=1`, the output will have 2 entries.
        impulse : int or array_like
            If an integer, the state innovation to pulse; must be between 0
            and `k_posdef-1`. Alternatively, a custom impulse vector may be
            provided; must be shaped `k_posdef x 1`.
        orthogonalized : boolean, optional
            Whether or not to perform impulse using orthogonalized innovations.
            Note that this will also affect custum `impulse` vectors. Default
            is False.
        cumulative : boolean, optional
            Whether or not to return cumulative impulse responses. Default is
            False.
        **kwargs
            If the model is time-varying and `steps` is greater than the number
            of observations, any of the state space representation matrices
            that are time-varying must have updated values provided for the
            out-of-sample steps.
            For example, if `design` is a time-varying component, `nobs` is 10,
            and `steps` is 15, a (`k_endog` x `k_states` x 5) matrix must be
            provided with the new design matrix values.
        Returns
        -------
        impulse_responses : array
            Responses for each endogenous variable due to the impulse
            given by the `impulse` argument. A (steps + 1 x k_endog) array.
        Notes
        -----
        Intercepts in the measurement and state equation are ignored when
        calculating impulse responses.
        """
        self.update(params)
        return self.ssm.impulse_responses(
            steps, impulse, orthogonalized, cumulative, **kwargs)

    @classmethod
    def from_formula(cls, formula, data, subset=None):
        """
        Not implemented for state space models
        """
        raise NotImplementedError

class MLEResults(tsbase.TimeSeriesModelResults):
    r"""
    Class to hold results from fitting a state space model.
    Parameters
    ----------
    model : MLEModel instance
        The fitted model instance
    params : array
        Fitted parameters
    filter_results : KalmanFilter instance
        The underlying state space model and Kalman filter output
    Attributes
    ----------
    model : Model instance
        A reference to the model that was fit.
    filter_results : KalmanFilter instance
        The underlying state space model and Kalman filter output
    nobs : float
        The number of observations used to fit the model.
    params : array
        The parameters of the model.
    scale : float
        This is currently set to 1.0 and not used by the model or its results.
    See Also
    --------
    MLEModel
    statsmodels.tsa.statespace.kalman_filter.FilterResults statsmodels.tsa.statespace.representation.FrozenRepresentation """

    _filter_and_smoother_attributes = ['filtered_state', 'filtered_state_cov',
            'predicted_state', 'predicted_state_cov', 'forecasts',
            'forecasts_error', 'forecasts_error_cov',
            'scaled_smoothed_estimator', 'scaled_smoothed_estimator_cov',
            'smoothing_error', 'smoothed_state', 'smoothed_state_cov',
            'smoothed_state_autocov', 'smoothed_measurement_disturbance',
            'smoothed_state_disturbance',
            'smoothed_measurement_disturbance_cov',
            'smoothed_state_disturbance_cov']

    def __init__(self, model, params, results, cov_type='opg',
                 cov_kwds=None, **kwargs):
        self.data = model.data

        tsbase.TimeSeriesModelResults.__init__(self, model, params,
                                               normalized_cov_params=None,
                                               scale=1.)

        # Save the state space representation output
        self.filter_results = results
        if isinstance(results, SmootherResults):
            self.smoother_results = results
        else:
            self.smoother_results = None

        # Dimensions
        self.nobs = model.nobs

        # Setup covariance matrix notes dictionary
        if not hasattr(self, 'cov_kwds'):
            self.cov_kwds = {}
        self.cov_type = cov_type



        # Handle covariance matrix calculation
        if cov_kwds is None:
                cov_kwds = {}
        self._cov_approx_complex_step = (
            cov_kwds.pop('approx_complex_step', True))
        self._cov_approx_centered = cov_kwds.pop('approx_centered', False)
        try:
            self._rank = None
            self._get_robustcov_results(cov_type=cov_type, use_self=True,
                                        **cov_kwds)
        except np.linalg.LinAlgError:
            self._rank = 0
            k_params = len(self.params)
            self.cov_params_default = np.zeros((k_params, k_params)) * np.nan
            self.cov_kwds['cov_type'] = (
                'Covariance matrix could not be calculated: singular.'
                ' information matrix.')
        self.model.update(self.params)

        # References of filter and smoother output
        for name in self._filter_and_smoother_attributes:
            setattr(self, name, getattr(self.filter_results, name, None))

    def _get_robustcov_results(self, cov_type='opg', **kwargs):
        """
        Create new results instance with specified covariance estimator as
        default
        Note: creating new results instance currently not supported.
        Parameters
        ----------
        cov_type : string
            the type of covariance matrix estimator to use. See Notes below
        kwargs : depends on cov_type
            Required or optional arguments for covariance calculation.
            See Notes below.
        Returns
        -------
        results : results instance
            This method creates a new results instance with the requested
            covariance as the default covariance of the parameters.
            Inferential statistics like p-values and hypothesis tests will be
            based on this covariance matrix.
        Notes
        -----
        The following covariance types and required or optional arguments are
        currently available:
        - 'opg' for the outer product of gradient estimator
        - 'oim' for the observed information matrix estimator, calculated
          using the method of Harvey (1989)
        - 'approx' for the observed information matrix estimator,
          calculated using a numerical approximation of the Hessian matrix.
          Uses complex step approximation by default, or uses finite
          differences if `approx_complex_step=False` in the `cov_kwds`
          dictionary.
        - 'robust' for an approximate (quasi-maximum likelihood) covariance
          matrix that may be valid even in the presense of some
          misspecifications. Intermediate calculations use the 'oim'
          method.
        - 'robust_approx' is the same as 'robust' except that the
          intermediate calculations use the 'approx' method.
        - 'none' for no covariance matrix calculation.
        """

        import statsmodels.stats.sandwich_covariance as sw

        use_self = kwargs.pop('use_self', False)
        if use_self:
            res = self
        else:
            raise NotImplementedError
            res = self.__class__(
                self.model, self.params,
                normalized_cov_params=self.normalized_cov_params,
                scale=self.scale)

        # Set the new covariance type
        res.cov_type = cov_type
        res.cov_kwds = {}

        # Calculate the new covariance matrix
        approx_complex_step = self._cov_approx_complex_step
        if approx_complex_step:
            approx_type_str = 'complex-step'
        elif self._cov_approx_centered:
            approx_type_str = 'centered finite differences'
        else:
            approx_type_str = 'finite differences'
        k_params = len(self.params)
        if k_params == 0:
            res.cov_params_default = np.zeros((0, 0))
            res._rank = 0
            res.cov_kwds['description'] = (
                'No parameters estimated.')
        elif cov_type == 'none':
            res.cov_params_default = np.zeros((k_params, k_params)) * np.nan
            res._rank = np.nan
            res.cov_kwds['description'] = (
                'Covariance matrix not calculated.')
        elif self.cov_type == 'approx':
            res.cov_params_default = res.cov_params_approx
            res.cov_kwds['description'] = (
                'Covariance matrix calculated using numerical (%s)'
                ' differentiation.' % approx_type_str)
        elif self.cov_type == 'oim':
            res.cov_params_default = res.cov_params_oim
            res.cov_kwds['description'] = (
                'Covariance matrix calculated using the observed information'
                ' matrix (%s) described in Harvey (1989).' % approx_type_str)
        elif self.cov_type == 'opg':
            res.cov_params_default = res.cov_params_opg
            res.cov_kwds['description'] = (
                'Covariance matrix calculated using the outer product of'
                ' gradients (%s).' % approx_type_str
            )
        elif self.cov_type == 'robust' or self.cov_type == 'robust_oim':
            res.cov_params_default = res.cov_params_robust_oim
            res.cov_kwds['description'] = (
                'Quasi-maximum likelihood covariance matrix used for'
                ' robustness to some misspecifications; calculated using the'
                ' observed information matrix (%s) described in'
                ' Harvey (1989).' % approx_type_str)
        elif self.cov_type == 'robust_approx':
            res.cov_params_default = res.cov_params_robust
            res.cov_kwds['description'] = (
                'Quasi-maximum likelihood covariance matrix used for'
                ' robustness to some misspecifications; calculated using'
                ' numerical (%s) differentiation.' % approx_type_str)
        else:
            raise NotImplementedError('Invalid covariance matrix type.')

        return res

    @cache_readonly
    def aic(self):
        """
        (float) Akaike Information Criterion
        """
        # return -2*self.llf + 2*self.params.shape[0]
        return aic(self.llf, self.nobs, self.params.shape[0])

    @cache_readonly
    def bic(self):
        """
        (float) Bayes Information Criterion
        """
        # return -2*self.llf + self.params.shape[0]*np.log(self.nobs)
        return bic(self.llf, self.nobs, self.params.shape[0])

    def _cov_params_approx(self, approx_complex_step=True,
                           approx_centered=False):
        nobs = (self.model.nobs - self. ults.loglikelihood_burn)
        if approx_complex_step:
            evaluated_hessian = self.model._hessian_complex_step(
                self.params, transformed=True
            )
        else:
            evaluated_hessian = self.model._hessian_finite_difference(
                self.params, transformed=True,
                approx_centered=approx_centered
            )
        self.model.update(self.params)

        neg_cov, singular_values = pinv_extended(nobs * evaluated_hessian)

        if self._rank is None:
            self._rank = np.linalg.matrix_rank(np.diag(singular_values))

        return -neg_cov

    @cache_readonly
    def cov_params_approx(self):
        """
        (array) The variance / covariance matrix. Computed using the numerical
        Hessian approximated by complex step or finite differences methods.
        """
        return self._cov_params_approx(self._cov_approx_complex_step,
                                       self._cov_approx_centered)

    def _cov_params_oim(self, approx_complex_step=True,
                        approx_centered=False):
        nobs = (self.model.nobs - self.filter_results.loglikelihood_burn)
        cov_params, singular_values = pinv_extended(
            nobs * self.model.observed_information_matrix(
                self.params, transformed=True,
                approx_complex_step=approx_complex_step,
                approx_centered=approx_centered)
        )
        self.model.update(self.params)

        if self._rank is None:
            self._rank = np.linalg.matrix_rank(np.diag(singular_values))

        return cov_params

    @cache_readonly
    def cov_params_oim(self):
        """
        (array) The variance / covariance matrix. Computed using the method
        from Harvey (1989).
        """
        return self._cov_params_oim(self._cov_approx_complex_step,
                                    self._cov_approx_centered)

    def _cov_params_opg(self, approx_complex_step=True,
                        approx_centered=False):
        nobs = (self.model.nobs - self.filter_results.loglikelihood_burn)
        cov_params, singular_values = pinv_extended(
            nobs * self.model.opg_information_matrix(
                self.params, transformed=True,
                approx_complex_step=approx_complex_step,
                approx_centered=approx_centered)
        )
        self.model.update(self.params)

        if self._rank is None:
            self._rank = np.linalg.matrix_rank(np.diag(singular_values))

        return cov_params

    @cache_readonly
    def cov_params_opg(self):
        """
        (array) The variance / covariance matrix. Computed using the outer
        product of gradients method.
        """
        return self._cov_params_opg(self._cov_approx_complex_step,
                                    self._cov_approx_centered)

    @cache_readonly
    def cov_params_robust(self):
        """
        (array) The QMLE variance / covariance matrix. Alias for
        `cov_params_robust_oim`
        """
        return self.cov_params_robust_oim


    def _cov_params_robust_oim(self, approx_complex_step=True,
                               approx_centered=False):
        nobs = (self.model.nobs - self.filter_results.loglikelihood_burn)
        cov_opg = self._cov_params_opg(approx_complex_step=approx_complex_step,
                                       approx_centered=approx_centered)
        evaluated_hessian = (
            nobs * self.model.observed_information_matrix(
                self.params, transformed=True,
                approx_complex_step=approx_complex_step,
                approx_centered=approx_centered)
        )
        self.model.update(self.params)
        cov_params, singular_values = pinv_extended(
            np.dot(np.dot(evaluated_hessian, cov_opg), evaluated_hessian)
        )

        if self._rank is None:
            self._rank = np.linalg.matrix_rank(np.diag(singular_values))

        return cov_params

    @cache_readonly
    def cov_params_robust_oim(self):
        """
        (array) The QMLE variance / covariance matrix. Computed using the
        method from Harvey (1989) as the evaluated hessian.
        """
        return self._cov_params_robust_oim(self._cov_approx_complex_step,
                                           self._cov_approx_centered)

    def _cov_params_robust_approx(self, approx_complex_step=True,
                                  approx_centered=False):
        nobs = (self.model.nobs - self.filter_results.loglikelihood_burn)
        cov_opg = self._cov_params_opg(approx_complex_step=approx_complex_step,
                                       approx_centered=approx_centered)
        if approx_complex_step:
            evaluated_hessian = nobs * self.model._hessian_complex_step(
                self.params, transformed=True
            )
        else:
            evaluated_hessian = nobs * self.model._hessian_finite_difference(
                self.params, transformed=True,
                approx_centered=approx_centered
            )
        self.model.update(self.params)
        cov_params, singular_values = pinv_extended(
            np.dot(np.dot(evaluated_hessian, cov_opg), evaluated_hessian)
        )

        if self._rank is None:
            self._rank = np.linalg.matrix_rank(np.diag(singular_values))

        return cov_params

    @cache_readonly
    def cov_params_robust_approx(self):
        """
        (array) The QMLE variance / covariance matrix. Computed using the
        numerical Hessian as the evaluated hessian.
        """
        return self._cov_params_robust_approx(self._cov_approx_complex_step,
                                              self._cov_approx_centered)

    @cache_readonly
    def fittedvalues(self):
        """
        (array) The predicted values of the model. An (nobs x k_endog) array.
        """
        # This is a (k_endog x nobs array; don't want to squeeze in case of
        # the corner case where nobs = 1 (mostly a concern in the predict or
        # forecast functions, but here also to maintain consistency)
        fittedvalues = self.filter_results.forecasts
        if fittedvalues.shape[0] == 1:
            fittedvalues = fittedvalues[0, :]
        else:
            fittedvalues = fittedvalues.T
        return fittedvalues

    @cache_readonly
    def hqic(self):
        """
        (float) Hannan-Quinn Information Criterion
        """
        # return -2*self.llf + 2*np.log(np.log(self.nobs))*self.params.shape[0]
        return hqic(self.llf, self.nobs, self.params.shape[0])

    @cache_readonly
    def llf_obs(self):
        """
        (float) The value of the log-likelihood function evaluated at `params`.
        """
        return self.model.loglikeobs(self.params)

    @cache_readonly
    def llf(self):
        """
        (float) The value of the log-likelihood function evaluated at `params`.
        """
        return self.llf_obs[self.filter_results.loglikelihood_burn:].sum()

    @cache_readonly
    def loglikelihood_burn(self):
        """
        (float) The number of observations during which the likelihood is not
        evaluated.
        """
        return self.filter_results.loglikelihood_burn

    @cache_readonly
    def pvalues(self):
        """
        (array) The p-values associated with the z-statistics of the
        coefficients. Note that the coefficients are assumed to have a Normal
        distribution.
        """
        return norm.sf(np.abs(self.zvalues)) * 2

    @cache_readonly
    def resid(self):
        """
        (array) The model residuals. An (nobs x k_endog) array.
        """
        # This is a (k_endog x nobs array; don't want to squeeze in case of
        # the corner case where nobs = 1 (mostly a concern in the predict or
        # forecast functions, but here also to maintain consistency)
        resid = self.filter_results.forecasts_error
        if resid.shape[0] == 1:
            resid = resid[0, :]
        else:
            resid = resid.T
        return resid

    @cache_readonly
    def zvalues(self):
        """
        (array) The z-statistics for the coefficients.
        """
        return self.params / self.bse

    def test_normality(self, method):
        """
        Test for normality of standardized residuals.
        Null hypothesis is normality.
        Parameters
        ----------
        method : string {'jarquebera'} or None
            The statistical test for normality. Must be 'jarquebera' for
            Jarque-Bera normality test. If None, an attempt is made to select
            an appropriate test.
        Notes
        -----
        If the first `d` loglikelihood values were burned (i.e. in the
        specified model, `loglikelihood_burn=d`), then this test is calculated
        ignoring the first `d` residuals.
        In the case of missing data, the maintained hypothesis is that the
        data are missing completely at random. This test is then run on the
        standardized residuals excluding those corresponding to missing
        observations.
        See Also
        --------
        statsmodels.stats.stattools.jarque_bera
        """
        if method is None:
            method = 'jarquebera'

        if method == 'jarquebera':
            from statsmodels.stats.stattools import jarque_bera
            d = self.loglikelihood_burn
            output = []
            for i in range(self.model.k_endog):
                resid = self.filter_results.standardized_forecasts_error[i, d:]
                mask = ~np.isnan(resid)
                output.append(jarque_bera(resid[mask]))
        else:
            raise NotImplementedError('Invalid normality test method.')

        return np.array(output)

    def test_heteroskedasticity(self, method, alternative='two-sided',
                                use_f=True):
        r"""
        Test for heteroskedasticity of standardized residuals
        Tests whether the sum-of-squares in the first third of the sample is
        significantly different than the sum-of-squares in the last third
        of the sample. Analogous to a Goldfeld-Quandt test. The null hypothesis
        is of no heteroskedasticity.
        Parameters
        ----------
        method : string {'breakvar'} or None
            The statistical test for heteroskedasticity. Must be 'breakvar'
            for test of a break in the variance. If None, an attempt is
            made to select an appropriate test.
        alternative : string, 'increasing', 'decreasing' or 'two-sided'
            This specifies the alternative for the p-value calculation. Default
            is two-sided.
        use_f : boolean, optional
            Whether or not to compare against the asymptotic distribution
            (chi-squared) or the approximate small-sample distribution (F).
            Default is True (i.e. default is to compare against an F
            distribution).
        Returns
        -------
        output : array
            An array with `(test_statistic, pvalue)` for each endogenous
            variable. The array is then sized `(k_endog, 2)`. If the method is
            called as `het = res.test_heteroskedasticity()`, then `het[0]` is
            an array of size 2 corresponding to the first endogenous variable,
            where `het[0][0]` is the test statistic, and `het[0][1]` is the
            p-value.
        Notes
        -----
        The null hypothesis is of no heteroskedasticity. That means different
        things depending on which alternative is selected:
        - Increasing: Null hypothesis is that the variance is not increasing
          throughout the sample; that the sum-of-squares in the later
          subsample is *not* greater than the sum-of-squares in the earlier
          subsample.
        - Decreasing: Null hypothesis is that the variance is not decreasing
          throughout the sample; that the sum-of-squares in the earlier
          subsample is *not* greater than the sum-of-squares in the later
          subsample.
        - Two-sided: Null hypothesis is that the variance is not changing
          throughout the sample. Both that the sum-of-squares in the earlier
          subsample is not greater than the sum-of-squares in the later
          subsample *and* that the sum-of-squares in the later subsample is
          not greater than the sum-of-squares in the earlier subsample.
        For :math:`h = [T/3]`, the test statistic is:
        .. math::
            H(h) = \sum_{t=T-h+1}^T  \tilde v_t^2
            \Bigg / \sum_{t=d+1}^{d+1+h} \tilde v_t^2
        where :math:`d` is the number of periods in which the loglikelihood was
        burned in the parent model (usually corresponding to diffuse
        initialization).
        This statistic can be tested against an :math:`F(h,h)` distribution.
        Alternatively, :math:`h H(h)` is asymptotically distributed according
        to :math:`\chi_h^2`; this second test can be applied by passing
        `asymptotic=True` as an argument.
        See section 5.4 of [1]_ for the above formula and discussion, as well
        as additional details.
        TODO
        - Allow specification of :math:`h`
        References
        ----------
        .. [1] Harvey, Andrew C. 1990.
           Forecasting, Structural Time Series Models and the Kalman Filter.
           Cambridge University Press.
        """
        if method is None:
            method = 'breakvar'

        if method == 'breakvar':
            # Store some values
            squared_resid = self.filter_results.standardized_forecasts_error**2
            d = self.loglikelihood_burn

            test_statistics = []
            p_values = []
            for i in range(self.model.k_endog):
                h = int(np.round((self.nobs - d) / 3))
                numer_resid = squared_resid[i, -h:]
                numer_resid = numer_resid[~np.isnan(numer_resid)]
                numer_dof = len(numer_resid)

                denom_resid = squared_resid[i, d:d+h]
                denom_resid = denom_resid[~np.isnan(denom_resid)]
                denom_dof = len(denom_resid)

                if numer_dof < 2:
                    warnings.warn('Early subset of data for variable %d'
                                  '  has too few non-missing observations to'
                                  ' calculate test statistic.' % i)
                    numer_resid = np.nan
                if denom_dof < 2:
                    warnings.warn('Later subset of data for variable %d'
                                  '  has too few non-missing observations to'
                                  ' calculate test statistic.' % i)
                    denom_resid = np.nan

                test_statistic = np.sum(numer_resid) / np.sum(denom_resid)

                # Setup functions to calculate the p-values
                if use_f:
                    from scipy.stats import f
                    pval_lower = lambda test_statistics: f.cdf(
                        test_statistics, numer_dof, denom_dof)
                    pval_upper = lambda test_statistics: f.sf(
                        test_statistics, numer_dof, denom_dof)
                else:
                    from scipy.stats import chi2
                    pval_lower = lambda test_statistics: chi2.cdf(
                        numer_dof * test_statistics, denom_dof)
                    pval_upper = lambda test_statistics: chi2.sf(
                        numer_dof * test_statistics, denom_dof)

                # Calculate the one- or two-sided p-values
                alternative = alternative.lower()
                if alternative in ['i', 'inc', 'increasing']:
                    p_value = pval_upper(test_statistic)
                elif alternative in ['d', 'dec', 'decreasing']:
                    test_statistic = 1. / test_statistic
                    p_value = pval_upper(test_statistic)
                elif alternative in ['2', '2-sided', 'two-sided']:
                    p_value = 2 * np.minimum(
                        pval_lower(test_statistic),
                        pval_upper(test_statistic)
                    )
                else:
                    raise ValueError('Invalid alternative.')

                test_statistics.append(test_statistic)
                p_values.append(p_value)

            output = np.c_[test_statistics, p_values]
        else:
            raise NotImplementedError('Invalid heteroskedasticity test'
                                      ' method.')

        return output

    def test_serial_correlation(self, method, lags=None):
        """
        Ljung-box test for no serial correlation of standardized residuals
        Null hypothesis is no serial correlation.
        Parameters
        ----------
        method : string {'ljungbox','boxpierece'} or None
            The statistical test for serial correlation. If None, an attempt is
            made to select an appropriate test.
        lags : None, int or array_like
            If lags is an integer then this is taken to be the largest lag
            that is included, the test result is reported for all smaller lag
            length.
            If lags is a list or array, then all lags are included up to the
            largest lag in the list, however only the tests for the lags in the
            list are reported.
            If lags is None, then the default maxlag is 12*(nobs/100)^{1/4}
        Returns
        -------
        output : array
            An array with `(test_statistic, pvalue)` for each endogenous
            variable and each lag. The array is then sized
            `(k_endog, 2, lags)`. If the method is called as
            `ljungbox = res.test_serial_correlation()`, then `ljungbox[i]`
            holds the results of the Ljung-Box test (as would be returned by
            `statsmodels.stats.diagnostic.acorr_ljungbox`) for the `i`th
            endogenous variable.
        Notes
        -----
        If the first `d` loglikelihood values were burned (i.e. in the
        specified model, `loglikelihood_burn=d`), then this test is calculated
        ignoring the first `d` residuals.
        Output is nan for any endogenous variable which has missing values.
        See Also
        --------
        statsmodels.stats.diagnostic.acorr_ljungbox
        """
        if method is None:
            method = 'ljungbox'

        if method == 'ljungbox' or method == 'boxpierce':
            from statsmodels.stats.diagnostic import acorr_ljungbox
            d = self.loglikelihood_burn
            output = []

            # Default lags for acorr_ljungbox is 40, but may not always have
            # that many observations
            if lags is None:
                lags = min(40, self.nobs - d - 1)

            for i in range(self.model.k_endog):
                results = acorr_ljungbox(
                    self.filter_results.standardized_forecasts_error[i][d:],
                    lags=lags, boxpierce=(method == 'boxpierce'))
                if method == 'ljungbox':
                    output.append(results[0:2])
                else:
                    output.append(results[2:])

            output = np.c_[output]
        else:
            raise NotImplementedError('Invalid serial correlation test'
                                      ' method.')
        return output

    def get_prediction(self, start=None, end=None, dynamic=False, **kwargs):
        """
        In-sample prediction and out-of-sample forecasting
        Parameters
        ----------
        start : int, str, or datetime, optional
            Zero-indexed observation number at which to start forecasting,
            i.e., the first forecast is start. Can also be a date string to
            parse or a datetime type. Default is the the zeroth observation.
        end : int, str, or datetime, optional
            Zero-indexed observation number at which to end forecasting, i.e.,
            the last forecast is end. Can also be a date string to
            parse or a datetime type. However, if the dates index does not
            have a fixed frequency, end must be an integer index if you
            want out of sample prediction. Default is the last observation in
            the sample.
        dynamic : boolean, int, str, or datetime, optional
            Integer offset relative to `start` at which to begin dynamic
            prediction. Can also be an absolute date string to parse or a
            datetime type (these are not interpreted as offsets).
            Prior to this observation, true endogenous values will be used for
            prediction; starting with this observation and continuing through
            the end of prediction, forecasted endogenous values will be used
            instead.
        **kwargs
            Additional arguments may required for forecasting beyond the end
            of the sample. See `FilterResults.predict` for more details.
        Returns
        -------
        forecast : array
            Array of out of in-sample predictions and / or out-of-sample
            forecasts. An (npredict x k_endog) array.
        """
        if start is None:
            start = 0

        # Handle start and end (e.g. dates)
        start = self.model._get_predict_start(start)
        end, out_of_sample = self.model._get_predict_end(end)

        # Handle string dynamic
        dates = self.data.dates
        if isinstance(dynamic, str):
            if dates is None:
                raise ValueError("Got a string for dynamic and dates is None")
            dtdynamic = self.model._str_to_date(dynamic)
            try:
                dynamic_start = self.model._get_dates_loc(dates, dtdynamic)

                dynamic = dynamic_start - start
            except KeyError:
                raise ValueError("Dynamic must be in dates. Got %s | %s" %
                                 (str(dynamic), str(dtdynamic)))

        # Perform the prediction
        # This is a (k_endog x npredictions) array; don't want to squeeze in
        # case of npredictions = 1
        prediction_results = self.filter_results.predict(
            start, end+out_of_sample+1, dynamic, **kwargs
        )

        # Return a new mlemodel.PredictionResults object
        if self.data.dates is None:
            row_labels = self.data.row_labels
        else:
            row_labels = self.data.predict_dates
        return PredictionResultsWrapper(
            PredictionResults(self, prediction_results, row_labels=row_labels))

    def get_forecast(self, steps=1, **kwargs):
        """
        Out-of-sample forecasts
        Parameters
        ----------
        steps : int, str, or datetime, optional
            If an integer, the number of steps to forecast from the end of the
            sample. Can also be a date string to parse or a datetime type.
            However, if the dates index does not have a fixed frequency, steps
            must be an integer. Default
        **kwargs
            Additional arguments may required for forecasting beyond the end
            of the sample. See `FilterResults.predict` for more details.
        Returns
        -------
        forecast : array
            Array of out of sample forecasts. A (steps x k_endog) array.
        """
        if isinstance(steps, int):
            end = self.nobs+steps-1
        else:
            end = steps
        return self.get_prediction(start=self.nobs, end=end, **kwargs)

    def predict(self, start=None, end=None, dynamic=False, **kwargs):
        """
        In-sample prediction and out-of-sample forecasting
        Parameters
        ----------
        start : int, str, or datetime, optional
            Zero-indexed observation number at which to start forecasting,
            i.e., the first forecast is start. Can also be a date string to
            parse or a datetime type. Default is the the zeroth observation.
        end : int, str, or datetime, optional
            Zero-indexed observation number at which to end forecasting, i.e.,
            the last forecast is end. Can also be a date string to
            parse or a datetime type. However, if the dates index does not
            have a fixed frequency, end must be an integer index if you
            want out of sample prediction. Default is the last observation in
            the sample.
        dynamic : boolean, int, str, or datetime, optional
            Integer offset relative to `start` at which to begin dynamic
            prediction. Can also be an absolute date string to parse or a
            datetime type (these are not interpreted as offsets).
            Prior to this observation, true endogenous values will be used for
            prediction; starting with this observation and continuing through
            the end of prediction, forecasted endogenous values will be used
            instead.
        **kwargs
            Additional arguments may required for forecasting beyond the end
            of the sample. See `FilterResults.predict` for more details.
        Returns
        -------
        forecast : array
            Array of out of in-sample predictions and / or out-of-sample
            forecasts. An (npredict x k_endog) array.
        """
        # Perform the prediction
        prediction_results = self.get_prediction(start, end, dynamic, **kwargs)
        return prediction_results.predicted_mean

    def forecast(self, steps=1, **kwargs):
        """
        Out-of-sample forecasts
        Parameters
        ----------
        steps : int, str, or datetime, optional
            If an integer, the number of steps to forecast from the end of the
            sample. Can also be a date string to parse or a datetime type.
            However, if the dates index does not have a fixed frequency, steps
            must be an integer. Default
        **kwargs
            Additional arguments may required for forecasting beyond the end
            of the sample. See `FilterResults.predict` for more details.
        Returns
        -------
        forecast : array
            Array of out of sample forecasts. A (steps x k_endog) array.
        """
        if isinstance(steps, int):
            end = self.nobs+steps-1
        else:
            end = steps
        return self.predict(start=self.nobs, end=end, **kwargs)

    def simulate(self, nsimulations, measurement_shocks=None,
                 state_shocks=None, initial_state=None):
        """
        Simulate a new time series following the state space model
        Parameters
        ----------
        nsimulations : int
            The number of observations to simulate. If the model is
            time-invariant this can be any number. If the model is
            time-varying, then this number must be less than or equal to the
            number
        measurement_shocks : array_like, optional
            If specified, these are the shocks to the measurement equation,
            :math:`\varepsilon_t`. If unspecified, these are automatically
            generated using a pseudo-random number generator. If specified,
            must be shaped `nsimulations` x `k_endog`, where `k_endog` is the
            same as in the state space model.
        state_shocks : array_like, optional
            If specified, these are the shocks to the state equation,
            :math:`\eta_t`. If unspecified, these are automatically
            generated using a pseudo-random number generator. If specified,
            must be shaped `nsimulations` x `k_posdef` where `k_posdef` is the
            same as in the state space model.
        initial_state : array_like, optional
            If specified, this is the state vector at time zero, which should
            be shaped (`k_states` x 1), where `k_states` is the same as in the
            state space model. If unspecified, but the model has been
            initialized, then that initialization is used. If unspecified and
            the model has not been initialized, then a vector of zeros is used.
            Note that this is not included in the returned `simulated_states`
            array.
        Returns
        -------
        simulated_obs : array
            An (nsimulations x k_endog) array of simulated observations.
        """
        return self.model.simulate(self.params, nsimulations,
                                   measurement_shocks, state_shocks,
                                   initial_state)

    def impulse_responses(self, steps=1, impulse=0, orthogonalized=False,
                          cumulative=False, **kwargs):
        """
        Impulse response function
        Parameters
        ----------
        steps : int, optional
            The number of steps for which impulse responses are calculated.
            Default is 1. Note that the initial impulse is not counted as a
            step, so if `steps=1`, the output will have 2 entries.
        impulse : int or array_like
            If an integer, the state innovation to pulse; must be between 0
            and `k_posdef-1`. Alternatively, a custom impulse vector may be
            provided; must be shaped `k_posdef x 1`.
        orthogonalized : boolean, optional
            Whether or not to perform impulse using orthogonalized innovations.
            Note that this will also affect custum `impulse` vectors. Default
            is False.
        cumulative : boolean, optional
            Whether or not to return cumulative impulse responses. Default is
            False.
        **kwargs
            If the model is time-varying and `steps` is greater than the number
            of observations, any of the state space representation matrices
            that are time-varying must have updated values provided for the
            out-of-sample steps.
            For example, if `design` is a time-varying component, `nobs` is 10,
            and `steps` is 15, a (`k_endog` x `k_states` x 5) matrix must be
            provided with the new design matrix values.
        Returns
        -------
        impulse_responses : array
            Responses for each endogenous variable due to the impulse
            given by the `impulse` argument. A (steps + 1 x k_endog) array.
        Notes
        -----
        Intercepts in the measurement and state equation are ignored when
        calculating impulse responses.
        """
        return self.model.impulse_responses(self.params, steps, impulse,
                                            orthogonalized, cumulative,
                                            **kwargs)

    def plot_diagnostics(self, variable=0, lags=10, fig=None, figsize=None):
        """
        Diagnostic plots for standardized residuals of one endogenous variable
        Parameters
        ----------
        variable : integer, optional
            Index of the endogenous variable for which the diagnostic plots
            should be created. Default is 0.
        lags : integer, optional
            Number of lags to include in the correlogram. Default is 10.
        fig : Matplotlib Figure instance, optional
            If given, subplots are created in this figure instead of in a new
            figure. Note that the 2x2 grid will be created in the provided
            figure using `fig.add_subplot()`.
        figsize : tuple, optional
            If a figure is created, this argument allows specifying a size.
            The tuple is (width, height).
        Notes
        -----
        Produces a 2x2 plot grid with the following plots (ordered clockwise
        from top left):
        1. Standardized residuals over time
        2. Histogram plus estimated density of standardized residulas, along
           with a Normal(0,1) density plotted for reference.
        3. Normal Q-Q plot, with Normal reference line.
        4. Correlogram
        See Also
        --------
        statsmodels.graphics.gofplots.qqplot
        statsmodels.graphics.tsaplots.plot_acf
        """
        from statsmodels.graphics.utils import _import_mpl, create_mpl_fig
        _import_mpl()
        fig = create_mpl_fig(fig, figsize)
        # Eliminate residuals associated with burned likelihoods
        d = self.loglikelihood_burn
        resid = self.filter_results.standardized_forecasts_error[variable, d:]

        # Top-left: residuals vs time
        ax = fig.add_subplot(221)
        if hasattr(self.data, 'dates') and self.data.dates is not None:
            x = self.data.dates[self.loglikelihood_burn:]._mpl_repr()
        else:
            x = np.arange(len(resid))
        ax.plot(x, resid)
        ax.hlines(0, x[0], x[-1], alpha=0.5)
        ax.set_xlim(x[0], x[-1])
        ax.set_title('Standardized residual')

        # Top-right: histogram, Gaussian kernel density, Normal density
        # Can only do histogram and Gaussian kernel density on the non-null
        # elements
        resid_nonmissing = resid[~(np.isnan(resid))]
        ax = fig.add_subplot(222)
        ax.hist(resid_nonmissing, normed=True, label='Hist')
        from scipy.stats import gaussian_kde, norm
        kde = gaussian_kde(resid_nonmissing)
        xlim = (-1.96*2, 1.96*2)
        x = np.linspace(xlim[0], xlim[1])
        ax.plot(x, kde(x), label='KDE')
        ax.plot(x, norm.pdf(x), label='N(0,1)')
        ax.set_xlim(xlim)
        ax.legend()
        ax.set_title('Histogram plus estimated density')

        # Bottom-left: QQ plot
        ax = fig.add_subplot(223)
        from statsmodels.graphics.gofplots import qqplot
        qqplot(resid_nonmissing, line='s', ax=ax)
        ax.set_title('Normal Q-Q')

        # Bottom-right: Correlogram
        ax = fig.add_subplot(224)
        from statsmodels.graphics.tsaplots import plot_acf
        plot_acf(resid, ax=ax, lags=lags)
        ax.set_title('Correlogram')

        ax.set_ylim(-1, 1)

        return fig

    def summary(self, alpha=.05, start=None, title=None, model_name=None,
                display_params=True):
        """
        Summarize the Model
        Parameters
        ----------
        alpha : float, optional
            Significance level for the confidence intervals. Default is 0.05.
        start : int, optional
            Integer of the start observation. Default is 0.
        model_name : string
            The name of the model used. Default is to use model class name.
        Returns
        -------
        summary : Summary instance
            This holds the summary table and text, which can be printed or
            converted to various output formats.
        See Also
        --------
        statsmodels.iolib.summary.Summary
        """
        from statsmodels.iolib.summary import Summary

        # Model specification results
        model = self.model
        if title is None:
            title = 'Statespace Model Results'

        if start is None:
            start = 0
        if self.data.dates is not None:
            dates = self.data.dates
            d = dates[start]
            sample = ['%02d-%02d-%02d' % (d.month, d.day, d.year)]
            d = dates[-1]
            sample += ['- ' + '%02d-%02d-%02d' % (d.month, d.day, d.year)]
        else:
            sample = [str(start), ' - ' + str(self.model.nobs)]

        # Standardize the model name as a list of str
        if model_name is None:
            model_name = model.__class__.__name__

        # Diagnostic tests results
        het = self.test_heteroskedasticity(method='breakvar')
        lb = self.test_serial_correlation(method='ljungbox')
        jb = self.test_normality(method='jarquebera')

        # Create the tables
        if not isinstance(model_name, list):
            model_name = [model_name]

        top_left = [('Dep. Variable:', None)]
        top_left.append(('Model:', [model_name[0]]))
        for i in range(1, len(model_name)):
            top_left.append(('', ['+ ' + model_name[i]]))
        top_left += [
            ('Date:', None),
            ('Time:', None),
            ('Sample:', [sample[0]]),
            ('', [sample[1]])
        ]

        top_right = [
            ('No. Observations:', [self.model.nobs]),
            ('Log Likelihood', ["%#5.3f" % self.llf]),
            ('AIC', ["%#5.3f" % self.aic]),
            ('BIC', ["%#5.3f" % self.bic]),
            ('HQIC', ["%#5.3f" % self.hqic])
        ]

        if hasattr(self, 'cov_type'):
            top_left.append(('Covariance Type:', [self.cov_type]))

        format_str = lambda array: [
            ', '.join(['{0:.2f}'.format(i) for i in array])
        ]
        diagn_left = [('Ljung-Box (Q):', format_str(lb[:, 0, -1])),
                      ('Prob(Q):', format_str(lb[:, 1, -1])),
                      ('Heteroskedasticity (H):', format_str(het[:, 0])),
                      ('Prob(H) (two-sided):', format_str(het[:, 1]))
                      ]

        diagn_right = [('Jarque-Bera (JB):', format_str(jb[:, 0])),
                       ('Prob(JB):', format_str(jb[:, 1])),
                       ('Skew:', format_str(jb[:, 2])),
                       ('Kurtosis:', format_str(jb[:, 3]))
                       ]

        summary = Summary()
        summary.add_table_2cols(self, gleft=top_left, gright=top_right,
                                title=title)
        if len(self.params) > 0 and display_params:
            summary.add_table_params(self, alpha=alpha,
                                     xname=self.data.param_names, use_t=False)
        summary.add_table_2cols(self, gleft=diagn_left, gright=diagn_right,
                                title="")

        # Add warnings/notes, added to text format only
        etext = []
        if hasattr(self, 'cov_type') and 'description' in self.cov_kwds:
            etext.append(self.cov_kwds['description'])
        if self._rank < len(self.params):
            etext.append("Covariance matrix is singular or near-singular,"
                         " with condition number %6.3g. Standard errors may be"
                         " unstable." % np.linalg.cond(self.cov_params()))

        if etext:
            etext = ["[{0}] {1}".format(i + 1, text)
                     for i, text in enumerate(etext)]
            etext.insert(0, "Warnings:")
            summary.add_extra_txt(etext)

        return summary

    def _plot_values(self, filtered_values, filtered_value_covs, value_labels,
            alpha, legend_loc, fig, figsize):
        r"""
        Plot univariate values, changing with time.
        Parameters
        ----------
        filtered_values : array_like
            (`k_values` x `nobs`) shaped array, containing filtered values.
        filtered_value_covs : array_like
            (`k_values` x `nobs`) shaped array, containing variances/covariances
            of filtered values.
        alpha : float
            The confidence intervals for the coefficient are (1 - alpha) %
        legend_loc : string
            The location of the legend in the plot. Default is upper left.
        fig : Matplotlib Figure instance
            If given, subplots are created in this figure instead of in a new
            figure. Note that the grid will be created in the provided
            figure using `fig.add_subplot()`.
        figsize : tuple
            If a figure is created, this argument allows specifying a size.
            The tuple is (width, height).
        Notes
        -----
        This is a helping method, used internally.
        All plots contain (1 - `alpha`) %  confidence intervals.
        """

        # Create the plot
        from scipy.stats import norm
        from statsmodels.graphics.utils import _import_mpl, create_mpl_fig
        plt = _import_mpl()
        fig = create_mpl_fig(fig, figsize)

        k_values = filtered_values.shape[0]

        for i in range(k_values):
            filtered = filtered_values[i]
            filtered_cov = filtered_value_covs[i]

            ax = fig.add_subplot(k_values, 1, i + 1)

            # Get dates, if applicable
            if hasattr(self.data, 'dates') and self.data.dates is not None:
                dates = self.data.dates._mpl_repr()
            else:
                dates = np.arange(self.nobs)
            llb = self.loglikelihood_burn

            # Plot the value
            ax.plot(dates[llb:], filtered[llb:],
                    label=value_labels[i])

            # Legend
            handles, labels = ax.get_legend_handles_labels()

            # Get the critical value for confidence intervals
            if alpha is not None:
                critical_value = norm.ppf(1 - alpha / 2.)

                # Plot confidence intervals
                std_errors = np.sqrt(filtered_cov)
                ci_lower = (
                    filtered - critical_value * std_errors)
                ci_upper = (
                    filtered + critical_value * std_errors)
                ci_poly = ax.fill_between(
                    dates[llb:], ci_lower[llb:], ci_upper[llb:], alpha=0.2
                )
                ci_label = ('$%.3g \\%%$ confidence interval'
                            % ((1 - alpha)*100))

                # Only add CI to legend for the first plot
                if i == 0:
                    # Proxy artist for fill_between legend entry
                    # See http://matplotlib.org/1.3.1/users/legend_guide.html
                    p = plt.Rectangle((0, 0), 1, 1,
                                      fc=ci_poly.get_facecolor()[0])

                    handles.append(p)
                    labels.append(ci_label)

            ax.legend(handles, labels, loc=legend_loc)

            # Remove xticks for all but the last plot
            if i < k_values - 1:
                ax.xaxis.set_ticklabels([])

        fig.tight_layout()

        return fig

class MLEResultsWrapper(wrap.ResultsWrapper):
    _attrs = {
        'zvalues': 'columns',
        'cov_params_approx': 'cov',
        'cov_params_default': 'cov',
        'cov_params_oim': 'cov',
        'cov_params_opg': 'cov',
        'cov_params_robust': 'cov',
        'cov_params_robust_approx': 'cov',
        'cov_params_robust_oim': 'cov',
    }
    _wrap_attrs = wrap.union_dicts(tsbase.TimeSeriesResultsWrapper._wrap_attrs,
                                   _attrs)
    _methods = {
        'forecast': 'dates',
        'simulate': 'ynames',
        'impulse_responses': 'ynames'
    }
    _wrap_methods = wrap.union_dicts(
        tsbase.TimeSeriesResultsWrapper._wrap_methods, _methods)
wrap.populate_wrapper(MLEResultsWrapper, MLEResults)


class PredictionResults(pred.PredictionResults):
    """
    Parameters
    ----------
    prediction_results : kalman_filter.PredictionResults instance
        Results object from prediction after fitting or filtering a state space
        model.
    row_labels : iterable
        Row labels for the predicted data.
    Attributes
    ----------
    """
    def __init__(self, model, prediction_results, row_labels=None):
        if model.model.k_endog == 1:
            endog = pd.Series(prediction_results.endog[:, 0],
                              name=model.model.endog_names)
        else:
            endog = pd.DataFrame(prediction_results.endog.T,
                                 columns=model.model.endog_names)
        self.model = Bunch(data=model.data.__class__(
            endog=endog,
            predict_dates=getattr(model.data, 'predict_dates', None)),
        )
        self.prediction_results = prediction_results

        # Get required values
        predicted_mean = self.prediction_results.forecasts
        if predicted_mean.shape[0] == 1:
            predicted_mean = predicted_mean[0, :]
        else:
            predicted_mean = predicted_mean.transpose()

        var_pred_mean = self.prediction_results.forecasts_error_cov
        if var_pred_mean.shape[0] == 1:
            var_pred_mean = var_pred_mean[0, 0, :]
        else:
            var_pred_mean = var_pred_mean.transpose()

        # Initialize
        super(PredictionResults, self).__init__(predicted_mean, var_pred_mean,
                                                dist='norm',
                                                row_labels=row_labels,
                                                link=identity())

    @property
    def se_mean(self):
        if self.var_pred_mean.ndim == 1:
            se_mean = np.sqrt(self.var_pred_mean)
        else:
            se_mean = np.sqrt(self.var_pred_mean.T.diagonal())
        return se_mean

    def conf_int(self, method='endpoint', alpha=0.05, **kwds):
        # TODO: this performs metadata wrapping, and that should be handled
        #       by attach_* methods. However, they don't currently support
        #       this use case.
        conf_int = super(PredictionResults, self).conf_int(
            method, alpha, **kwds)

        # Create a dataframe
        if self.model.data.predict_dates is not None:
            conf_int = pd.DataFrame(conf_int,
                                    index=self.model.data.predict_dates)
        else:
            conf_int = pd.DataFrame(conf_int)

        # Attach the endog names
        ynames = self.model.data.ynames
        if not type(ynames) == list:
            ynames = [ynames]
        names = (['lower %s' % name for name in ynames] +
                 ['upper %s' % name for name in ynames])
        conf_int.columns = names

        return conf_int

    def summary_frame(self, endog=0, what='all', alpha=0.05):
        # TODO: finish and cleanup
        # import pandas as pd
        from statsmodels.compat.collections import OrderedDict
        # ci_obs = self.conf_int(alpha=alpha, obs=True) # need to split
        ci_mean = self.conf_int(alpha=alpha).values
        to_include = OrderedDict()
        if self.predicted_mean.ndim == 1:
            yname = self.model.data.ynames
            to_include['mean'] = self.predicted_mean
            to_include['mean_se'] = self.se_mean
            k_endog = 1
        else:
            yname = self.model.data.ynames[endog]
            to_include['mean'] = self.predicted_mean[:, endog]
            to_include['mean_se'] = self.se_mean[:, endog]
            k_endog = self.predicted_mean.shape[1]
        to_include['mean_ci_lower'] = ci_mean[:, endog]
        to_include['mean_ci_upper'] = ci_mean[:, k_endog + endog]

        self.table = to_include
        # OrderedDict doesn't work to preserve sequence
        # pandas dict doesn't handle 2d_array
        # data = np.column_stack(list(to_include.values()))
        # names = ....
        res = pd.DataFrame(to_include, index=self.row_labels,
                           columns=to_include.keys())
        res.columns.name = yname
        return res


class PredictionResultsWrapper(wrap.ResultsWrapper):
    _attrs = {
        'predicted_mean': 'dates',
        'se_mean': 'dates',
        't_values': 'dates',
    }
    _wrap_attrs = wrap.union_dicts(_attrs)

    _methods = {}
    _wrap_methods = wrap.union_dicts(_methods)
wrap.populate_wrapper(PredictionResultsWrapper, PredictionResults)

# Define constants
FILTER_CONVENTIONAL = 0x01     # Durbin and Koopman (2012), Chapter 4
FILTER_EXACT_INITIAL = 0x02    # ibid., Chapter 5.6
FILTER_AUGMENTED = 0x04        # ibid., Chapter 5.7
FILTER_SQUARE_ROOT = 0x08      # ibid., Chapter 6.3
FILTER_UNIVARIATE = 0x10       # ibid., Chapter 6.4
FILTER_COLLAPSED = 0x20        # ibid., Chapter 6.5
FILTER_EXTENDED = 0x40         # ibid., Chapter 10.2
FILTER_UNSCENTED = 0x80        # ibid., Chapter 10.3

INVERT_UNIVARIATE = 0x01
SOLVE_LU = 0x02
INVERT_LU = 0x04
SOLVE_CHOLESKY = 0x08
INVERT_CHOLESKY = 0x10

STABILITY_FORCE_SYMMETRY = 0x01

MEMORY_STORE_ALL = 0
MEMORY_NO_FORECAST = 0x01
MEMORY_NO_PREDICTED = 0x02
MEMORY_NO_FILTERED = 0x04
MEMORY_NO_LIKELIHOOD = 0x08
MEMORY_NO_GAIN = 0x10
MEMORY_NO_SMOOTHING = 0x20
MEMORY_CONSERVE = (
    MEMORY_NO_FORECAST | MEMORY_NO_PREDICTED | MEMORY_NO_FILTERED |
    MEMORY_NO_LIKELIHOOD | MEMORY_NO_GAIN | MEMORY_NO_SMOOTHING
)

TIMING_INIT_PREDICTED = 0
TIMING_INIT_FILTERED = 1


class KalmanFilter(Representation):
    r"""
    State space representation of a time series process, with Kalman filter
    Parameters
    ----------
    k_endog : array_like or integer
        The observed time-series process :math:`y` if array like or the
        number of variables in the process if an integer.
    k_states : int
        The dimension of the unobserved state process.
    k_posdef : int, optional
        The dimension of a guaranteed positive definite covariance matrix
        describing the shocks in the measurement equation. Must be less than
        or equal to `k_states`. Default is `k_states`.
    loglikelihood_burn : int, optional
        The number of initial periods during which the loglikelihood is not
        recorded. Default is 0.
    tolerance : float, optional
        The tolerance at which the Kalman filter determines convergence to
        steady-state. Default is 1e-19.
    results_class : class, optional
        Default results class to use to save filtering output. Default is
        `FilterResults`. If specified, class must extend from `FilterResults`.
    **kwargs
        Keyword arguments may be used to provide values for the filter,
        inversion, and stability methods. See `set_filter_method`,
        `set_inversion_method`, and `set_stability_method`.
        Keyword arguments may be used to provide default values for state space
        matrices. See `Representation` for more details.
    Notes
    -----
    There are several types of options available for controlling the Kalman
    filter operation. All options are internally held as bitmasks, but can be
    manipulated by setting class attributes, which act like boolean flags. For
    more information, see the `set_*` class method documentation. The options
    are:
    filter_method
        The filtering method controls aspects of which
        Kalman filtering approach will be used.
    inversion_method
        The Kalman filter may contain one matrix inversion: that of the
        forecast error covariance matrix. The inversion method controls how and
        if that inverse is performed.
    stability_method
        The Kalman filter is a recursive algorithm that may in some cases
        suffer issues with numerical stability. The stability method controls
        what, if any, measures are taken to promote stability.
    conserve_memory
        By default, the Kalman filter computes a number of intermediate
        matrices at each iteration. The memory conservation options control
        which of those matrices are stored.
    filter_timing
        By default, the Kalman filter follows Durbin and Koopman, 2012, in
        initializing the filter with predicted values. Kim and Nelson, 1999,
        instead initialize the filter with filtered values, which is
        essentially just a different timing convention.
    The `filter_method` and `inversion_method` options intentionally allow
    the possibility that multiple methods will be indicated. In the case that
    multiple methods are selected, the underlying Kalman filter will attempt to
    select the optional method given the input data.
    For example, it may be that INVERT_UNIVARIATE and SOLVE_CHOLESKY are
    indicated (this is in fact the default case). In this case, if the
    endogenous vector is 1-dimensional (`k_endog` = 1), then INVERT_UNIVARIATE
    is used and inversion reduces to simple division, and if it has a larger
    dimension, the Cholesky decomposition along with linear solving (rather
    than explicit matrix inversion) is used. If only SOLVE_CHOLESKY had been
    set, then the Cholesky decomposition method would *always* be used, even in
    the case of 1-dimensional data.
    See Also
    --------
    FilterResults
    statsmodels.tsa.statespace.representation.Representation
    """

    filter_methods = [
        'filter_conventional', 'filter_exact_initial', 'filter_augmented',
        'filter_square_root', 'filter_univariate', 'filter_collapsed',
        'filter_extended', 'filter_unscented'
    ]

    filter_conventional = OptionWrapper('filter_method', FILTER_CONVENTIONAL)
    """
    (bool) Flag for conventional Kalman filtering.
    """
    filter_exact_initial = OptionWrapper('filter_method', FILTER_EXACT_INITIAL)
    """
    (bool) Flag for exact initial Kalman filtering. Not implemented.
    """
    filter_augmented = OptionWrapper('filter_method', FILTER_AUGMENTED)
    """
    (bool) Flag for augmented Kalman filtering. Not implemented.
    """
    filter_square_root = OptionWrapper('filter_method', FILTER_SQUARE_ROOT)
    """
    (bool) Flag for square-root Kalman filtering. Not implemented.
    """
    filter_univariate = OptionWrapper('filter_method', FILTER_UNIVARIATE)
    """
    (bool) Flag for univariate filtering of multivariate observation vector.
    """
    filter_collapsed = OptionWrapper('filter_method', FILTER_COLLAPSED)
    """
    (bool) Flag for Kalman filtering with collapsed observation vector.
    """
    filter_extended = OptionWrapper('filter_method', FILTER_EXTENDED)
    """
    (bool) Flag for extended Kalman filtering. Not implemented.
    """
    filter_unscented = OptionWrapper('filter_method', FILTER_UNSCENTED)
    """
    (bool) Flag for unscented Kalman filtering. Not implemented.
    """

    inversion_methods = [
        'invert_univariate', 'solve_lu', 'invert_lu', 'solve_cholesky',
        'invert_cholesky'
    ]

    invert_univariate = OptionWrapper('inversion_method', INVERT_UNIVARIATE)
    """
    (bool) Flag for univariate inversion method (recommended).
    """
    solve_lu = OptionWrapper('inversion_method', SOLVE_LU)
    """
    (bool) Flag for LU and linear solver inversion method.
    """
    invert_lu = OptionWrapper('inversion_method', INVERT_LU)
    """
    (bool) Flag for LU inversion method.
    """
    solve_cholesky = OptionWrapper('inversion_method', SOLVE_CHOLESKY)
    """
    (bool) Flag for Cholesky and linear solver inversion method (recommended).
    """
    invert_cholesky = OptionWrapper('inversion_method', INVERT_CHOLESKY)
    """
    (bool) Flag for Cholesky inversion method.
    """

    stability_methods = ['stability_force_symmetry']

    stability_force_symmetry = (
        OptionWrapper('stability_method', STABILITY_FORCE_SYMMETRY)
    )
    """
    (bool) Flag for enforcing covariance matrix symmetry
    """

    memory_options = [
        'memory_store_all', 'memory_no_forecast', 'memory_no_predicted',
        'memory_no_filtered', 'memory_no_likelihood', 'memory_no_gain',
        'memory_no_smoothing', 'memory_conserve'
    ]

    memory_store_all = OptionWrapper('conserve_memory', MEMORY_STORE_ALL)
    """
    (bool) Flag for storing all intermediate results in memory (default).
    """
    memory_no_forecast = OptionWrapper('conserve_memory', MEMORY_NO_FORECAST)
    """
    (bool) Flag to prevent storing forecasts.
    """
    memory_no_predicted = OptionWrapper('conserve_memory', MEMORY_NO_PREDICTED)
    """
    (bool) Flag to prevent storing predicted state and covariance matrices.
    """
    memory_no_filtered = OptionWrapper('conserve_memory', MEMORY_NO_FILTERED)
    """
    (bool) Flag to prevent storing filtered state and covariance matrices.
    """
    memory_no_likelihood = (
        OptionWrapper('conserve_memory', MEMORY_NO_LIKELIHOOD)
    )
    """
    (bool) Flag to prevent storing likelihood values for each observation.
    """
    memory_no_gain = OptionWrapper('conserve_memory', MEMORY_NO_GAIN)
    """
    (bool) Flag to prevent storing the Kalman gain matrices.
    """
    memory_no_smoothing = OptionWrapper('conserve_memory', MEMORY_NO_SMOOTHING)
    """
    (bool) Flag to prevent storing likelihood values for each observation.
    """
    memory_conserve = OptionWrapper('conserve_memory', MEMORY_CONSERVE)
    """
    (bool) Flag to conserve the maximum amount of memory.
    """

    timing_options = [
        'timing_init_predicted', 'timing_init_filtered'
    ]
    timing_init_predicted = OptionWrapper('filter_timing',
                                          TIMING_INIT_PREDICTED)
    """
    (bool) Flag for the default timing convention (Durbin and Koopman, 2012).
    """
    timing_init_filtered = OptionWrapper('filter_timing', TIMING_INIT_FILTERED)
    """
    (bool) Flag for the alternate timing convention (Kim and Nelson, 2012).
    """

    # Default filter options
    filter_method = FILTER_CONVENTIONAL
    """
    (int) Filtering method bitmask.
    """
    inversion_method = INVERT_UNIVARIATE | SOLVE_CHOLESKY
    """
    (int) Inversion method bitmask.
    """
    stability_method = STABILITY_FORCE_SYMMETRY
    """
    (int) Stability method bitmask.
    """
    conserve_memory = MEMORY_STORE_ALL
    """
    (int) Memory conservation bitmask.
    """
    filter_timing = TIMING_INIT_PREDICTED
    """
    (int) Filter timing.
    """

    def __init__(self, k_endog, k_states, k_posdef=None,
                 loglikelihood_burn=0, tolerance=1e-19, results_class=None,
                 kalman_filter_classes=None, **kwargs):
        super(KalmanFilter, self).__init__(
            k_endog, k_states, k_posdef, **kwargs
        )

        # Setup the underlying Kalman filter storage
        self._kalman_filters = {}

        # Filter options
        self.loglikelihood_burn = loglikelihood_burn
        self.results_class = (
            results_class if results_class is not None else FilterResults
        )
        # Options
        self.prefix_kalman_filter_map = (
            kalman_filter_classes
            if kalman_filter_classes is not None
            else tools.prefix_kalman_filter_map.copy())

        self.set_filter_method(**kwargs)
        self.set_inversion_method(**kwargs)
        self.set_stability_method(**kwargs)
        self.set_conserve_memory(**kwargs)
        self.set_filter_timing(**kwargs)

        self.tolerance = tolerance

    @property
    def _kalman_filter(self):
        prefix = self.prefix
        if prefix in self._kalman_filters:
            return self._kalman_filters[prefix]
        return None

    def _initialize_filter(self, filter_method=None, inversion_method=None,
                           stability_method=None, conserve_memory=None,
                           tolerance=None, filter_timing=None,
                           loglikelihood_burn=None):
        if filter_method is None:
            filter_method = self.filter_method
        if inversion_method is None:
            inversion_method = self.inversion_method
        if stability_method is None:
            stability_method = self.stability_method
        if conserve_memory is None:
            conserve_memory = self.conserve_memory
        if loglikelihood_burn is None:
            loglikelihood_burn = self.loglikelihood_burn
        if filter_timing is None:
            filter_timing = self.filter_timing
        if tolerance is None:
            tolerance = self.tolerance

        # Make sure we have endog
        if self.endog is None:
            raise RuntimeError('Must bind a dataset to the model before'
                               ' filtering or smoothing.')

        # Initialize the representation matrices
        prefix, dtype, create_statespace = self._initialize_representation()

        # Determine if we need to (re-)create the filter
        # (definitely need to recreate if we recreated the _statespace object)
        create_filter = create_statespace or prefix not in self._kalman_filters
        if not create_filter:
            kalman_filter = self._kalman_filters[prefix]

            create_filter = (
                not kalman_filter.conserve_memory == conserve_memory or
                not kalman_filter.loglikelihood_burn == loglikelihood_burn
            )

        # If the dtype-specific _kalman_filter does not exist (or if we need
        # to re-create it), create it
        if create_filter:
            if prefix in self._kalman_filters:
                # Delete the old filter
                del self._kalman_filters[prefix]
            # Setup the filter
            cls = self.prefix_kalman_filter_map[prefix]
            self._kalman_filters[prefix] = cls(
                self._statespaces[prefix], filter_method, inversion_method,
                stability_method, conserve_memory, filter_timing, tolerance,
                loglikelihood_burn
            )
        # Otherwise, update the filter parameters
        else:
            kalman_filter = self._kalman_filters[prefix]
            kalman_filter.set_filter_method(filter_method, False)
            kalman_filter.inversion_method = inversion_method
            kalman_filter.stability_method = stability_method
            kalman_filter.filter_timing = filter_timing
            kalman_filter.tolerance = tolerance
            # conserve_memory and loglikelihood_burn changes always lead to
            # re-created filters

        return prefix, dtype, create_filter, create_statespace

    def set_filter_method(self, filter_method=None, **kwargs):
        r"""
        Set the filtering method
        The filtering method controls aspects of which Kalman filtering
        approach will be used.
        Parameters
        ----------
        filter_method : integer, optional
            Bitmask value to set the filter method to. See notes for details.
        **kwargs
            Keyword arguments may be used to influence the filter method by
            setting individual boolean flags. See notes for details.
        Notes
        -----
        The filtering method is defined by a collection of boolean flags, and
        is internally stored as a bitmask. The methods available are:
        FILTER_CONVENTIONAL = 0x01
            Conventional Kalman filter.
        FILTER_UNIVARIATE = 0x10
            Univariate approach to Kalman filtering. Overrides conventional
            method if both are specified.
        FILTER_COLLAPSED = 0x20
            Collapsed approach to Kalman filtering. Will be used *in addition*
            to conventional or univariate filtering.
        Note that only the first method is available if using a Scipy version
        older than 0.16.
        If the bitmask is set directly via the `filter_method` argument, then
        the full method must be provided.
        If keyword arguments are used to set individual boolean flags, then
        the lowercase of the method must be used as an argument name, and the
        value is the desired value of the boolean flag (True or False).
        Note that the filter method may also be specified by directly modifying
        the class attributes which are defined similarly to the keyword
        arguments.
        The default filtering method is FILTER_CONVENTIONAL.
        Examples
        --------
        >>> mod = sm.tsa.statespace.SARIMAX(range(10))
        >>> mod.filter_method
        1
        >>> mod.filter_conventional
        True
        >>> mod.filter_univariate = True
        >>> mod.filter_method
        17
        >>> mod.set_filter_method(filter_univariate=False,
                                  filter_collapsed=True)
        >>> mod.filter_method
        33
        >>> mod.set_filter_method(filter_method=1)
        >>> mod.filter_conventional
        True
        >>> mod.filter_univariate
        False
        >>> mod.filter_collapsed
        False
        >>> mod.filter_univariate = True
        >>> mod.filter_method
        17
        """
        if filter_method is not None:
            self.filter_method = filter_method
        for name in KalmanFilter.filter_methods:
            if name in kwargs:
                setattr(self, name, kwargs[name])

        if self._compatibility_mode and not self.filter_method == 1:
            raise NotImplementedError('Only conventional Kalman filtering'
                                      ' is available. Consider updating'
                                      ' dependencies for more options.')

    def set_inversion_method(self, inversion_method=None, **kwargs):
        r"""
        Set the inversion method
        The Kalman filter may contain one matrix inversion: that of the
        forecast error covariance matrix. The inversion method controls how and
        if that inverse is performed.
        Parameters
        ----------
        inversion_method : integer, optional
            Bitmask value to set the inversion method to. See notes for
            details.
        **kwargs
            Keyword arguments may be used to influence the inversion method by
            setting individual boolean flags. See notes for details.
        Notes
        -----
        The inversion method is defined by a collection of boolean flags, and
        is internally stored as a bitmask. The methods available are:
        INVERT_UNIVARIATE = 0x01
            If the endogenous time series is univariate, then inversion can be
            performed by simple division. If this flag is set and the time
            series is univariate, then division will always be used even if
            other flags are also set.
        SOLVE_LU = 0x02
            Use an LU decomposition along with a linear solver (rather than
            ever actually inverting the matrix).
        INVERT_LU = 0x04
            Use an LU decomposition along with typical matrix inversion.
        SOLVE_CHOLESKY = 0x08
            Use a Cholesky decomposition along with a linear solver.
        INVERT_CHOLESKY = 0x10
            Use an Cholesky decomposition along with typical matrix inversion.
        If the bitmask is set directly via the `inversion_method` argument,
        then the full method must be provided.
        If keyword arguments are used to set individual boolean flags, then
        the lowercase of the method must be used as an argument name, and the
        value is the desired value of the boolean flag (True or False).
        Note that the inversion method may also be specified by directly
        modifying the class attributes which are defined similarly to the
        keyword arguments.
        The default inversion method is `INVERT_UNIVARIATE | SOLVE_CHOLESKY`
        Several things to keep in mind are:
        - If the filtering method is specified to be univariate, then simple
          division is always used regardless of the dimension of the endogenous
          time series.
        - Cholesky decomposition is about twice as fast as LU decomposition,
          but it requires that the matrix be positive definite. While this
          should generally be true, it may not be in every case.
        - Using a linear solver rather than true matrix inversion is generally
          faster and is numerically more stable.
        Examples
        --------
        >>> mod = sm.tsa.statespace.SARIMAX(range(10))
        >>> mod.inversion_method
        1
        >>> mod.solve_cholesky
        True
        >>> mod.invert_univariate
        True
        >>> mod.invert_lu
        False
        >>> mod.invert_univariate = False
        >>> mod.inversion_method
        8
        >>> mod.set_inversion_method(solve_cholesky=False,
                                     invert_cholesky=True)
        >>> mod.inversion_method
        16
        """
        if inversion_method is not None:
            self.inversion_method = inversion_method
        for name in KalmanFilter.inversion_methods:
            if name in kwargs:
                setattr(self, name, kwargs[name])

    def set_stability_method(self, stability_method=None, **kwargs):
        r"""
        Set the numerical stability method
        The Kalman filter is a recursive algorithm that may in some cases
        suffer issues with numerical stability. The stability method controls
        what, if any, measures are taken to promote stability.
        Parameters
        ----------
        stability_method : integer, optional
            Bitmask value to set the stability method to. See notes for
            details.
        **kwargs
            Keyword arguments may be used to influence the stability method by
            setting individual boolean flags. See notes for details.
        Notes
        -----
        The stability method is defined by a collection of boolean flags, and
        is internally stored as a bitmask. The methods available are:
        STABILITY_FORCE_SYMMETRY = 0x01
            If this flag is set, symmetry of the predicted state covariance
            matrix is enforced at each iteration of the filter, where each
            element is set to the average of the corresponding elements in the
            upper and lower triangle.
        If the bitmask is set directly via the `stability_method` argument,
        then the full method must be provided.
        If keyword arguments are used to set individual boolean flags, then
        the lowercase of the method must be used as an argument name, and the
        value is the desired value of the boolean flag (True or False).
        Note that the stability method may also be specified by directly
        modifying the class attributes which are defined similarly to the
        keyword arguments.
        The default stability method is `STABILITY_FORCE_SYMMETRY`
        Examples
        --------
        >>> mod = sm.tsa.statespace.SARIMAX(range(10))
        >>> mod.stability_method
        1
        >>> mod.stability_force_symmetry
        True
        >>> mod.stability_force_symmetry = False
        >>> mod.stability_method
        0
        """
        if stability_method is not None:
            self.stability_method = stability_method
        for name in KalmanFilter.stability_methods:
            if name in kwargs:
                setattr(self, name, kwargs[name])

    def set_conserve_memory(self, conserve_memory=None, **kwargs):
        r"""
        Set the memory conservation method
        By default, the Kalman filter computes a number of intermediate
        matrices at each iteration. The memory conservation options control
        which of those matrices are stored.
        Parameters
        ----------
        conserve_memory : integer, optional
            Bitmask value to set the memory conservation method to. See notes
            for details.
        **kwargs
            Keyword arguments may be used to influence the memory conservation
            method by setting individual boolean flags. See notes for details.
        Notes
        -----
        The memory conservation method is defined by a collection of boolean
        flags, and is internally stored as a bitmask. The methods available
        are:
        MEMORY_STORE_ALL = 0
            Store all intermediate matrices. This is the default value.
        MEMORY_NO_FORECAST = 0x01
            Do not store the forecast, forecast error, or forecast error
            covariance matrices. If this option is used, the `predict` method
            from the results class is unavailable.
        MEMORY_NO_PREDICTED = 0x02
            Do not store the predicted state or predicted state covariance
            matrices.
        MEMORY_NO_FILTERED = 0x04
            Do not store the filtered state or filtered state covariance
            matrices.
        MEMORY_NO_LIKELIHOOD = 0x08
            Do not store the vector of loglikelihood values for each
            observation. Only the sum of the loglikelihood values is stored.
        MEMORY_NO_GAIN = 0x10
            Do not store the Kalman gain matrices.
        MEMORY_NO_SMOOTHING = 0x20
            Do not store temporary variables related to Klaman smoothing. If
            this option is used, smoothing is unavailable.
        MEMORY_CONSERVE
            Do not store any intermediate matrices.
        Note that if using a Scipy version less than 0.16, the options
        MEMORY_NO_GAIN and MEMORY_NO_SMOOTHING have no effect.
        If the bitmask is set directly via the `conserve_memory` argument,
        then the full method must be provided.
        If keyword arguments are used to set individual boolean flags, then
        the lowercase of the method must be used as an argument name, and the
        value is the desired value of the boolean flag (True or False).
        Note that the memory conservation method may also be specified by
        directly modifying the class attributes which are defined similarly to
        the keyword arguments.
        The default memory conservation method is `MEMORY_STORE_ALL`, so that
        all intermediate matrices are stored.
        Examples
        --------
        >>> mod = sm.tsa.statespace.SARIMAX(range(10))
        >>> mod.conserve_memory
        0
        >>> mod.memory_no_predicted
        False
        >>> mod.memory_no_predicted = True
        >>> mod.conserve_memory
        2
        >>> mod.set_conserve_memory(memory_no_filtered=True,
                                    memory_no_forecast=True)
        >>> mod.conserve_memory
        7
        """
        if conserve_memory is not None:
            self.conserve_memory = conserve_memory
        for name in KalmanFilter.memory_options:
            if name in kwargs:
                setattr(self, name, kwargs[name])

    def set_filter_timing(self, alternate_timing=None, **kwargs):
        r"""
        Set the filter timing convention
        By default, the Kalman filter follows Durbin and Koopman, 2012, in
        initializing the filter with predicted values. Kim and Nelson, 1999,
        instead initialize the filter with filtered values, which is
        essentially just a different timing convention.
        Parameters
        ----------
        alternate_timing : integer, optional
            Whether or not to use the alternate timing convention. Default is
            unspecified.
        **kwargs
            Keyword arguments may be used to influence the memory conservation
            method by setting individual boolean flags. See notes for details.
        """
        if alternate_timing is not None:
            self.filter_timing = int(alternate_timing)
        if 'timing_init_predicted' in kwargs:
            self.filter_timing = int(not kwargs['timing_init_predicted'])
        if 'timing_init_filtered' in kwargs:
            self.filter_timing = int(kwargs['timing_init_filtered'])

        if (self._compatibility_mode and
                self.filter_timing == TIMING_INIT_FILTERED):
            raise NotImplementedError('Only "predicted" Kalman filter'
                                      ' timing is available. Consider'
                                      ' updating dependencies for more'
                                      ' options.')

    def _filter(self, filter_method=None, inversion_method=None,
                stability_method=None, conserve_memory=None,
                filter_timing=None, tolerance=None, loglikelihood_burn=None,
                complex_step=False):
        # Initialize the filter
        prefix, dtype, create_filter, create_statespace = (
            self._initialize_filter(
                filter_method, inversion_method, stability_method,
                conserve_memory, filter_timing, tolerance, loglikelihood_burn
            )
        )
        kfilter = self._kalman_filters[prefix]

        # Initialize the state
        self._initialize_state(prefix=prefix, complex_step=complex_step)

        # Run the filter
        kfilter()

        return kfilter

    def filter(self, filter_method=None, inversion_method=None,
               stability_method=None, conserve_memory=None, filter_timing=None,
               tolerance=None, loglikelihood_burn=None, complex_step=False):
        r"""
        Apply the Kalman filter to the statespace model.
        Parameters
        ----------
        filter_method : int, optional
            Determines which Kalman filter to use. Default is conventional.
        inversion_method : int, optional
            Determines which inversion technique to use. Default is by Cholesky
            decomposition.
        stability_method : int, optional
            Determines which numerical stability techniques to use. Default is
            to enforce symmetry of the predicted state covariance matrix.
        conserve_memory : int, optional
            Determines what output from the filter to store. Default is to
            store everything.
        filter_timing : int, optional
            Determines the timing convention of the filter. Default is that
            from Durbin and Koopman (2012), in which the filter is initialized
            with predicted values.
        tolerance : float, optional
            The tolerance at which the Kalman filter determines convergence to
            steady-state. Default is 1e-19.
        loglikelihood_burn : int, optional
            The number of initial periods during which the loglikelihood is not
            recorded. Default is 0.
        Notes
        -----
        This function by default does not compute variables required for
        smoothing.
        """
        if conserve_memory is None:
            conserve_memory = self.conserve_memory | MEMORY_NO_SMOOTHING

        # Run the filter
        kfilter = self._filter(
            filter_method, inversion_method, stability_method, conserve_memory,
            filter_timing, tolerance, loglikelihood_burn, complex_step)

        # Create the results object
        results = self.results_class(self)
        results.update_representation(self)
        results.update_filter(kfilter)

        return results

    def loglike(self, **kwargs):
        r"""
        Calculate the loglikelihood associated with the statespace model.
        Parameters
        ----------
        **kwargs
            Additional keyword arguments to pass to the Kalman filter. See
            `KalmanFilter.filter` for more details.
        Returns
        -------
        loglike : float
            The joint loglikelihood.
        """
        if self.memory_no_likelihood:
            raise RuntimeError('Cannot compute loglikelihood if'
                               ' MEMORY_NO_LIKELIHOOD option is selected.')
        kwargs['conserve_memory'] = MEMORY_CONSERVE ^ MEMORY_NO_LIKELIHOOD
        kfilter = self._filter(**kwargs)
        loglikelihood_burn = kwargs.get('loglikelihood_burn',
                                        self.loglikelihood_burn)
        return np.sum(kfilter.loglikelihood[loglikelihood_burn:])

    def loglikeobs(self, **kwargs):
        r"""
        Calculate the loglikelihood for each observation associated with the
        statespace model.
        Parameters
        ----------
        **kwargs
            Additional keyword arguments to pass to the Kalman filter. See
            `KalmanFilter.filter` for more details.
        Notes
        -----
        If `loglikelihood_burn` is positive, then the entries in the returned
        loglikelihood vector are set to be zero for those initial time periods.
        Returns
        -------
        loglike : array of float
            Array of loglikelihood values for each observation.
        """
        if self.memory_no_likelihood:
            raise RuntimeError('Cannot compute loglikelihood if'
                               ' MEMORY_NO_LIKELIHOOD option is selected.')
        kwargs['conserve_memory'] = MEMORY_CONSERVE ^ MEMORY_NO_LIKELIHOOD
        kfilter = self._filter(**kwargs)
        llf_obs = np.array(kfilter.loglikelihood, copy=True)

        # Set any burned observations to have zero likelihood
        loglikelihood_burn = kwargs.get('loglikelihood_burn',
                                        self.loglikelihood_burn)
        llf_obs[:loglikelihood_burn] = 0

        return llf_obs

    def simulate(self, nsimulations, measurement_shocks=None,
                 state_shocks=None, initial_state=None):
        r"""
        Simulate a new time series following the state space model
        Parameters
        ----------
        nsimulations : int
            The number of observations to simulate. If the model is
            time-invariant this can be any number. If the model is
            time-varying, then this number must be less than or equal to the
            number
        measurement_shocks : array_like, optional
            If specified, these are the shocks to the measurement equation,
            :math:`\varepsilon_t`. If unspecified, these are automatically
            generated using a pseudo-random number generator. If specified,
            must be shaped `nsimulations` x `k_endog`, where `k_endog` is the
            same as in the state space model.
        state_shocks : array_like, optional
            If specified, these are the shocks to the state equation,
            :math:`\eta_t`. If unspecified, these are automatically
            generated using a pseudo-random number generator. If specified,
            must be shaped `nsimulations` x `k_posdef` where `k_posdef` is the
            same as in the state space model.
        initial_state : array_like, optional
            If specified, this is the state vector at time zero, which should
            be shaped (`k_states` x 1), where `k_states` is the same as in the
            state space model. If unspecified, but the model has been
            initialized, then that initialization is used. If unspecified and
            the model has not been initialized, then a vector of zeros is used.
            Note that this is not included in the returned `simulated_states`
            array.
        Returns
        -------
        simulated_obs : array
            An (nsimulations x k_endog) array of simulated observations.
        simulated_states : array
            An (nsimulations x k_states) array of simulated states.
        """
        time_invariant = self.time_invariant
        # Check for valid number of simulations
        if not time_invariant and nsimulations > self.nobs:
            raise ValueError('In a time-varying model, cannot create more'
                             ' simulations than there are observations.')

        # Check / generate measurement shocks
        if measurement_shocks is not None:
            measurement_shocks = np.array(measurement_shocks)
            if measurement_shocks.ndim == 0:
                measurement_shocks = measurement_shocks[np.newaxis, np.newaxis]
            elif measurement_shocks.ndim == 1:
                measurement_shocks = measurement_shocks[:, np.newaxis]
            if not measurement_shocks.shape == (nsimulations, self.k_endog):
                raise ValueError('Invalid shape of provided measurement'
                                 ' shocks. Required (%d, %d)'
                                 % (nsimulations, self.k_endog))
        elif self.shapes['obs_cov'][-1] == 1:
            measurement_shocks = np.random.multivariate_normal(
                mean=np.zeros(self.k_endog), cov=self['obs_cov'],
                size=nsimulations)

        # Check / generate state shocks
        if state_shocks is not None:
            state_shocks = np.array(state_shocks)
            if state_shocks.ndim == 0:
                state_shocks = state_shocks[np.newaxis, np.newaxis]
            elif state_shocks.ndim == 1:
                state_shocks = state_shocks[:, np.newaxis]
            if not state_shocks.shape == (nsimulations, self.k_posdef):
                raise ValueError('Invalid shape of provided state shocks.'
                                 ' Required (%d, %d).'
                                 % (nsimulations, self.k_posdef))
        elif self.shapes['state_cov'][-1] == 1:
            state_shocks = np.random.multivariate_normal(
                mean=np.zeros(self.k_posdef), cov=self['state_cov'],
                size=nsimulations)

        # Get the initial states
        if initial_state is not None:
            initial_state = np.array(initial_state)
            if initial_state.ndim == 0:
                initial_state = initial_state[np.newaxis]
            elif (initial_state.ndim > 1 and
                  not initial_state.shape == (self.k_states, 1)):
                raise ValueError('Invalid shape of provided initial state'
                                 ' vector. Required (%d, 1)' % self.k_states)
        elif self.initialization == 'known':
            initial_state = self._initial_state
        elif self.initialization in ['approximate_diffuse', 'stationary']:
            initial_state = np.zeros(self.k_states)
        else:
            initial_state = np.zeros(self.k_states)

        return self._simulate(nsimulations, measurement_shocks, state_shocks,
                              initial_state)

    def _simulate(self, nsimulations, measurement_shocks, state_shocks,
                  initial_state):
        time_invariant = self.time_invariant

        # Holding variables for the simulations
        simulated_obs = np.zeros((nsimulations, self.k_endog),
                                 dtype=self.dtype)
        simulated_states = np.zeros((nsimulations+1, self.k_states),
                                    dtype=self.dtype)
        simulated_states[0] = initial_state

        # Perform iterations to create the new time series
        obs_intercept_t = 0
        design_t = 0
        state_intercept_t = 0
        transition_t = 0
        selection_t = 0
        for t in range(nsimulations):
            # Get the current shocks (this accomodates time-varying matrices)
            if measurement_shocks is None:
                measurement_shock = np.random.multivariate_normal(
                    mean=np.zeros(self.k_endog), cov=self['obs_cov', :, :, t])
            else:
                measurement_shock = measurement_shocks[t]

            if state_shocks is None:
                state_shock = np.random.multivariate_normal(
                    mean=np.zeros(self.k_posdef),
                    cov=self['state_cov', :, :, t])
            else:
                state_shock = state_shocks[t]

            # Get current-iteration matrices
            if not time_invariant:
                obs_intercept_t = 0 if self.obs_intercept.shape[-1] == 1 else t
                design_t = 0 if self.design.shape[-1] == 1 else t
                state_intercept_t = (
                    0 if self.state_intercept.shape[-1] == 1 else t)
                transition_t = 0 if self.transition.shape[-1] == 1 else t
                selection_t = 0 if self.selection.shape[-1] == 1 else t

            obs_intercept = self['obs_intercept', :, obs_intercept_t]
            design = self['design', :, :, design_t]
            state_intercept = self['state_intercept', :, state_intercept_t]
            transition = self['transition', :, :, transition_t]
            selection = self['selection', :, :, selection_t]

            # Iterate the measurement equation
            simulated_obs[t] = (
                obs_intercept + np.dot(design, simulated_states[t]) +
                measurement_shock)

            # Iterate the state equation
            simulated_states[t+1] = (
                state_intercept + np.dot(transition, simulated_states[t]) +
                np.dot(selection, state_shock))

        return simulated_obs, simulated_states[:-1]

    def impulse_responses(self, steps=10, impulse=0, orthogonalized=False,
                          cumulative=False, **kwargs):
        r"""
        Impulse response function
        Parameters
        ----------
        steps : int, optional
            The number of steps for which impulse responses are calculated.
            Default is 10. Note that the initial impulse is not counted as a
            step, so if `steps=1`, the output will have 2 entries.
        impulse : int or array_like
            If an integer, the state innovation to pulse; must be between 0
            and `k_posdef-1` where `k_posdef` is the same as in the state
            space model. Alternatively, a custom impulse vector may be
            provided; must be a column vector with shape `(k_posdef, 1)`.
        orthogonalized : boolean, optional
            Whether or not to perform impulse using orthogonalized innovations.
            Note that this will also affect custum `impulse` vectors. Default
            is False.
        cumulative : boolean, optional
            Whether or not to return cumulative impulse responses. Default is
            False.
        **kwargs
            If the model is time-varying and `steps` is greater than the number
            of observations, any of the state space representation matrices
            that are time-varying must have updated values provided for the
            out-of-sample steps.
            For example, if `design` is a time-varying component, `nobs` is 10,
            and `steps` is 15, a (`k_endog` x `k_states` x 5) matrix must be
            provided with the new design matrix values.
        Returns
        -------
        impulse_responses : array
            Responses for each endogenous variable due to the impulse
            given by the `impulse` argument. A (steps + 1 x k_endog) array.
        Notes
        -----
        Intercepts in the measurement and state equation are ignored when
        calculating impulse responses.
        """
        # Since the first step is the impulse itself, we actually want steps+1
        steps += 1

        # Check for what kind of impulse we want
        if type(impulse) == int:
            if impulse >= self.k_posdef or impulse < 0:
                raise ValueError('Invalid value for `impulse`. Must be the'
                                 ' index of one of the state innovations.')

            # Create the (non-orthogonalized) impulse vector
            idx = impulse
            impulse = np.zeros(self.k_posdef)
            impulse[idx] = 1
        else:
            impulse = np.array(impulse)
            if impulse.ndim > 1:
                impulse = np.squeeze(impulse)
            if not impulse.shape == (self.k_posdef,):
                raise ValueError('Invalid impulse vector. Must be shaped'
                                 ' (%d,)' % self.k_posdef)

        # Orthogonalize the impulses, if requested, using Cholesky on the
        # first state covariance matrix
        if orthogonalized:
            state_chol = np.linalg.cholesky(self.state_cov[:, :, 0])
            impulse = np.dot(state_chol, impulse)

        # If we have a time-invariant system, we can solve for the IRF directly
        if self.time_invariant:
            # Get the state space matrices
            design = self.design[:, :, 0]
            transition = self.transition[:, :, 0]
            selection = self.selection[:, :, 0]

            # Holding arrays
            irf = np.zeros((steps, self.k_endog), dtype=self.dtype)
            states = np.zeros((steps, self.k_states), dtype=self.dtype)

            # First iteration
            states[0] = np.dot(selection, impulse)
            irf[0] = np.dot(design, states[0])

            # Iterations
            for t in range(1, steps):
                states[t] = np.dot(transition, states[t-1])
                irf[t] = np.dot(design, states[t])

        # Otherwise, create a new model
        else:
            # Get the basic model components
            representation = {}
            for name, shape in self.shapes.items():
                if name in ['obs', 'obs_intercept', 'state_intercept']:
                    continue
                representation[name] = getattr(self, name)

            # Allow additional specification
            warning = ('Model has time-invariant %s matrix, so the %s'
                       ' argument to `irf` has been ignored.')
            exception = ('Impulse response functions for models with'
                         ' time-varying %s matrix requires an updated'
                         ' time-varying matrix for any periods beyond those in'
                         ' the original model.')
            for name, shape in self.shapes.items():
                if name in ['obs', 'obs_intercept', 'state_intercept']:
                    continue
                if representation[name].shape[-1] == 1:
                    if name in kwargs:
                        warn(warning % (name, name))
                elif name not in kwargs:
                    raise ValueError(exception % name)
                else:
                    mat = np.asarray(kwargs[name])
                    validate_matrix_shape(name, mat.shape, shape[0],
                                          shape[1], nforecast)
                    if mat.ndim < 3 or not mat.shape[2] == nforecast:
                        raise ValueError(exception % name)
                    representation[name] = np.c_[representation[name], mat]

            # Setup the new statespace representation
            model_kwargs = {
                'filter_method': self.filter_method,
                'inversion_method': self.inversion_method,
                'stability_method': self.stability_method,
                'conserve_memory': self.conserve_memory,
                'tolerance': self.tolerance,
                'loglikelihood_burn': self.loglikelihood_burn
            }
            model_kwargs.update(representation)
            model = KalmanFilter(np.zeros(self.endog.T.shape), self.k_states,
                                 self.k_posdef, **model_kwargs)
            model.initialize_approximate_diffuse()
            model._initialize_filter()
            model._initialize_state()

            # Get the impulse response function via simulation of the state
            # space model, but with other shocks set to zero
            measurement_shocks = np.zeros((steps, self.k_endog))
            state_shocks = np.zeros((steps, self.k_posdef))
            state_shocks[0] = impulse
            irf, _ = model.simulate(
                steps, measurement_shocks=measurement_shocks,
                state_shocks=state_shocks)

        # Get the cumulative response if requested
        if cumulative:
            irf = np.cumsum(irf, axis=0)

        return irf


class FilterResults(FrozenRepresentation):
    """
    Results from applying the Kalman filter to a state space model.
    Parameters
    ----------
    model : Representation
        A Statespace representation
    Attributes
    ----------
    nobs : int
        Number of observations.
    k_endog : int
        The dimension of the observation series.
    k_states : int
        The dimension of the unobserved state process.
    k_posdef : int
        The dimension of a guaranteed positive definite
        covariance matrix describing the shocks in the
        measurement equation.
    dtype : dtype
        Datatype of representation matrices
    prefix : str
        BLAS prefix of representation matrices
    shapes : dictionary of name,tuple
        A dictionary recording the shapes of each of the
        representation matrices as tuples.
    endog : array
        The observation vector.
    design : array
        The design matrix, :math:`Z`.
    obs_intercept : array
        The intercept for the observation equation, :math:`d`.
    obs_cov : array
        The covariance matrix for the observation equation :math:`H`.
    transition : array
        The transition matrix, :math:`T`.
    state_intercept : array
        The intercept for the transition equation, :math:`c`.
    selection : array
        The selection matrix, :math:`R`.
    state_cov : array
        The covariance matrix for the state equation :math:`Q`.
    missing : array of bool
        An array of the same size as `endog`, filled
        with boolean values that are True if the
        corresponding entry in `endog` is NaN and False
        otherwise.
    nmissing : array of int
        An array of size `nobs`, where the ith entry
        is the number (between 0 and `k_endog`) of NaNs in
        the ith row of the `endog` array.
    time_invariant : bool
        Whether or not the representation matrices are time-invariant
    initialization : str
        Kalman filter initialization method.
    initial_state : array_like
        The state vector used to initialize the Kalamn filter.
    initial_state_cov : array_like
        The state covariance matrix used to initialize the Kalamn filter.
    filter_method : int
        Bitmask representing the Kalman filtering method
    inversion_method : int
        Bitmask representing the method used to
        invert the forecast error covariance matrix.
    stability_method : int
        Bitmask representing the methods used to promote
        numerical stability in the Kalman filter
        recursions.
    conserve_memory : int
        Bitmask representing the selected memory conservation method.
    filter_timing : int
        Whether or not to use the alternate timing convention.
    tolerance : float
        The tolerance at which the Kalman filter
        determines convergence to steady-state.
    loglikelihood_burn : int
        The number of initial periods during which
        the loglikelihood is not recorded.
    converged : bool
        Whether or not the Kalman filter converged.
    period_converged : int
        The time period in which the Kalman filter converged.
    filtered_state : array
        The filtered state vector at each time period.
    filtered_state_cov : array
        The filtered state covariance matrix at each time period.
    predicted_state : array
        The predicted state vector at each time period.
    predicted_state_cov : array
        The predicted state covariance matrix at each time period.
    kalman_gain : array
        The Kalman gain at each time period.
    forecasts : array
        The one-step-ahead forecasts of observations at each time period.
    forecasts_error : array
        The forecast errors at each time period.
    forecasts_error_cov : array
        The forecast error covariance matrices at each time period.
    llf_obs : array
        The loglikelihood values at each time period.
    """
    _filter_attributes = [
        'filter_method', 'inversion_method', 'stability_method',
        'conserve_memory', 'filter_timing', 'tolerance', 'loglikelihood_burn',
        'converged', 'period_converged', 'filtered_state',
        'filtered_state_cov', 'predicted_state', 'predicted_state_cov',
        'tmp1', 'tmp2', 'tmp3', 'tmp4', 'forecasts',
        'forecasts_error', 'forecasts_error_cov', 'llf_obs',
        'collapsed_forecasts', 'collapsed_forecasts_error',
        'collapsed_forecasts_error_cov',
    ]

    _filter_options = (
        KalmanFilter.filter_methods + KalmanFilter.stability_methods +
        KalmanFilter.inversion_methods + KalmanFilter.memory_options
    )

    _attributes = FrozenRepresentation._model_attributes + _filter_attributes

    def __init__(self, model):
        super(FilterResults, self).__init__(model)

        # Setup caches for uninitialized objects
        self._kalman_gain = None
        self._standardized_forecasts_error = None

    def update_representation(self, model, only_options=False):
        """
        Update the results to match a given model
        Parameters
        ----------
        model : Representation
            The model object from which to take the updated values.
        only_options : boolean, optional
            If set to true, only the filter options are updated, and the state
            space representation is not updated. Default is False.
        Notes
        -----
        This method is rarely required except for internal usage.
        """
        if not only_options:
            super(FilterResults, self).update_representation(model)

        # Save the options as boolean variables
        for name in self._filter_options:
            setattr(self, name, getattr(model, name, None))

    def update_filter(self, kalman_filter):
        """
        Update the filter results
        Parameters
        ----------
        kalman_filter : KalmanFilter
            The model object from which to take the updated values.
        Notes
        -----
        This method is rarely required except for internal usage.
        """
        # State initialization
        self.initial_state = np.array(
            kalman_filter.model.initial_state, copy=True
        )
        self.initial_state_cov = np.array(
            kalman_filter.model.initial_state_cov, copy=True
        )

        # Save Kalman filter parameters
        self.filter_method = kalman_filter.filter_method
        self.inversion_method = kalman_filter.inversion_method
        self.stability_method = kalman_filter.stability_method
        self.conserve_memory = kalman_filter.conserve_memory
        self.filter_timing = kalman_filter.filter_timing
        self.tolerance = kalman_filter.tolerance
        self.loglikelihood_burn = kalman_filter.loglikelihood_burn

        # Save Kalman filter output
        self.converged = bool(kalman_filter.converged)
        self.period_converged = kalman_filter.period_converged

        self.filtered_state = np.array(kalman_filter.filtered_state, copy=True)
        self.filtered_state_cov = np.array(
            kalman_filter.filtered_state_cov, copy=True
        )
        self.predicted_state = np.array(
            kalman_filter.predicted_state, copy=True
        )
        self.predicted_state_cov = np.array(
            kalman_filter.predicted_state_cov, copy=True
        )

        # Reset caches
        self._standardized_forecasts_error = None
        if not self._compatibility_mode:
            has_missing = np.sum(self.nmissing) > 0
            # In the partially missing data case, all entries will
            # be in the upper left submatrix rather than the correct placement
            # Re-ordering does not make sense in the collapsed case.
            if has_missing and (not self.memory_no_gain and
                                not self.filter_collapsed):
                self._kalman_gain = np.array(reorder_missing_matrix(
                    kalman_filter.kalman_gain, self.missing, reorder_cols=True,
                    prefix=self.prefix))
                self.tmp1 = np.array(reorder_missing_matrix(
                    kalman_filter.tmp1, self.missing, reorder_cols=True,
                    prefix=self.prefix))
                self.tmp2 = np.array(reorder_missing_vector(
                    kalman_filter.tmp2, self.missing, prefix=self.prefix))
                self.tmp3 = np.array(reorder_missing_matrix(
                    kalman_filter.tmp3, self.missing, reorder_rows=True,
                    prefix=self.prefix))
                self.tmp4 = np.array(reorder_missing_matrix(
                    kalman_filter.tmp4, self.missing, reorder_cols=True,
                    reorder_rows=True, prefix=self.prefix))
            else:
                self._kalman_gain = np.array(
                    kalman_filter.kalman_gain, copy=True)
                self.tmp1 = np.array(kalman_filter.tmp1, copy=True)
                self.tmp2 = np.array(kalman_filter.tmp2, copy=True)
                self.tmp3 = np.array(kalman_filter.tmp3, copy=True)
                self.tmp4 = np.array(kalman_filter.tmp4, copy=True)
        else:
            self._kalman_gain = None

        # Note: use forecasts rather than forecast, so as not to interfer
        # with the `forecast` methods in subclasses
        self.forecasts = np.array(kalman_filter.forecast, copy=True)
        self.forecasts_error = np.array(
            kalman_filter.forecast_error, copy=True
        )
        self.forecasts_error_cov = np.array(
            kalman_filter.forecast_error_cov, copy=True
        )
        self.llf_obs = np.array(kalman_filter.loglikelihood, copy=True)

        # If there was missing data, save the original values from the Kalman
        # filter output, since below will set the values corresponding to
        # the missing observations to nans.
        self.missing_forecasts = None
        self.missing_forecasts_error = None
        self.missing_forecasts_error_cov = None
        if np.sum(self.nmissing) > 0:
            # Copy the provided arrays (which are as the Kalman filter dataset)
            # into new variables
            self.missing_forecasts = np.copy(self.forecasts)
            self.missing_forecasts_error = np.copy(self.forecasts_error)
            self.missing_forecasts_error_cov = (
                np.copy(self.forecasts_error_cov)
            )

        # Save the collapsed values
        self.collapsed_forecasts = None
        self.collapsed_forecasts_error = None
        self.collapsed_forecasts_error_cov = None
        if self.filter_collapsed:
            # Copy the provided arrays (which are from the collapsed dataset)
            # into new variables
            self.collapsed_forecasts = self.forecasts[:self.k_states, :]
            self.collapsed_forecasts_error = (
                self.forecasts_error[:self.k_states, :]
            )
            self.collapsed_forecasts_error_cov = (
                self.forecasts_error_cov[:self.k_states, :self.k_states, :]
            )
            # Recreate the original arrays (which should be from the original
            # dataset) in the appropriate dimension
            self.forecasts = np.zeros((self.k_endog, self.nobs))
            self.forecasts_error = np.zeros((self.k_endog, self.nobs))
            self.forecasts_error_cov = (
                np.zeros((self.k_endog, self.k_endog, self.nobs))
            )

        # Fill in missing values in the forecast, forecast error, and
        # forecast error covariance matrix (this is required due to how the
        # Kalman filter implements observations that are either partly or
        # completely missing)
        # Construct the predictions, forecasts
        if not (self.memory_no_forecast or self.memory_no_predicted):
            for t in range(self.nobs):
                design_t = 0 if self.design.shape[2] == 1 else t
                obs_cov_t = 0 if self.obs_cov.shape[2] == 1 else t
                obs_intercept_t = 0 if self.obs_intercept.shape[1] == 1 else t

                # For completely missing observations, the Kalman filter will
                # produce forecasts, but forecast errors and the forecast
                # error covariance matrix will be zeros - make them nan to
                # improve clarity of results.
                if self.nmissing[t] > 0:
                    mask = ~self.missing[:, t].astype(bool)
                    # We can recover forecasts
                    # For partially missing observations, the Kalman filter
                    # will produce all elements (forecasts, forecast errors,
                    # forecast error covariance matrices) as usual, but their
                    # dimension will only be equal to the number of non-missing
                    # elements, and their location in memory will be in the
                    # first blocks (e.g. for the forecasts_error, the first
                    # k_endog - nmissing[t] columns will be filled in),
                    # regardless of which endogenous variables they refer to
                    # (i.e. the non- missing endogenous variables for that
                    # observation). Furthermore, the forecast error covariance
                    # matrix is only valid for those elements. What is done is
                    # to set all elements to nan for these observations so that
                    # they are flagged as missing. The variables
                    # missing_forecasts, etc. then provide the forecasts, etc.
                    # provided by the Kalman filter, from which the data can be
                    # retrieved if desired.
                    self.forecasts[:, t] = np.dot(
                        self.design[:, :, design_t], self.predicted_state[:, t]
                    ) + self.obs_intercept[:, obs_intercept_t]
                    self.forecasts_error[:, t] = np.nan
                    self.forecasts_error[mask, t] = (
                        self.endog[mask, t] - self.forecasts[mask, t])
                    self.forecasts_error_cov[:, :, t] = np.dot(
                        np.dot(self.design[:, :, design_t],
                               self.predicted_state_cov[:, :, t]),
                        self.design[:, :, design_t].T
                    ) + self.obs_cov[:, :, obs_cov_t]
                # In the collapsed case, everything just needs to be rebuilt
                # for the original observed data, since the Kalman filter
                # produced these values for the collapsed data.
                elif self.filter_collapsed:
                    self.forecasts[:, t] = np.dot(
                        self.design[:, :, design_t], self.predicted_state[:, t]
                    ) + self.obs_intercept[:, obs_intercept_t]

                    self.forecasts_error[:, t] = (
                        self.endog[:, t] - self.forecasts[:, t]
                    )

                    self.forecasts_error_cov[:, :, t] = np.dot(
                        np.dot(self.design[:, :, design_t],
                               self.predicted_state_cov[:, :, t]),
                        self.design[:, :, design_t].T
                    ) + self.obs_cov[:, :, obs_cov_t]

    @property
    def kalman_gain(self):
        """
        Kalman gain matrices
        """
        if self._kalman_gain is None:
            # k x n
            self._kalman_gain = np.zeros(
                (self.k_states, self.k_endog, self.nobs), dtype=self.dtype)
            for t in range(self.nobs):
                # In the case of entirely missing observations, let the Kalman
                # gain be zeros.
                if self.nmissing[t] == self.k_endog:
                    continue

                design_t = 0 if self.design.shape[2] == 1 else t
                transition_t = 0 if self.transition.shape[2] == 1 else t
                if self.nmissing[t] == 0:
                    self._kalman_gain[:, :, t] = np.dot(
                        np.dot(
                            self.transition[:, :, transition_t],
                            self.predicted_state_cov[:, :, t]
                        ),
                        np.dot(
                            np.transpose(self.design[:, :, design_t]),
                            np.linalg.inv(self.forecasts_error_cov[:, :, t])
                        )
                    )
                else:
                    mask = ~self.missing[:, t].astype(bool)
                    n = self.k_endog - self.nmissing[t]
                    F = self.forecasts_error_cov[np.ix_(mask, mask, [t])]
                    self._kalman_gain[:, mask, t] = np.dot(
                        np.dot(
                            self.transition[:, :, transition_t],
                            self.predicted_state_cov[:, :, t]
                        ),
                        np.dot(
                            np.transpose(self.design[mask, :, design_t]),
                            np.linalg.inv(F[:, :, 0])
                        )
                    )
        return self._kalman_gain

    @property
    def standardized_forecasts_error(self):
        """
        Standardized forecast errors
        """
        if self._standardized_forecasts_error is None:
            from scipy import linalg
            self._standardized_forecasts_error = np.zeros(
                self.forecasts_error.shape, dtype=self.dtype)

            for t in range(self.forecasts_error_cov.shape[2]):
                if self.nmissing[t] > 0:
                    self._standardized_forecasts_error[:, t] = np.nan
                if self.nmissing[t] < self.k_endog:
                    mask = ~self.missing[:, t].astype(bool)
                    F = self.forecasts_error_cov[np.ix_(mask, mask, [t])]
                    upper, _ = linalg.cho_factor(F[:, :, 0])
                    self._standardized_forecasts_error[mask, t] = (
                        linalg.solve_triangular(
                            upper, self.forecasts_error[mask, t]
                        )
                    )

        return self._standardized_forecasts_error

    def predict(self, start=None, end=None, dynamic=None, **kwargs):
        r"""
        In-sample and out-of-sample prediction for state space models generally
        Parameters
        ----------
        start : int, optional
            Zero-indexed observation number at which to start forecasting,
            i.e., the first forecast will be at start.
        end : int, optional
            Zero-indexed observation number at which to end forecasting, i.e.,
            the last forecast will be at end.
        dynamic : int, optional
            Offset relative to `start` at which to begin dynamic prediction.
            Prior to this observation, true endogenous values will be used for
            prediction; starting with this observation and continuing through
            the end of prediction, forecasted endogenous values will be used
            instead.
        **kwargs
            If the prediction range is outside of the sample range, any
            of the state space representation matrices that are time-varying
            must have updated values provided for the out-of-sample range.
            For example, of `obs_intercept` is a time-varying component and
            the prediction range extends 10 periods beyond the end of the
            sample, a (`k_endog` x 10) matrix must be provided with the new
            intercept values.
        Returns
        -------
        results : PredictionResults
            A PredictionResults object.
        Notes
        -----
        All prediction is performed by applying the deterministic part of the
        measurement equation using the predicted state variables.
        Out-of-sample prediction first applies the Kalman filter to missing
        data for the number of periods desired to obtain the predicted states.
        """
        # Cannot predict if we do not have appropriate arrays
        if self.memory_no_forecast or self.memory_no_predicted:
            raise ValueError('Predict is not possible if memory conservation'
                             ' has been used to avoid storing forecasts or'
                             ' predicted values.')

        # Get the start and the end of the entire prediction range
        if start is None:
            start = 0
        elif start < 0:
            raise ValueError('Cannot predict values previous to the sample.')
        if end is None:
            end = self.nobs

        # Prediction and forecasting is performed by iterating the Kalman
        # Kalman filter through the entire range [0, end]
        # Then, everything is returned corresponding to the range [start, end].
        # In order to perform the calculations, the range is separately split
        # up into the following categories:
        # - static:   (in-sample) the Kalman filter is run as usual
        # - dynamic:  (in-sample) the Kalman filter is run, but on missing data
        # - forecast: (out-of-sample) the Kalman filter is run, but on missing
        #             data

        # Short-circuit if end is before start
        if end <= start:
            raise ValueError('End of prediction must be after start.')

        # Get the number of forecasts to make after the end of the sample
        nforecast = max(0, end - self.nobs)

        # Get the number of dynamic prediction periods

        # If `dynamic=True`, then assume that we want to begin dynamic
        # prediction at the start of the sample prediction.
        if dynamic is True:
            dynamic = 0
        # If `dynamic=False`, then assume we want no dynamic prediction
        if dynamic is False:
            dynamic = None

        ndynamic = 0
        if dynamic is not None:
            # Replace the relative dynamic offset with an absolute offset
            dynamic = start + dynamic

            # Validate the `dynamic` parameter
            if dynamic < 0:
                raise ValueError('Dynamic prediction cannot begin prior to the'
                                 ' first observation in the sample.')
            elif dynamic > end:
                warn('Dynamic prediction specified to begin after the end of'
                     ' prediction, and so has no effect.')
                dynamic = None
            elif dynamic > self.nobs:
                warn('Dynamic prediction specified to begin during'
                     ' out-of-sample forecasting period, and so has no'
                     ' effect.')
                dynamic = None

            # Get the total size of the desired dynamic forecasting component
            # Note: the first `dynamic` periods of prediction are actually
            # *not* dynamic, because dynamic prediction begins at observation
            # `dynamic`.
            if dynamic is not None:
                ndynamic = max(0, min(end, self.nobs) - dynamic)

        # Get the number of in-sample static predictions
        nstatic = min(end, self.nobs) if dynamic is None else dynamic

        # Construct the design and observation intercept and covariance
        # matrices for start-npadded:end. If not time-varying in the original
        # model, then they will be copied over if none are provided in
        # `kwargs`. Otherwise additional matrices must be provided in `kwargs`.
        representation = {}
        for name, shape in self.shapes.items():
            if name == 'obs':
                continue
            representation[name] = getattr(self, name)

        # Update the matrices from kwargs for forecasts
        warning = ('Model has time-invariant %s matrix, so the %s'
                   ' argument to `predict` has been ignored.')
        exception = ('Forecasting for models with time-varying %s matrix'
                     ' requires an updated time-varying matrix for the'
                     ' period to be forecasted.')
        if nforecast > 0:
            for name, shape in self.shapes.items():
                if name == 'obs':
                    continue
                if representation[name].shape[-1] == 1:
                    if name in kwargs:
                        warn(warning % (name, name))
                elif name not in kwargs:
                    raise ValueError(exception % name)
                else:
                    mat = np.asarray(kwargs[name])
                    if len(shape) == 2:
                        validate_vector_shape(name, mat.shape,
                                              shape[0], nforecast)
                        if mat.ndim < 2 or not mat.shape[1] == nforecast:
                            raise ValueError(exception % name)
                        representation[name] = np.c_[representation[name], mat]
                    else:
                        validate_matrix_shape(name, mat.shape, shape[0],
                                              shape[1], nforecast)
                        if mat.ndim < 3 or not mat.shape[2] == nforecast:
                            raise ValueError(exception % name)
                        representation[name] = np.c_[representation[name], mat]

        # Update the matrices from kwargs for dynamic prediction in the case
        # that `end` is less than `nobs` and `dynamic` is less than `end`. In
        # this case, any time-varying matrices in the default `representation`
        # will be too long, causing an error to be thrown below in the
        # KalmanFilter(...) construction call, because the endog has length
        # nstatic + ndynamic + nforecast, whereas the time-varying matrices
        # from `representation` have length nobs.
        if ndynamic > 0 and end < self.nobs:
            for name, shape in self.shapes.items():
                if not name == 'obs' and representation[name].shape[-1] > 1:
                    representation[name] = representation[name][..., :end]

        # Construct the predicted state and covariance matrix for each time
        # period depending on whether that time period corresponds to
        # one-step-ahead prediction, dynamic prediction, or out-of-sample
        # forecasting.

        # If we only have simple prediction, then we can use the already saved
        # Kalman filter output
        if ndynamic == 0 and nforecast == 0:
            results = self
        else:
            # Construct the new endogenous array.
            endog = np.empty((self.k_endog, ndynamic + nforecast))
            endog.fill(np.nan)
            endog = np.asfortranarray(np.c_[self.endog[:, :nstatic], endog])

            # Setup the new statespace representation
            model_kwargs = {
                'filter_method': self.filter_method,
                'inversion_method': self.inversion_method,
                'stability_method': self.stability_method,
                'conserve_memory': self.conserve_memory,
                'filter_timing': self.filter_timing,
                'tolerance': self.tolerance,
                'loglikelihood_burn': self.loglikelihood_burn
            }
            model_kwargs.update(representation)
            model = KalmanFilter(
                endog, self.k_states, self.k_posdef, **model_kwargs
            )
            model.initialize_known(
                self.initial_state,
                self.initial_state_cov
            )
            model._initialize_filter()
            model._initialize_state()

            results = self._predict(nstatic, ndynamic, nforecast, model)

        return PredictionResults(results, start, end, nstatic, ndynamic,
                                 nforecast)

    def _predict(self, nstatic, ndynamic, nforecast, model):
        # Note: this doesn't use self, and can either be a static method or
        #       moved outside the class altogether.

        # Get the underlying filter
        kfilter = model._kalman_filter

        # Save this (which shares memory with the memoryview on which the
        # Kalman filter will be operating) so that we can replace actual data
        # with predicted data during dynamic forecasting
        endog = model._representations[model.prefix]['obs']

        # print(nstatic, ndynamic, nforecast, model.nobs)

        for t in range(kfilter.model.nobs):
            # Run the Kalman filter for the first `nstatic` periods (for
            # which dynamic computation will not be performed)
            if t < nstatic:
                next(kfilter)
            # Perform dynamic prediction
            elif t < nstatic + ndynamic:
                design_t = 0 if model.design.shape[2] == 1 else t
                obs_intercept_t = 0 if model.obs_intercept.shape[1] == 1 else t

                # Unconditional value is the intercept (often zeros)
                endog[:, t] = model.obs_intercept[:, obs_intercept_t]
                # If t > 0, then we can condition the forecast on the state
                if t > 0:
                    # Predict endog[:, t] given `predicted_state` calculated in
                    # previous iteration (i.e. t-1)
                    endog[:, t] += np.dot(
                        model.design[:, :, design_t],
                        kfilter.predicted_state[:, t]
                    )

                # Advance Kalman filter
                next(kfilter)
            # Perform any (one-step-ahead) forecasting
            else:
                next(kfilter)

        # Return the predicted state and predicted state covariance matrices
        results = FilterResults(model)
        results.update_representation(model)
        results.update_filter(kfilter)
        return results


class PredictionResults(FilterResults):
    r"""
    Results of in-sample and out-of-sample prediction for state space models
    generally
    Parameters
    ----------
    results : FilterResults
        Output from filtering, corresponding to the prediction desired
    start : int
        Zero-indexed observation number at which to start forecasting,
        i.e., the first forecast will be at start.
    end : int
        Zero-indexed observation number at which to end forecasting, i.e.,
        the last forecast will be at end.
    nstatic : int
        Number of in-sample static predictions (these are always the first
        elements of the prediction output).
    ndynamic : int
        Number of in-sample dynamic predictions (these always follow the static
        predictions directly, and are directly followed by the forecasts).
    nforecast : int
        Number of in-sample forecasts (these always follow the dynamic
        predictions directly).
    Attributes
    ----------
    npredictions : int
        Number of observations in the predicted series; this is not necessarily
        the same as the number of observations in the original model from which
        prediction was performed.
    start : int
        Zero-indexed observation number at which to start prediction,
        i.e., the first predict will be at `start`; this is relative to the
        original model from which prediction was performed.
    end : int
        Zero-indexed observation number at which to end prediction,
        i.e., the last predict will be at `end`; this is relative to the
        original model from which prediction was performed.
    nstatic : int
        Number of in-sample static predictions.
    ndynamic : int
        Number of in-sample dynamic predictions.
    nforecast : int
        Number of in-sample forecasts.
    endog : array
        The observation vector.
    design : array
        The design matrix, :math:`Z`.
    obs_intercept : array
        The intercept for the observation equation, :math:`d`.
    obs_cov : array
        The covariance matrix for the observation equation :math:`H`.
    transition : array
        The transition matrix, :math:`T`.
    state_intercept : array
        The intercept for the transition equation, :math:`c`.
    selection : array
        The selection matrix, :math:`R`.
    state_cov : array
        The covariance matrix for the state equation :math:`Q`.
    filtered_state : array
        The filtered state vector at each time period.
    filtered_state_cov : array
        The filtered state covariance matrix at each time period.
    predicted_state : array
        The predicted state vector at each time period.
    predicted_state_cov : array
        The predicted state covariance matrix at each time period.
    forecasts : array
        The one-step-ahead forecasts of observations at each time period.
    forecasts_error : array
        The forecast errors at each time period.
    forecasts_error_cov : array
        The forecast error covariance matrices at each time period.
    Notes
    -----
    The provided ranges must be conformable, meaning that it must be that
    `end - start == nstatic + ndynamic + nforecast`.
    This class is essentially a view to the FilterResults object, but
    returning the appropriate ranges for everything.
    """
    representation_attributes = [
        'endog', 'design', 'design', 'obs_intercept',
        'obs_cov', 'transition', 'state_intercept', 'selection',
        'state_cov'
    ]
    filter_attributes = [
        'filtered_state', 'filtered_state_cov',
        'predicted_state', 'predicted_state_cov',
        'forecasts', 'forecasts_error', 'forecasts_error_cov'
    ]

    def __init__(self, results, start, end, nstatic, ndynamic, nforecast):
        from scipy import stats

        # Save the filter results object
        self.results = results

        # Save prediction ranges
        self.npredictions = start - end
        self.start = start
        self.end = end
        self.nstatic = nstatic
        self.ndynamic = ndynamic
        self.nforecast = nforecast

    def __getattr__(self, attr):
        """
        Provide access to the representation and filtered output in the
        appropriate range (`start` - `end`).
        """
        # Prevent infinite recursive lookups
        if attr[0] == '_':
            raise AttributeError("'%s' object has no attribute '%s'" %
                                 (self.__class__.__name__, attr))

        _attr = '_' + attr

        # Cache the attribute
        if not hasattr(self, _attr):
            if attr == 'endog' or attr in self.filter_attributes:
                # Get a copy
                value = getattr(self.results, attr).copy()
                # Subset to the correct time frame
                value = value[..., self.start:self.end]
            elif attr in self.representation_attributes:
                value = getattr(self.results, attr).copy()
                # If a time-invariant matrix, return it. Otherwise, subset to
                # the correct period.
                if value.shape[-1] == 1:
                    value = value[..., 0]
                else:
                    value = value[..., self.start:self.end]
            else:
                raise AttributeError("'%s' object has no attribute '%s'" %
                                     (self.__class__.__name__, attr))

            setattr(self, _attr, value)

        return getattr(self, _attr)

class SwitchingMLEModel(MLEModel):
    r"""
    Markov switching state space model for maximum likelihood estimation
    Parameters
    ----------
    k_regimes : int
        The number of switching regimes.
    endog : array_like
        The observed time-series process :math:`y`
    k_states : int
        The dimension of the unobserved state process.
    param_k_regimes : int, optional
        Order of regime transition matrix in parameters vector. If not
        specified, which is usual, `k_regimes` is used instead.
        Regime transition matrix in parameters vector can be different from
        transition matrix, used internally by switching state space
        representation. For example, Markov switching :math:`AR(p)` model of
        :math:`k` regimes is internally evaluated using `k^{p + 1}` regimes.
        See `MarkovAutoregression` class for details.
    **kwargs
        This additional arguments are used in superclass intializer. See
        `MLEModel` documentation for details.
    Attributes
    ----------
    ssm : KimSmoother
        Underlying Markov switching state space representation.
    Notes
    -----
    This class wraps the Markov switching state space model with Kim filtering
    to add in functionality for maximum likelihood estimation. In particular,
    it adds the concept of updating the state space representation based on a
    defined set of parameters, through the `update` method, and it adds a `fit`
    method which uses a numerical optimizer to select the parameters that
    maximize the likelihood of the model.
    The `start_params` `update` method must be overridden in the
    child class (and the `transform_*` and `untransform_*` methods, if needed).
    This class also has a feature of using non-switching model to evaluate
    starting parameters of optimization. This feature is based on hypothesis, that
    parameters of non-switching model provide a good starting parameters for
    switching model fit. To non-switching initalization feature,
    `get_nonswitching_model` and `update_params` methods must be overridden.
    See Also
    --------
    SwitchingMLEResults
    statsmodels.tsa.statespace.regime_switching.switching_representation. \
    SwitchingRepresentation
    statsmodels.tsa.statespace.regime_switching.kim_filter.KimFilter
    statsmodels.tsa.statespace.regime_switching.kim_smoother.KimSmoother
    statsmodels.tsa.statespace.mlemodel.MLEModel
    statsmodels.tsa.statespace.regime_switching.ms_ar.MarkovAutoregression
    """

    def __init__(self, k_regimes, endog, k_states, param_k_regimes=None,
            **kwargs):

        self.k_regimes = k_regimes

        # If `param_k_regimes` is not specified, use `k_regimes`
        if param_k_regimes is None:
            self.param_k_regimes = k_regimes
        else:
            self.param_k_regimes = param_k_regimes

        # A convenient organizing of parameters
        self.parameters = MarkovSwitchingParams(self.param_k_regimes)

        # Parameters vector saves the transition matrix without last row, which
        # can be easily recovered due to left stochastic feature of the matrix
        self.parameters['regime_transition'] = [False] * \
                self.param_k_regimes * (self.param_k_regimes - 1)

        # Create param names with regime transition prob names
        self._param_names = ['Pr[{0}->{1}]'.format(j, i) for i in range(
                self.param_k_regimes - 1) for j in range(self.param_k_regimes)]

        # Superclass initialization
        super(SwitchingMLEModel, self).__init__(endog, k_states, **kwargs)

    def initialize_statespace(self, **kwargs):
        """
        Initialize the state space representation
        Parameters
        ----------
        **kwargs
            Additional keyword arguments to pass to the state space class
            constructor.
        Notes
        -----
        This method is overridden to change base class `ssm` attribute type
        from `KalmanSmoother` to `KimSmoother`.
        """

        # Match the shape, required by `KimSmoother`
        endog = self.endog.T

        # Instantiate the state space objest
        self.ssm = KimSmoother(endog.shape[0], self.k_states, self.k_regimes,
                **kwargs)
        # Bind the data to the model
        self.ssm.bind(endog)

        # Save endog vector length
        self.k_endog = self.ssm.k_endog

    def get_nonswitching_model(self):
        """
        Get a non-switching model, corresponding to this switching model.
        To override, if non-switching initialization is used.
        Returns
        -------
        subclass of MLEModel instance
        Notes
        -----
        See existing models code, notebooks and tests for example of
        `get_nonswitching_model` implementation.
        """

        raise NotImplementedError

    def update_params(self, params, nonswitching_params):
        """
        Update constrained parameters of the model, using parameters of
        non-switching model.
        To override, if non-switching initialization is used.
        Parameters
        ----------
        params : array_like
            Parameters vector to update.
        nonswitching_params : array_like
            Parameters vector of the non-switching analog, used to update
            `params`.
        Returns
        -------
        result_params : array_like
            Updated parameters.
        Notes
        -----
        Parameters vector for Markov switching model can be logically splitted
        into two parts:
        - regime transition parameters, which define regime transtition matrix.
        - model parameters, which describe common and different for every
            regime set of parameters, which are used to recover state space
            representations of every regime.
        This method is supposed to update starting parameters using
        non-switching fit, that is, regimes common parameters values are set
        to their non-switching analog, and regimes switching parameters are all
        set to one non-switching value. But don't forget to add some random or
        determinate noise to switching parameters to break the symmetry.
        See existing models code, notebooks and tests for example of
        `update_params` implementation.
        """

        return np.array(params, ndmin=1)

    def transform_regime_transition(self, unconstrained):
        """
        Transform regime transition parameters of `unconstrained` vector.
        Parameters
        ----------
        unconstrained : array_like
            Array of unconstrained parameters used by the optimizer, to be
            transformed.
        Returns
        -------
        parameters : array_like
            Input vector with constrained regime transition parameters.
        Notes
        -----
        Parameters vector for Markov switching model can be logically splitted
        into two parts:
        - regime transition parameters, which define regime transtition matrix.
        - model parameters, which describe common and different for every
            regime set of parameters, which are used to recover state space
            representations of every regime.
        This method transforms regime transition parameters using
        logistic transformation, leaving model parameters unchanged. Usually no
        need to override this method.
        """

        param_k_regimes = self.param_k_regimes

        constrained = np.array(unconstrained)

        # Unconstrained regime transition matrix without last row
        unconstrained_transition = \
                unconstrained[self.parameters['regime_transition']].reshape(
                (param_k_regimes - 1, param_k_regimes))

        # Logistic transformation
        constrained_transition = np.exp(unconstrained_transition)
        constrained_transition /= \
                (1 + constrained_transition.sum(axis=0)).reshape((1, -1))

        # Copying result to parameters vector
        constrained[self.parameters['regime_transition']] = \
                constrained_transition.ravel()

        return constrained

    def untransform_regime_transition(self, constrained):
        """
        Untransform regime transition parameters of `constrained` vector.
        Parameters
        ----------
        constrained : array_like
            Array of constrained parameters used in likelihood evalution, to be
            transformed.
        Returns
        -------
        parameters : array_like
            Input vector with unconstrained regime transition parameters.
        Notes
        -----
        Parameters vector for Markov switching model can be logically splitted
        into two parts:
        - regime transition parameters, which define regime transtition matrix.
        - model parameters, which describe common and different for every
            regime set of parameters, which are used to recover state space
            representations of every regime.
        This method untransforms regime transition parameters using
        logistic transformation, leaving model parameters unchanged. Usually no
        need to override this method.
        """

        param_k_regimes = self.param_k_regimes

        unconstrained = np.array(constrained)

        eps = 1e-8

        # Constrained regime transition matrix without last row
        constrained_transition = \
                constrained[self.parameters['regime_transition']].reshape(
                (param_k_regimes - 1, param_k_regimes))

        unconstrained_transition = np.array(constrained_transition)

        # Setting zero probabilities to a small value to avoid dealing with
        # -np.inf after switching to logarithms
        unconstrained_transition[unconstrained_transition == 0] = eps

        # Logistic transformation
        unconstrained_transition /= \
                (1 - unconstrained_transition.sum(axis=0)).reshape(1, -1)
        unconstrained_transition = np.log(unconstrained_transition)

        # Copying result to parameters vector
        unconstrained[self.parameters['regime_transition']] = \
                unconstrained_transition.ravel()

        return unconstrained

    def transform_model_params(self, unconstrained):
        """
        Transform model parameters of `unconstrained` vector.
        To override in subclasses.
        Parameters
        ----------
        unconstrained : array_like
            Array of unconstrained parameters used by the optimizer, to be
            transformed.
        Returns
        -------
        parameters : array_like
            Input vector with constrained model parameters.
        Notes
        -----
        Parameters vector for Markov switching model can be logically splitted
        into two parts:
        - regime transition parameters, which define regime transtition matrix.
        - model parameters, which describe common and different for every
            regime set of parameters, which are used to recover state space
            representations of every regime.
        This method transforms model parameters leaving regime transition
        parameters unchanged.
        """

        return np.array(unconstrained)

    def untransform_model_params(self, constrained):
        """
        Untransform model parameters of `constrained` vector.
        To override in subclasses.
        Parameters
        ----------
        constrained : array_like
            Array of constrained parameters used in likelihood evalution, to
            be transformed.
        Returns
        -------
        parameters : array_like
            Input vector with unconstrained model parameters.
        Notes
        -----
        Parameters vector for Markov switching model can be logically splitted
        into two parts:
        - regime transition parameters, which define regime transtition matrix.
        - model parameters, which describe common and different for every
            regime set of parameters, which are used to recover state space
            representations of every regime.
        This method untransforms model parameters leaving regime transition
        parameters unchanged.
        """

        return np.array(constrained)

    def transform_params(self, unconstrained):
        """
        Transform unconstrained parameters used by the optimizer to constrained
        parameters used in likelihood evaluation
        Parameters
        ----------
        unconstrained : array_like
            Array of unconstrained parameters used by the optimizer, to be
            transformed.
        Returns
        -------
        constrained : array_like
            Array of constrained parameters which may be used in likelihood
            evalation.
        Notes
        -----
        Unlike in `MLEModel` class, no need to override this method. Instead
        consider overriding `transform_model_params`.
        """

        return self.transform_model_params(
                self.transform_regime_transition(unconstrained))

    def untransform_params(self, constrained):
        """
        Transform constrained parameters used in likelihood evaluation
        to unconstrained parameters used by the optimizer
        Parameters
        ----------
        constrained : array_like
            Array of constrained parameters used in likelihood evalution, to be
            transformed.
        Returns
        -------
        unconstrained : array_like
            Array of unconstrained parameters used by the optimizer.
        Notes
        -----
        Unlike in `MLEModel` class, no need to override this method. Instead
        consider overriding `untransform_model_params`.
        """

        return self.untransform_model_params(
                self.untransform_regime_transition(constrained))

    def _permute_regimes(self, params, permutation):

        # This is a useful method, permuting regime switching parameters in
        # `params` vector.
        # It is used in `normalize_regimes`.

        param_k_regimes = self.param_k_regimes
        dtype = self.ssm.dtype

        # Get regime transition matrix, encoded in `params` vector
        regime_transition = self._get_param_regime_transition(params)

        # Permute regimes in matrix
        new_regime_transition = np.zeros((param_k_regimes, param_k_regimes),
                dtype=dtype)
        for i in range(param_k_regimes):
            for j in range(param_k_regimes):
                new_regime_transition[i, j] = \
                        regime_transition[permutation[i],
                        permutation[j]]

        # Instantiating new parameters vector
        new_params = np.zeros((self.parameters.k_params,), dtype=dtype)

        # Copying permuted regime transition matrix into new vector
        self._set_param_regime_transition(new_params, new_regime_transition)

        # Permuting model parameters
        for i in range(param_k_regimes):
            new_params[self.parameters[i]] = \
                    params[self.parameters[permutation[i]]]

        return new_params

    def get_normal_regimes_permutation(self, params):
        """
        Return normal permutation of regimes.
        To override in subclass, when required.
        Parameters
        ----------
        params : array_like
            Constrained parameters of the model.
        Returns
        -------
        permutation : array_like
            Array of size `param_k_regimes`, containing permutation of
            indices `[0, 1, ..., param_k_regimes - 1]`.
        Notes
        -----
        Several unique parameter vectors can represent the only model
        configuration, because of different order of regime enumeration. To
        compare two configurations (e.g. for testing), we need to
        normalize both parameter vectors first, that is to use permutation of
        regimes, which is determined by model configuration only.
        See `MarkovAutoregression` for `get_normal_regimes_permutation`
        example, where sorting of switching parameters is used.
        """

        param_k_regimes = self.param_k_regimes

        # Identity permutation by default
        return list(range(param_k_regimes))

    def normalize_params(self, params, transformed=True):
        """
        Normalization of parameters vector.
        Parameters
        ----------
        params : array_like
            Parameters of the model.
        transformed : bool
            Whether or not parameters are transformed.
        Returns
        -------
        result : array_like
            Normalized parameters vector.
        Notes
        -----
        Several unique parameter vectors can represent the only model
        configuration, because of different order of regime enumeration. To
        compare two configurations (e.g. for testing), we need to
        normalize both parameter vectors first, that is to use permutation of
        regimes, which is determined by model configuration only.
        This method relies on user-defined `get_normal_regimes_permutation`
        method.
        """
        if not transformed:
            params = self.transform_params(params)

        permutation = self.get_normal_regimes_permutation(params)
        params = self._permute_regimes(params, permutation)

        if not transformed:
            params = self.untransform_params(params)

        return params

    def initialize_known_regime_probs(self, *args):

        self.ssm.initialize_known_regime_probs(*args)

    def initialize_uniform_regime_probs(self):

        self.ssm.initialize_uniform_regime_probs()

    def initialize_stationary_regime_probs(self):

        self.ssm.initialize_stationary_regime_probs()

    def _get_param_regime_transition(self, constrained_params):

        # Useful method, extracting regime transition matrix from contrained
        # parameters.

        dtype = self.ssm.dtype
        param_k_regimes = self.param_k_regimes

        # Matrix initialization.
        regime_transition = np.zeros((param_k_regimes, param_k_regimes),
                dtype=dtype)

        # Fill all elements except of the last row
        regime_transition[:-1, :] = constrained_params[
                self.parameters['regime_transition']].reshape((-1,
                param_k_regimes))

        # Fill the last row
        regime_transition[-1, :] = 1 - regime_transition[:-1, :].sum(axis=0)

        return regime_transition

    def _set_param_regime_transition(self, constrained_params, regime_transition):

        # Useful method, encoding regime transition matrix into constrained
        # parameters vector.

        constrained_params[self.parameters['regime_transition']] = \
                regime_transition[:-1, :].ravel()

    @property
    def start_params(self):
        """
        (array) Starting parameters for maximum likelihood estimation. Note,
        that this is a bad initialization with identical regimes. Consider using
        user-defined starting parameters, non-switching fit or model-specific
        initialization (e.g. EM-algorithm in case of MS AR).
        """
        return self.transform_params(np.ones((self.parameters.k_params,),
            dtype=self.ssm.dtype))

    def fit(self, start_params=None, fit_nonswitching_first=False,
            default_transition_probs=None, **kwargs):
        """
        Fits the model by maximum likelihood via Kim filter.
        Parameters
        ----------
        start_params : array_like, optional
            Initial guess of the solution for the loglikelihood maximization.
            If `None`, the default is given by Model.start_params. If
            `fit_nonswitching_first=True`, then this is passed to
            non-switching model `fit` arguments.
        fit_nonswitching_first : bool, optional
            Use non-switching initialization feature. Default is `False`.
        default_transition_probs : array_like, optional
            If `fit_nonswitching_first=True`, this argument is used to specify
            regime transition matrix initial guess, because non-switching model
            only guesses parameters related to regimes representation. Equal
            probabilities by default.
        **kwargs
            Additional keyword arguments, which are passed to superclass `fit`
            method and also to non-switching model `fit` method, if it is used.
        Returns
        -------
        params : array_like
            Estimated parameters
        Notes
        -----
        Kalman-filter-specific options `optim_score='harvey'` and
        `optim_hessian='oim'` are unavailable.
        See also
        --------
        statsmodels.base.model.LikelihoodModel.fit
        statsmodels.tsa.statespace.mlemodel.fit
        """

        dtype = self.ssm.dtype

        if fit_nonswitching_first:
            # Initializing of non-switching model
            nonswitching_model = self.get_nonswitching_model()
            # Copying kwargs
            nonswitching_kwargs = dict(kwargs)
            # Need to return parameters
            nonswitching_kwargs['return_params'] = True

            start_nonswitching_params = start_params
            # Fit non-switching model
            nonswitching_params = nonswitching_model.fit(
                    start_params=start_nonswitching_params,
                    **nonswitching_kwargs)

            # Constructing starting parameters for switching model
            start_params = np.zeros((self.parameters.k_params,), dtype=dtype)

            # If default distributions are not provided, use uniform
            if default_transition_probs is None:
                default_transition_probs = \
                        np.ones((self.param_k_regimes, self.param_k_regimes),
                        dtype=self.ssm.dtype) / self.param_k_regimes

            # Encoding regime transition matrix
            self._set_param_regime_transition(start_params,
                    default_transition_probs)

            # Encoding model parameters
            start_params = self.update_params(start_params, nonswitching_params)

        kwargs['start_params'] = start_params

        #kwargs['return_params'] = True

        # Kalman-filter-specific Harvey method is not available
        if 'optim_score' in kwargs and kwargs['optim_score'] == 'harvey':
            raise NotImplementedError
        if 'optim_hessian' in kwargs and kwargs['optim_hessian'] == 'oim':
            raise NotImplementedError

        return super(SwitchingMLEModel, self).fit(**kwargs)

    def filter(self, params, transformed=True, complex_step=False,
            cov_type=None, cov_kwds=None, return_ssm=False,
            results_class=None, results_wrapper_class=None, **kwargs):
        """
        Kim filtering
        Notes
        -----
        This method is inherited from base `MLEModel` class, see arguments
        explanation, etc. in the corresponding docs.
        Also, Kalman-filter-specific Harvey method (`cov_type == 'oim'`) is
        unavailable here.
        See Also
        --------
        MLEModel.filter
        """

        if not return_ssm and results_class is None:
            # In this case base class returns `MLEResults` instance, so we need
            # to specify results class
            results_class = SwitchingMLEResults

        return super(SwitchingMLEModel, self).filter(params,
                transformed=transformed, complex_step=complex_step,
                cov_type=cov_type, cov_kwds=cov_kwds, return_ssm=return_ssm,
                results_class=results_class,
                results_wrapper_class=results_wrapper_class, **kwargs)

    def smooth(self, params, transformed=True, complex_step=False,
            cov_type=None, cov_kwds=None, return_ssm=False,
            results_class=None, results_wrapper_class=None, **kwargs):
        """
        Kim smoothing
        Notes
        -----
        This method is inherited from base `MLEModel` class, see arguments
        explanation, etc. in the corresponding docs.
        Also, Kalman-filter-specific Harvey method (`cov_type == 'oim'`) is
        unavailable here.
        See Also
        --------
        MLEModel.smooth
        """

        if not return_ssm and results_class is None:
            # In this case base class returns `MLEResults` instance, so we need
            # to specify results class
            results_class = SwitchingMLEResults

        return super(SwitchingMLEModel, self).smooth(params,
                transformed=transformed, complex_step=complex_step,
                cov_type=cov_type, cov_kwds=cov_kwds, return_ssm=return_ssm,
                results_class=results_class,
                results_wrapper_class=results_wrapper_class, **kwargs)



    def update(self, params, transformed=True, complex_step=False):
        """
        Update the parameters of the model
        Parameters
        ----------
        params : array_like
            Array of new parameters.
        transformed : boolean, optional
            Whether or not `params` is already transformed. If set to False,
            `transform_params` is called. Default is True.
        Returns
        -------
        params : array_like
            Array of parameters.
        Notes
        -----
        This method should be overridden by subclasses to perform actual
        updating steps.
        See Also
        --------
        MLEModel.update
        """

        # When `complex_step` is used, params can have a small imaginary part
        if complex_step:
            params = np.real(params)

        return super(SwitchingMLEModel, self).update(params,
                transformed=transformed, complex_step=complex_step)

    #TODO: add this functionality
    def set_smoother_output(self, **kwargs):
        raise NotImplementedError

    def initialize_approximate_diffuse(self, **kwargs):
        raise NotImplementedError(
                'Diffuse initialization is not defined for Kim filtering.')

    @property
    def initial_variance(self):
        raise NotImplementedError(
                'Diffuse initialization is not defined for Kim filtering.')

    @initial_variance.setter
    def initial_variance(self, value):
        raise NotImplementedError(
                'Diffuse initialization is not defined for Kim filtering.')

    def simulation_smoother(self, *args, **kwargs):
        # Simulation is not implemented yet
        raise NotImplementedError

    def _forecast_error_partial_derivatives(self, *args, **kwargs):
        raise NotImplementedError('Kalman filter specific functionality.')

    def observed_information_matrix(self, *args, **kwargs):
        raise NotImplementedError('Kalman filter specific functionality.')

    #def opg_information_matrix(self, *args, **kwargs):
    #    raise NotImplementedError

    #def _score_complex_step(self, *args, **kwargs):
    #    raise NotImplementedError

    #def _score_finite_difference(self, *args, **kwargs):
    #    raise NotImplementedError

    def _score_harvey(self, *args, **kwargs):
        raise NotImplementedError('Kalman filter specific functionality.')

    def _score_obs_harvey(self, *args, **kwargs):
        raise NotImplementedError('Kalman filter specific functionality.')

    def score(self, *args, **kwargs):
        """
        Compute the score function at params
        Notes
        -----
        This method is inherited from base `MLEModel` class, see arguments
        explanation, etc. in the corresponding docs.
        Also, Kalman-filter-specific Harvey method (`method == 'harvey'`) is
        unavailable here.
        See Also
        --------
        MLEModel.score
        """

        return super(SwitchingMLEModel, self).score(*args, **kwargs)

    def score_obs(self, *args, **kwargs):
        """
        Compute the score per observation, evaluated at params
        Notes
        -----
        This method is inherited from base `MLEModel` class, see arguments
        explanation, etc. in the corresponding docs.
        Also, Kalman-filter-specific Harvey method (`method == 'harvey'`) is
        unavailable here.
        See Also
        --------
        MLEModel.score_obs
        """

        return super(SwitchingMLEModel, self).score_obs(*args, **kwargs)

    def hessian(self, *args, **kwargs):
        """
        Hessian matrix of the likelihood function, evaluated at the given
        parameters
        Notes
        -----
        This method is inherited from base `MLEModel` class, see arguments
        explanation, etc. in the corresponding docs.
        Also, Kalman-filter-specific Harvey method (`method == 'oim'`) is
        unavailable here.
        See Also
        --------
        MLEModel.hessian
        """

        return super(SwitchingMLEModel, self).hessian(*args, **kwargs)

    def _hessian_oim(self, *args, **kwargs):
        raise NotImplementedError('Kalman filter specific functionality.')

    #def _hessian_opg(self, *args, **kwargs):
    #    raise NotImplementedError

    #def _hessian_finite_difference(self, *args, **kwargs):
    #    raise NotImplementedError

    #def _hessian_complex_step(self, *args, **kwargs):
    #    raise NotImplementedError

    def transform_jacobian(self, *args, **kwargs):
        """
        Jacobian matrix matrix for the parameter transformation function
        Notes
        -----
        This method is inherited from base `MLEModel` class, see arguments
        explanation, etc. in the corresponding docs.
        See Also
        --------
        MLEModel.transform_jacobian
        """

        return super(SwitchingMLEModel, self).transform_jacobian(*args,
                **kwargs)

    def simulate(self, *args, **kwargs):
        # Simulation is not implemented yet
        raise NotImplementedError

    def impulse_responses(self, *args, **kwargs):
        # Not implemented yet
        raise NotImplementedError

class SwitchingMLEResults(MLEResults):

    _filter_and_smoother_attributes = ['filtered_state', 'filtered_state_cov',
            'filtered_regime_probs', 'predicted_regime_probs',
            'initial_regime_probs', 'smoothed_regime_probs',
            'smoothed_curr_and_next_regime_probs']

    def __init__(self, model, params, results, cov_type='opg',
            cov_kwds=None, **kwargs):

        if cov_type == 'oim' or cov_type == 'robust_oim':
            raise NotImplementedError('Kalman filter specific functionality.')

        #TODO: check for correctness
        #TODO: take away attributes array
        super(SwitchingMLEResults, self).__init__(model, params, results,
                cov_type=cov_type, cov_kwds=cov_kwds, **kwargs)

    def _get_robustcov_results(self, cov_type='opg', **kwargs):

        if cov_type == 'oim' or cov_type == 'robust_oim':
            raise NotImplementedError('Kalman filter specific functionality.')

        return super(SwitchingMLEResults, self)._get_robustcov_results(
                cov_type=cov_type, **kwargs)

    #def aic(self, *args, **kwargs):
    #    raise NotImplementedError

    #def bic(self, *args, **kwargs):
    #    raise NotImplementedError

    #def _cov_params_approx(self, *args, **kwargs):
    #    raise NotImplementedError

    #def cov_params_approx(self, *args, **kwargs):
    #    raise NotImplementedError

    def _cov_params_oim(self, *args, **kwargs):
        raise NotImplementedError('Kalman filter specific functionality.')

    def cov_params_oim(self, *args, **kwargs):
        raise NotImplementedError('Kalman filter specific functionality.')

    #def _cov_params_opg(self, *args, **kwargs):
    #    raise NotImplementedError

    #def cov_params_opg(self, *args, **kwargs):
    #    raise NotImplementedError

    def cov_params_robust(self, *args, **kwargs):
        raise NotImplementedError('Kalman filter specific functionality.')

    def _cov_params_robust_oim(self, *args, **kwargs):
        raise NotImplementedError('Kalman filter specific functionality.')

    def cov_params_robust_oim(self, *args, **kwargs):
        raise NotImplementedError('Kalman filter specific functionality.')

    #def _cov_params_robust_approx(self, *args, **kwargs):
    #    raise NotImplementedError

    #def cov_params_robust_approx(self, *args, **kwargs):
    #    raise NotImplementedError

    #def fitted_values(self, *args, **kwargs):
    #    raise NotImplementedError

    #def hqic(self, *args, **kwargs):
    #    raise NotImplementedError

    #def llf_obs(self, *args, **kwargs):
    #    raise NotImplementedError

    #def llf(self, *args, **kwargs):
    #    raise NotImplementedError

    #def loglikelihood_burn(self, *args, **kwargs):
    #    raise NotImplementedError

    #def pvalues(self, *args, **kwargs):
    #    raise NotImplementedError

    #def resid(self, *args, **kwargs):
    #    raise NotImplementedError

    #def zvalues(self, *args, **kwargs):
    #    raise NotImplementedError

    #def test_normality(self, *args, **kwargs):
    #    raise NotImplementedError

    #def test_heteroscedasticity(self, *args, **kwargs):
    #    raise NotImplementedError

    #def test_serial_correlation(self, *args, **kwargs):
    #    raise NotImplementedError

    #def get_prediction(self, *args, **kwargs):
    #    raise NotImplementedError

    #def get_forecast(self, *args, **kwargs):
    #    raise NotImplementedError

    #def predict(self, *args, **kwargs):
    #    raise NotImplementedError

    #def forecast(self, *args, **kwargs):
    #    raise NotImplementedError

    def simulate(self, *args, **kwargs):
        # Simulation is not implemented yet
        raise NotImplementedError

    def impulse_responses(self, *args, **kwargs):
        # Not implemented yet
        raise NotImplementedError

    #def plot_diagnostics(self, *args, **kwargs):
    #    raise NotImplementedError

    def summary(self, title=None, **kwargs):
        """
        Summarize the Model
        Notes
        -----
        This method is inherited from base `MLEModel` class, see arguments
        explanation, etc. in the corresponding docs.
        See Also
        --------
        MLEModel.summary
        """

        # change statespace model title
        if title is None:
            title = 'Markov Switching Statespace Model Results'

        return super(SwitchingMLEResults, self).summary(title=title, **kwargs)

class SwitchingDynamicFactor(SwitchingMLEModel):
    '''
    Dynamic factor model with switching intercept term in factor changing law
    '''

    def __init__(self, k_regimes, endog, k_factors, factor_order, exog=None,
            error_order=0, error_var=False, error_cov_type='diagonal',
            enforce_stationarity=True, **kwargs):

        # Most of the logic is delegated to non-switching dynamic factor model
        self._dynamic_factor_model = DynamicFactor(endog,
                k_factors, factor_order, exog=exog, error_order=error_order,
                error_var=error_var, error_cov_type=error_cov_type,
                enforce_stationarity=enforce_stationarity, **kwargs)

        super(SwitchingDynamicFactor, self).__init__(k_regimes, endog,
                self._dynamic_factor_model.k_states, exog=exog, **kwargs)

        # A dirty hack.
        # This is required to delegate "update" method to non-switching model
        # No way to do it without rewriting DynamicFactor code
        self._dynamic_factor_model.ssm = self.ssm

        # Initializing fixed components of state space matrices, one time
        # again for new `ssm`
        self._dynamic_factor_model._initialize_loadings()
        self._dynamic_factor_model._initialize_exog()
        self._dynamic_factor_model._initialize_error_cov()
        self._dynamic_factor_model._initialize_factor_transition()
        self._dynamic_factor_model._initialize_error_transition()

        # This is required to initialize nonswitching_model
        self._init_kwargs = kwargs

        self.parameters['dynamic_factor'] = [False] * \
                self._dynamic_factor_model.k_params
        self.parameters['factor_intercept'] = [True]

    def initialize_statespace(self, **kwargs):

        endog = self.endog.T

        self.ssm = _SwitchingDynamicFactorSmoother(endog.shape[0],
                self.k_states, self.k_regimes, **kwargs)

        self.ssm.bind(endog)

        self.k_endog = self.ssm.k_endog

    def get_nonswitching_model(self):

        endog = self.endog
        k_factors = self._dynamic_factor_model.k_factors
        factor_order = self._dynamic_factor_model.factor_order
        exog = self.exog
        error_order = self._dynamic_factor_model.error_order
        error_var = self._dynamic_factor_model.error_var
        error_cov_type = self.error_cov_type
        enforce_stationarity = self._dynamic_factor_model.enforce_stationarity
        kwargs = self._init_kwargs

        return _DynamicFactorWithFactorIntercept(endog, k_factors, factor_order,
                exog=exog, error_order=error_order, error_var=error_var,
                error_cov_type=error_cov_type,
                enforce_stationarity=enforce_stationarity, **kwargs)

    def update_params(self, params, nonswitching_params, noise=0.5, seed=1):
        '''
        `nonswitching_params` is DynamicFactorWithFactorIntercept parameters
        vector. It is consists of DynamicFactor parameters concatenated with
        single factor intercept value.
        `noise` is a white noise scale, relative to factor intercept
        absolute value. It is used to break the symmetry of equal factor
        intercepts. It's defined in arguments to make redefinition in
        ancestors possible.
        '''

        dynamic_factor_params = nonswitching_params[ \
                _DynamicFactorWithFactorIntercept._dynamic_factor_params_idx]
        factor_intercept = nonswitching_params[ \
                _DynamicFactorWithFactorIntercept._factor_intercept_idx]

        params[self.parameters['dynamic_factor']] = dynamic_factor_params

        # Setting all intercepts to one value.
        params[self.parameters['factor_intercept']] = factor_intercept

        np.random.seed(seed=seed)

        # Adding noise to break the symmetry
        noise_scale = np.linalg.norm(
                params[self.parameters['factor_intercept']], np.inf) * noise
        params[self.parameters['factor_intercept']] += \
                np.random.normal(scale=noise_scale, size=self.k_regimes)

        return params

    def transform_model_params(self, unconstrained):

        constrained = np.array(unconstrained)

        dynamic_factor_unconstrained = \
                unconstrained[self.parameters['dynamic_factor']]
        dynamic_factor_constrained = \
                self._dynamic_factor_model.transform_params(
                dynamic_factor_unconstrained)

        constrained[self.parameters['dynamic_factor']] = \
                dynamic_factor_constrained

        return constrained

    def untransform_model_params(self, constrained):

        unconstrained = np.array(constrained)

        dynamic_factor_constrained = \
                constrained[self.parameters['dynamic_factor']]
        dynamic_factor_unconstrained = \
                self._dynamic_factor_model.untransform_params(
                dynamic_factor_constrained)

        unconstrained[self.parameters['dynamic_factor']] = \
                dynamic_factor_unconstrained

        return unconstrained

    def get_normal_regimes_permutation(self, params):

        k_regimes = self.k_regimes

        factor_intercepts = list(params[self.parameters['factor_intercept']])

        permutation = sorted(range(k_regimes),
                key=lambda x:factor_intercepts[x])

        return permutation

    def update(self, params, **kwargs):
        '''
        `params` vector is concatenated transition matrix params,
        DynamicFactor params and intercepts
        for every regime.
        '''

        k_regimes = self.k_regimes
        k_states = self.k_states
        dtype = self.ssm.dtype

        params = super(SwitchingDynamicFactor, self).update(params,
                **kwargs)

        self['regime_transition'] = self._get_param_regime_transition(params)

        dynamic_factor_params = params[self.parameters['dynamic_factor']]

        # `ssm` in `_dynamic_factor_model` is a KimFilter instance.
        # So this call makes sence.
        self._dynamic_factor_model.update(dynamic_factor_params, **kwargs)

        factor_intercepts = params[self.parameters['factor_intercept']]

        state_intercept = np.zeros((k_regimes, k_states, 1), dtype=dtype)
        state_intercept[:, 0, 0] = factor_intercepts

        self['state_intercept'] = state_intercept

class DynamicFactor(MLEModel):
    r"""
    Dynamic factor model
    Parameters
    ----------
    endog : array_like
        The observed time-series process :math:`y`
    exog : array_like, optional
        Array of exogenous regressors for the observation equation, shaped
        nobs x k_exog.
    k_factors : int
        The number of unobserved factors.
    factor_order : int
        The order of the vector autoregression followed by the factors.
    error_cov_type : {'scalar', 'diagonal', 'unstructured'}, optional
        The structure of the covariance matrix of the observation error term,
        where "unstructured" puts no restrictions on the matrix, "diagonal"
        requires it to be any diagonal matrix (uncorrelated errors), and
        "scalar" requires it to be a scalar times the identity matrix. Default
        is "diagonal".
    error_order : int, optional
        The order of the vector autoregression followed by the observation
        error component. Default is None, corresponding to white noise errors.
    error_var : boolean, optional
        Whether or not to model the errors jointly via a vector autoregression,
        rather than as individual autoregressions. Has no effect unless
        `error_order` is set. Default is False.
    enforce_stationarity : boolean, optional
        Whether or not to transform the AR parameters to enforce stationarity
        in the autoregressive component of the model. Default is True.
    **kwargs
        Keyword arguments may be used to provide default values for state space
        matrices or for Kalman filtering options. See `Representation`, and
        `KalmanFilter` for more details.
    Attributes
    ----------
    exog : array_like, optional
        Array of exogenous regressors for the observation equation, shaped
        nobs x k_exog.
    k_factors : int
        The number of unobserved factors.
    factor_order : int
        The order of the vector autoregression followed by the factors.
    error_cov_type : {'diagonal', 'unstructured'}
        The structure of the covariance matrix of the error term, where
        "unstructured" puts no restrictions on the matrix and "diagonal"
        requires it to be a diagonal matrix (uncorrelated errors).
    error_order : int
        The order of the vector autoregression followed by the observation
        error component.
    error_var : boolean
        Whether or not to model the errors jointly via a vector autoregression,
        rather than as individual autoregressions. Has no effect unless
        `error_order` is set.
    enforce_stationarity : boolean, optional
        Whether or not to transform the AR parameters to enforce stationarity
        in the autoregressive component of the model. Default is True.
    Notes
    -----
    The dynamic factor model considered here is in the so-called static form,
    and is specified:
    .. math::
        y_t & = \Lambda f_t + B x_t + u_t \\
        f_t & = A_1 f_{t-1} + \dots + A_p f_{t-p} + \eta_t \\
        u_t & = C_1 u_{t-1} + \dots + C_1 f_{t-q} + \varepsilon_t
    where there are `k_endog` observed series and `k_factors` unobserved
    factors. Thus :math:`y_t` is a `k_endog` x 1 vector and :math:`f_t` is a
    `k_factors` x 1 vector.
    :math:`x_t` are optional exogenous vectors, shaped `k_exog` x 1.
    :math:`\eta_t` and :math:`\varepsilon_t` are white noise error terms. In
    order to identify the factors, :math:`Var(\eta_t) = I`. Denote
    :math:`Var(\varepsilon_t) \equiv \Sigma`.
    Options related to the unobserved factors:
    - `k_factors`: this is the dimension of the vector :math:`f_t`, above.
      To exclude factors completely, set `k_factors = 0`.
    - `factor_order`: this is the number of lags to include in the factor
      evolution equation, and corresponds to :math:`p`, above. To have static
      factors, set `factor_order = 0`.
    Options related to the observation error term :math:`u_t`:
    - `error_order`: the number of lags to include in the error evolution
      equation; corresponds to :math:`q`, above. To have white noise errors,
      set `error_order = 0` (this is the default).
    - `error_cov_type`: this controls the form of the covariance matrix
      :math:`\Sigma`. If it is "dscalar", then :math:`\Sigma = \sigma^2 I`. If
      it is "diagonal", then
      :math:`\Sigma = \text{diag}(\sigma_1^2, \dots, \sigma_n^2)`. If it is
      "unstructured", then :math:`\Sigma` is any valid variance / covariance
      matrix (i.e. symmetric and positive definite).
    - `error_var`: this controls whether or not the errors evolve jointly
      according to a VAR(q), or individually according to separate AR(q)
      processes. In terms of the formulation above, if `error_var = False`,
      then the matrices :math:C_i` are diagonal, otherwise they are general
      VAR matrices.
    References
    ----------
    .. [1] Lutkepohl, Helmut. 2007.
       New Introduction to Multiple Time Series Analysis.
       Berlin: Springer.
    """

    def __init__(self, endog, k_factors, factor_order, exog=None,
                 error_order=0, error_var=False, error_cov_type='diagonal',
                 enforce_stationarity=True, **kwargs):

        # Model properties
        self.enforce_stationarity = enforce_stationarity

        # Factor-related properties
        self.k_factors = k_factors
        self.factor_order = factor_order

        # Error-related properties
        self.error_order = error_order
        self.error_var = error_var and error_order > 0
        self.error_cov_type = error_cov_type

        # Exogenous data
        self.k_exog = 0
        if exog is not None:
            exog_is_using_pandas = _is_using_pandas(exog, None)
            if not exog_is_using_pandas:
                exog = np.asarray(exog)

            # Make sure we have 2-dimensional array
            if exog.ndim == 1:
                if not exog_is_using_pandas:
                    exog = exog[:, None]
                else:
                    exog = pd.DataFrame(exog)

            self.k_exog = exog.shape[1]

        # Note: at some point in the future might add state regression, as in
        # SARIMAX.
        self.mle_regression = self.k_exog > 0

        # We need to have an array or pandas at this point
        if not _is_using_pandas(endog, None):
            endog = np.asanyarray(endog, order='C')

        # Save some useful model orders, internally used
        k_endog = endog.shape[1] if endog.ndim > 1 else 1
        self._factor_order = max(1, self.factor_order) * self.k_factors
        self._error_order = self.error_order * k_endog

        # Calculate the number of states
        k_states = self._factor_order
        k_posdef = self.k_factors
        if self.error_order > 0:
            k_states += self._error_order
            k_posdef += k_endog

        if k_states == 0:
            k_states = 1
            k_posdef = 1

        # Test for non-multivariate endog
        if k_endog < 2:
            raise ValueError('The dynamic factors model is only valid for'
                             ' multivariate time series.')

        # Test for too many factors
        if self.k_factors >= k_endog:
            raise ValueError('Number of factors must be less than the number'
                             ' of endogenous variables.')

        # Test for invalid error_cov_type
        if self.error_cov_type not in ['scalar', 'diagonal', 'unstructured']:
            raise ValueError('Invalid error covariance matrix type'
                             ' specification.')

        # By default, initialize as stationary
        kwargs.setdefault('initialization', 'stationary')

        # Initialize the state space model
        super(DynamicFactor, self).__init__(
            endog, exog=exog, k_states=k_states, k_posdef=k_posdef, **kwargs
        )

        # Initialize the components
        self.parameters = OrderedDict()
        self._initialize_loadings()
        self._initialize_exog()
        self._initialize_error_cov()
        self._initialize_factor_transition()
        self._initialize_error_transition()
        self.k_params = sum(self.parameters.values())

        # Cache parameter vector slices
        def _slice(key, offset):
            length = self.parameters[key]
            param_slice = np.s_[offset:offset + length]
            offset += length
            return param_slice, offset

        offset = 0
        self._params_loadings, offset = _slice('factor_loadings', offset)
        self._params_exog, offset = _slice('exog', offset)
        self._params_error_cov, offset = _slice('error_cov', offset)
        self._params_factor_transition, offset = (
            _slice('factor_transition', offset))
        self._params_error_transition, offset = (
            _slice('error_transition', offset))

    def _initialize_loadings(self):
        # Initialize the parameters
        self.parameters['factor_loadings'] = self.k_endog * self.k_factors

        # Setup fixed components of state space matrices
        if self.error_order > 0:
            start = self._factor_order
            end = self._factor_order + self.k_endog
            self.ssm['design', :, start:end] = np.eye(self.k_endog)

        # Setup indices of state space matrices
        self._idx_loadings = np.s_['design', :, :self.k_factors]

    def _initialize_exog(self):
        # Initialize the parameters
        self.parameters['exog'] = self.k_exog * self.k_endog

        # If we have exog effects, then the obs intercept needs to be
        # time-varying
        if self.k_exog > 0:
            self.ssm['obs_intercept'] = np.zeros((self.k_endog, self.nobs))

        # Setup indices of state space matrices
        self._idx_exog = np.s_['obs_intercept', :self.k_endog, :]

    def _initialize_error_cov(self):
        if self.error_cov_type == 'scalar':
            self._initialize_error_cov_diagonal(scalar=True)
        elif self.error_cov_type == 'diagonal':
            self._initialize_error_cov_diagonal(scalar=False)
        elif self.error_cov_type == 'unstructured':
            self._initialize_error_cov_unstructured()

    def _initialize_error_cov_diagonal(self, scalar=False):
        # Initialize the parameters
        self.parameters['error_cov'] = 1 if scalar else self.k_endog

        # Setup fixed components of state space matrices

        # Setup indices of state space matrices
        k_endog = self.k_endog
        k_factors = self.k_factors
        idx = np.diag_indices(k_endog)
        if self.error_order > 0:
            matrix = 'state_cov'
            idx = (idx[0] + k_factors, idx[1] + k_factors)
        else:
            matrix = 'obs_cov'
        self._idx_error_cov = (matrix,) + idx

    def _initialize_error_cov_unstructured(self):
        # Initialize the parameters
        k_endog = self.k_endog
        self.parameters['error_cov'] = int(k_endog * (k_endog + 1) / 2)

        # Setup fixed components of state space matrices

        # Setup indices of state space matrices
        self._idx_lower_error_cov = np.tril_indices(self.k_endog)
        if self.error_order > 0:
            start = self.k_factors
            end = self.k_factors + self.k_endog
            self._idx_error_cov = (
                np.s_['state_cov', start:end, start:end])
        else:
            self._idx_error_cov = np.s_['obs_cov', :, :]

    def _initialize_factor_transition(self):
        order = self.factor_order * self.k_factors
        k_factors = self.k_factors

        # Initialize the parameters
        self.parameters['factor_transition'] = (
                self.factor_order * self.k_factors ** 2)

        # Setup fixed components of state space matrices
        # VAR(p) for factor transition
        if self.k_factors > 0:
            if self.factor_order > 0:
                self.ssm['transition', k_factors:order, :order - k_factors] = (
                    np.eye(order - k_factors))

            self.ssm['selection', :k_factors, :k_factors] = np.eye(k_factors)
            # Identification requires constraining the state covariance to an
            # identity matrix
            self.ssm['state_cov', :k_factors, :k_factors] = np.eye(k_factors)

        # Setup indices of state space matrices
        self._idx_factor_transition = np.s_['transition', :k_factors, :order]

    def _initialize_error_transition(self):
        # Initialize the appropriate situation
        if self.error_order == 0:
            self._initialize_error_transition_white_noise()
        else:
            # Generic setup fixed components of state space matrices
            # VAR(q) for error transition
            # (in the individual AR case, we still have the VAR(q) companion
            # matrix structure, but force the coefficient matrices to be
            # diagonal)
            k_endog = self.k_endog
            k_factors = self.k_factors
            _factor_order = self._factor_order
            _error_order = self._error_order
            _slice = np.s_['selection',
                     _factor_order:_factor_order + k_endog,
                     k_factors:k_factors + k_endog]
            self.ssm[_slice] = np.eye(k_endog)
            _slice = np.s_[
                     'transition',
                     _factor_order + k_endog:_factor_order + _error_order,
                     _factor_order:_factor_order + _error_order - k_endog]
            self.ssm[_slice] = np.eye(_error_order - k_endog)

            # Now specialized setups
            if self.error_var:
                self._initialize_error_transition_var()
            else:
                self._initialize_error_transition_individual()

    def _initialize_error_transition_white_noise(self):
        # Initialize the parameters
        self.parameters['error_transition'] = 0

        # No fixed components of state space matrices

        # Setup indices of state space matrices (just an empty slice)
        self._idx_error_transition = np.s_['transition', 0:0, 0:0]

    def _initialize_error_transition_var(self):
        k_endog = self.k_endog
        _factor_order = self._factor_order
        _error_order = self._error_order

        # Initialize the parameters
        self.parameters['error_transition'] = _error_order * k_endog

        # Fixed components already setup above

        # Setup indices of state space matrices
        # Here we want to set all of the elements of the coefficient matrices,
        # the same as in a VAR specification
        self._idx_error_transition = np.s_[
                                     'transition',
                                     _factor_order:_factor_order + k_endog,
                                     _factor_order:_factor_order + _error_order]

    def _initialize_error_transition_individual(self):
        k_endog = self.k_endog
        _factor_order = self._factor_order
        _error_order = self._error_order

        # Initialize the parameters
        self.parameters['error_transition'] = _error_order

        # Fixed components already setup above

        # Setup indices of state space matrices
        # Here we want to set only the diagonal elements of the coefficient
        # matrices, and we want to set them in order by equation, not by
        # matrix (i.e. set the first element of the first matrix's diagonal,
        # then set the first element of the second matrix's diagonal, then...)

        # The basic setup is a tiled list of diagonal indices, one for each
        # coefficient matrix
        idx = np.tile(np.diag_indices(k_endog), self.error_order)
        # Now we need to shift the rows down to the correct location
        row_shift = self._factor_order
        # And we need to shift the columns in an increasing way
        col_inc = self._factor_order + np.repeat(
            [i * k_endog for i in range(self.error_order)], k_endog)
        idx[0] += row_shift
        idx[1] += col_inc

        # Make a copy (without the row shift) so that we can easily get the
        # diagonal parameters back out of a generic coefficients matrix array
        idx_diag = idx.copy()
        idx_diag[0] -= row_shift
        idx_diag[1] -= self._factor_order
        idx_diag = idx_diag[:, np.lexsort((idx_diag[1], idx_diag[0]))]
        self._idx_error_diag = (idx_diag[0], idx_diag[1])

        # Finally, we want to fill the entries in in the correct order, which
        # is to say we want to fill in lexicographically, first by row then by
        # column
        idx = idx[:, np.lexsort((idx[1], idx[0]))]
        self._idx_error_transition = np.s_['transition', idx[0], idx[1]]

    def filter(self, params, **kwargs):
        kwargs.setdefault('results_class', DynamicFactorResults)
        kwargs.setdefault('results_wrapper_class', DynamicFactorResultsWrapper)
        return super(DynamicFactor, self).filter(params, **kwargs)

    def smooth(self, params, **kwargs):
        kwargs.setdefault('results_class', DynamicFactorResults)
        kwargs.setdefault('results_wrapper_class', DynamicFactorResultsWrapper)
        return super(DynamicFactor, self).smooth(params, **kwargs)

    @property
    def start_params(self):
        params = np.zeros(self.k_params, dtype=np.float64)

        endog = self.endog.copy()

        # 1. Factor loadings (estimated via PCA)
        if self.k_factors > 0:
            # Use principal components + OLS as starting values
            res_pca = PCA(endog, ncomp=self.k_factors)
            mod_ols = OLS(endog, res_pca.factors)
            res_ols = mod_ols.fit()

            # Using OLS params for the loadings tends to gives higher starting
            # log-likelihood.
            params[self._params_loadings] = res_ols.params.T.ravel()
            # params[self._params_loadings] = res_pca.loadings.ravel()

            # However, using res_ols.resid tends to causes non-invertible
            # starting VAR coefficients for error VARs
            # endog = res_ols.resid
            endog = endog - np.dot(res_pca.factors, res_pca.loadings.T)

        # 2. Exog (OLS on residuals)
        if self.k_exog > 0:
            mod_ols = OLS(endog, exog=self.exog)
            res_ols = mod_ols.fit()
            # In the form: beta.x1.y1, beta.x2.y1, beta.x1.y2, ...
            params[self._params_exog] = res_ols.params.T.ravel()
            endog = res_ols.resid

        # 3. Factors (VAR on res_pca.factors)
        stationary = True
        if self.k_factors > 1 and self.factor_order > 0:
            # 3a. VAR transition (OLS on factors estimated via PCA)
            mod_factors = VAR(res_pca.factors)
            res_factors = mod_factors.fit(maxlags=self.factor_order, ic=None,
                                          trend='nc')
            # Save the parameters
            params[self._params_factor_transition] = (
                res_factors.params.T.ravel())

            # Test for stationarity
            coefficient_matrices = (
                params[self._params_factor_transition].reshape(
                    self.k_factors * self.factor_order, self.k_factors
                ).T
            ).reshape(self.k_factors, self.k_factors, self.factor_order).T

            stationary = is_invertible([1] + list(-coefficient_matrices))
        elif self.k_factors > 0 and self.factor_order > 0:
            # 3b. AR transition
            Y = res_pca.factors[self.factor_order:]
            X = lagmat(res_pca.factors, self.factor_order, trim='both')
            params_ar = np.linalg.pinv(X).dot(Y)
            stationary = is_invertible(np.r_[1, -params_ar.squeeze()])
            params[self._params_factor_transition] = params_ar[:, 0]

        # Check for stationarity
        if not stationary and self.enforce_stationarity:
            raise ValueError('Non-stationary starting autoregressive'
                             ' parameters found with `enforce_stationarity`'
                             ' set to True.')

        # 4. Errors
        if self.error_order == 0:
            error_params = []
            if self.error_cov_type == 'scalar':
                params[self._params_error_cov] = endog.var(axis=0).mean()
            elif self.error_cov_type == 'diagonal':
                params[self._params_error_cov] = endog.var(axis=0)
            elif self.error_cov_type == 'unstructured':
                cov_factor = np.diag(endog.std(axis=0))
                params[self._params_error_cov] = (
                    cov_factor[self._idx_lower_error_cov].ravel())
        else:
            mod_errors = VAR(endog)
            res_errors = mod_errors.fit(maxlags=self.error_order, ic=None,
                                        trend='nc')

            # Test for stationarity
            coefficient_matrices = (
                np.array(res_errors.params.T).ravel().reshape(
                    self.k_endog * self.error_order, self.k_endog
                ).T
            ).reshape(self.k_endog, self.k_endog, self.error_order).T

            stationary = is_invertible([1] + list(-coefficient_matrices))
            if not stationary and self.enforce_stationarity:
                raise ValueError('Non-stationary starting error autoregressive'
                                 ' parameters found with'
                                 ' `enforce_stationarity` set to True.')

            # Get the error autoregressive parameters
            if self.error_var:
                params[self._params_error_transition] = (
                    np.array(res_errors.params.T).ravel())
            else:
                # In the case of individual autoregressions, extract just the
                # diagonal elements
                params[self._params_error_transition] = (
                    res_errors.params.T[self._idx_error_diag])

            # Get the error covariance parameters
            if self.error_cov_type == 'scalar':
                params[self._params_error_cov] = (
                    res_errors.sigma_u.diagonal().mean())
            elif self.error_cov_type == 'diagonal':
                params[self._params_error_cov] = res_errors.sigma_u.diagonal()
            elif self.error_cov_type == 'unstructured':
                try:
                    cov_factor = np.linalg.cholesky(res_errors.sigma_u)
                except np.linalg.LinAlgError:
                    cov_factor = np.eye(res_errors.sigma_u.shape[0]) * (
                            res_errors.sigma_u.diagonal().mean() ** 0.5)
                cov_factor = np.eye(res_errors.sigma_u.shape[0]) * (
                        res_errors.sigma_u.diagonal().mean() ** 0.5)
                params[self._params_error_cov] = (
                    cov_factor[self._idx_lower_error_cov].ravel())

        return params

    @property
    def param_names(self):
        param_names = []
        endog_names = self.endog_names

        # 1. Factor loadings
        param_names += [
            'loading.f%d.%s' % (j + 1, endog_names[i])
            for i in range(self.k_endog)
            for j in range(self.k_factors)
        ]

        # 2. Exog
        # Recall these are in the form: beta.x1.y1, beta.x2.y1, beta.x1.y2, ...
        param_names += [
            'beta.%s.%s' % (self.exog_names[j], endog_names[i])
            for i in range(self.k_endog)
            for j in range(self.k_exog)
        ]

        # 3. Error covariances
        if self.error_cov_type == 'scalar':
            param_names += ['sigma2']
        elif self.error_cov_type == 'diagonal':
            param_names += [
                'sigma2.%s' % endog_names[i]
                for i in range(self.k_endog)
            ]
        elif self.error_cov_type == 'unstructured':
            param_names += [
                ('sqrt.var.%s' % endog_names[i] if i == j else
                 'sqrt.cov.%s.%s' % (endog_names[j], endog_names[i]))
                for i in range(self.k_endog)
                for j in range(i + 1)
            ]

        # 4. Factor transition VAR
        param_names += [
            'L%d.f%d.f%d' % (i + 1, k + 1, j + 1)
            for j in range(self.k_factors)
            for i in range(self.factor_order)
            for k in range(self.k_factors)
        ]

        # 5. Error transition VAR
        if self.error_var:
            param_names += [
                'L%d.e(%s).e(%s)' % (i + 1, endog_names[k], endog_names[j])
                for j in range(self.k_endog)
                for i in range(self.error_order)
                for k in range(self.k_endog)
            ]
        else:
            param_names += [
                'L%d.e(%s).e(%s)' % (i + 1, endog_names[j], endog_names[j])
                for j in range(self.k_endog)
                for i in range(self.error_order)
            ]

        return param_names

    def transform_params(self, unconstrained):
        """
        Transform unconstrained parameters used by the optimizer to constrained
        parameters used in likelihood evaluation
        Parameters
        ----------
        unconstrained : array_like
            Array of unconstrained parameters used by the optimizer, to be
            transformed.
        Returns
        -------
        constrained : array_like
            Array of constrained parameters which may be used in likelihood
            evalation.
        Notes
        -----
        Constrains the factor transition to be stationary and variances to be
        positive.
        """
        unconstrained = np.array(unconstrained, ndmin=1)
        dtype = unconstrained.dtype
        constrained = np.zeros(unconstrained.shape, dtype=dtype)

        # 1. Factor loadings
        # The factor loadings do not need to be adjusted
        constrained[self._params_loadings] = (
            unconstrained[self._params_loadings])

        # 2. Exog
        # The regression coefficients do not need to be adjusted
        constrained[self._params_exog] = (
            unconstrained[self._params_exog])

        # 3. Error covariances
        # If we have variances, force them to be positive
        if self.error_cov_type in ['scalar', 'diagonal']:
            constrained[self._params_error_cov] = (
                    unconstrained[self._params_error_cov] ** 2)
        # Otherwise, nothing needs to be done
        elif self.error_cov_type == 'unstructured':
            constrained[self._params_error_cov] = (
                unconstrained[self._params_error_cov])

        # 4. Factor transition VAR
        # VAR transition: optionally force to be stationary
        if self.enforce_stationarity and self.factor_order > 0:
            # Transform the parameters
            unconstrained_matrices = (
                unconstrained[self._params_factor_transition].reshape(
                    self.k_factors, self._factor_order))
            # This is always an identity matrix, but because the transform
            # done prior to update (where the ssm representation matrices
            # change), it may be complex
            # cov = self.ssm[
            #    'state_cov', :self.k_factors, :self.k_factors].real

            cov = np.identity(self.k_factors, dtype=self.ssm.dtype)

            coefficient_matrices, variance = (
                constrain_stationary_multivariate(unconstrained_matrices, cov))
            constrained[self._params_factor_transition] = (
                coefficient_matrices.ravel())
        else:
            constrained[self._params_factor_transition] = (
                unconstrained[self._params_factor_transition])

        # 5. Error transition VAR
        # VAR transition: optionally force to be stationary
        if self.enforce_stationarity and self.error_order > 0:

            # Joint VAR specification
            if self.error_var:
                unconstrained_matrices = (
                    unconstrained[self._params_error_transition].reshape(
                        self.k_endog, self._error_order))
                start = self.k_factors
                end = self.k_factors + self.k_endog
                cov = self.ssm['state_cov', start:end, start:end].real
                coefficient_matrices, variance = (
                    constrain_stationary_multivariate(
                        unconstrained_matrices, cov))
                constrained[self._params_error_transition] = (
                    coefficient_matrices.ravel())
            # Separate AR specifications
            else:
                coefficients = (
                    unconstrained[self._params_error_transition].copy())
                for i in range(self.k_endog):
                    start = i * self.error_order
                    end = (i + 1) * self.error_order
                    coefficients[start:end] = constrain_stationary_univariate(
                        coefficients[start:end])
                constrained[self._params_error_transition] = coefficients

        else:
            constrained[self._params_error_transition] = (
                unconstrained[self._params_error_transition])

        return constrained

    def untransform_params(self, constrained):
        """
        Transform constrained parameters used in likelihood evaluation
        to unconstrained parameters used by the optimizer.
        Parameters
        ----------
        constrained : array_like
            Array of constrained parameters used in likelihood evalution, to be
            transformed.
        Returns
        -------
        unconstrained : array_like
            Array of unconstrained parameters used by the optimizer.
        """
        constrained = np.array(constrained, ndmin=1)
        dtype = constrained.dtype
        unconstrained = np.zeros(constrained.shape, dtype=dtype)

        # 1. Factor loadings
        # The factor loadings do not need to be adjusted
        unconstrained[self._params_loadings] = (
            constrained[self._params_loadings])

        # 2. Exog
        # The regression coefficients do not need to be adjusted
        unconstrained[self._params_exog] = (
            constrained[self._params_exog])

        # 3. Error covariances
        # If we have variances, force them to be positive
        if self.error_cov_type in ['scalar', 'diagonal']:
            unconstrained[self._params_error_cov] = (
                    constrained[self._params_error_cov] ** 0.5)
        # Otherwise, nothing needs to be done
        elif self.error_cov_type == 'unstructured':
            unconstrained[self._params_error_cov] = (
                constrained[self._params_error_cov])

        # 3. Factor transition VAR
        # VAR transition: optionally force to be stationary
        if self.enforce_stationarity and self.factor_order > 0:
            # Transform the parameters
            constrained_matrices = (
                constrained[self._params_factor_transition].reshape(
                    self.k_factors, self._factor_order))
            # cov = self.ssm[
            #    'state_cov', :self.k_factors, :self.k_factors].real

            cov = np.identity(self.k_factors, dtype=self.ssm.dtype)

            coefficient_matrices, variance = (
                unconstrain_stationary_multivariate(
                    constrained_matrices, cov))
            unconstrained[self._params_factor_transition] = (
                coefficient_matrices.ravel())
        else:
            unconstrained[self._params_factor_transition] = (
                constrained[self._params_factor_transition])

        # 5. Error transition VAR
        # VAR transition: optionally force to be stationary
        if self.enforce_stationarity and self.error_order > 0:

            # Joint VAR specification
            if self.error_var:
                constrained_matrices = (
                    constrained[self._params_error_transition].reshape(
                        self.k_endog, self._error_order))
                start = self.k_factors
                end = self.k_factors + self.k_endog
                cov = self.ssm['state_cov', start:end, start:end].real
                coefficient_matrices, variance = (
                    unconstrain_stationary_multivariate(
                        constrained_matrices, cov))
                unconstrained[self._params_error_transition] = (
                    coefficient_matrices.ravel())
            # Separate AR specifications
            else:
                coefficients = (
                    constrained[self._params_error_transition].copy())
                for i in range(self.k_endog):
                    start = i * self.error_order
                    end = (i + 1) * self.error_order
                    coefficients[start:end] = (
                        unconstrain_stationary_univariate(
                            coefficients[start:end]))
                unconstrained[self._params_error_transition] = coefficients

        else:
            unconstrained[self._params_error_transition] = (
                constrained[self._params_error_transition])

        return unconstrained

    def update(self, params, transformed=True, complex_step=False):
        """
        Update the parameters of the model
        Updates the representation matrices to fill in the new parameter
        values.
        Parameters
        ----------
        params : array_like
            Array of new parameters.
        transformed : boolean, optional
            Whether or not `params` is already transformed. If set to False,
            `transform_params` is called. Default is True..
        Returns
        -------
        params : array_like
            Array of parameters.
        Notes
        -----
        Let `n = k_endog`, `m = k_factors`, and `p = factor_order`. Then the
        `params` vector has length
        :math:`[n \times m] + [n] + [m^2 \times p]`.
        It is expanded in the following way:
        - The first :math:`n \times m` parameters fill out the factor loading
          matrix, starting from the [0,0] entry and then proceeding along rows.
          These parameters are not modified in `transform_params`.
        - The next :math:`n` parameters provide variances for the error_cov
          errors in the observation equation. They fill in the diagonal of the
          observation covariance matrix, and are constrained to be positive by
          `transofrm_params`.
        - The next :math:`m^2 \times p` parameters are used to create the `p`
          coefficient matrices for the vector autoregression describing the
          factor transition. They are transformed in `transform_params` to
          enforce stationarity of the VAR(p). They are placed so as to make
          the transition matrix a companion matrix for the VAR. In particular,
          we assume that the first :math:`m^2` parameters fill the first
          coefficient matrix (starting at [0,0] and filling along rows), the
          second :math:`m^2` parameters fill the second matrix, etc.
        """
        params = super(DynamicFactor, self).update(
            params, transformed=transformed, complex_step=complex_step)

        # 1. Factor loadings
        # Update the design / factor loading matrix
        self.ssm[self._idx_loadings] = (
            params[self._params_loadings].reshape(self.k_endog, self.k_factors)
        )

        # 2. Exog
        if self.k_exog > 0:
            exog_params = params[self._params_exog].reshape(
                self.k_endog, self.k_exog).T
            self.ssm[self._idx_exog] = np.dot(self.exog, exog_params).T

        # 3. Error covariances
        if self.error_cov_type in ['scalar', 'diagonal']:
            self.ssm[self._idx_error_cov] = (
                params[self._params_error_cov])
        elif self.error_cov_type == 'unstructured':
            error_cov_lower = np.zeros((self.k_endog, self.k_endog),
                                       dtype=params.dtype)
            error_cov_lower[self._idx_lower_error_cov] = (
                params[self._params_error_cov])
            self.ssm[self._idx_error_cov] = (
                np.dot(error_cov_lower, error_cov_lower.T))

        # 4. Factor transition VAR
        self.ssm[self._idx_factor_transition] = (
            params[self._params_factor_transition].reshape(
                self.k_factors, self.factor_order * self.k_factors))

        # 5. Error transition VAR
        if self.error_var:
            self.ssm[self._idx_error_transition] = (
                params[self._params_error_transition].reshape(
                    self.k_endog, self._error_order))
        else:
            self.ssm[self._idx_error_transition] = (
                params[self._params_error_transition])

class DynamicFactorResults(MLEResults):
    """
    Class to hold results from fitting an DynamicFactor model.
    Parameters
    ----------
    model : DynamicFactor instance
        The fitted model instance
    Attributes
    ----------
    specification : dictionary
        Dictionary including all attributes from the DynamicFactor model
        instance.
    coefficient_matrices_var : array
        Array containing autoregressive lag polynomial coefficient matrices,
        ordered from lowest degree to highest.
    See Also
    --------
    statsmodels.tsa.statespace.kalman_filter.FilterResults
    statsmodels.tsa.statespace.mlemodel.MLEResults
    """

    def __init__(self, model, params, filter_results, cov_type='opg',
                 **kwargs):
        super(DynamicFactorResults, self).__init__(model, params,
                                                   filter_results, cov_type,
                                                   **kwargs)

        self.df_resid = np.inf  # attribute required for wald tests

        self.specification = Bunch(**{
            # Model properties
            'k_endog': self.model.k_endog,
            'enforce_stationarity': self.model.enforce_stationarity,

            # Factor-related properties
            'k_factors': self.model.k_factors,
            'factor_order': self.model.factor_order,

            # Error-related properties
            'error_order': self.model.error_order,
            'error_var': self.model.error_var,
            'error_cov_type': self.model.error_cov_type,

            # Other properties
            'k_exog': self.model.k_exog
        })

        # Polynomials / coefficient matrices
        self.coefficient_matrices_var = None
        if self.model.factor_order > 0:
            ar_params = (
                np.array(self.params[self.model._params_factor_transition]))
            k_factors = self.model.k_factors
            factor_order = self.model.factor_order
            self.coefficient_matrices_var = (
                ar_params.reshape(k_factors * factor_order, k_factors).T
            ).reshape(k_factors, k_factors, factor_order).T

        self.coefficient_matrices_error = None
        if self.model.error_order > 0:
            ar_params = (
                np.array(self.params[self.model._params_error_transition]))
            k_endog = self.model.k_endog
            error_order = self.model.error_order
            if self.model.error_var:
                self.coefficient_matrices_error = (
                    ar_params.reshape(k_endog * error_order, k_endog).T
                ).reshape(k_endog, k_endog, error_order).T
            else:
                mat = np.zeros((k_endog, k_endog * error_order))
                mat[self.model._idx_error_diag] = ar_params
                self.coefficient_matrices_error = (
                    mat.T.reshape(error_order, k_endog, k_endog))

    @property
    def factors(self):
        """
        Estimates of unobserved factors
        Returns
        -------
        out: Bunch
            Has the following attributes:

            - `filtered`: a time series array with the filtered estimate of
                          the component
            - `filtered_cov`: a time series array with the filtered estimate of
                          the variance/covariance of the component
            - `smoothed`: a time series array with the smoothed estimate of
                          the component
            - `smoothed_cov`: a time series array with the smoothed estimate of
                          the variance/covariance of the component
            - `offset`: an integer giving the offset in the state vector where
                        this component begins
        """
        # If present, level is always the first component of the state vector
        out = None
        spec = self.specification
        if spec.k_factors > 0:
            offset = 0
            end = spec.k_factors
            res = self.filter_results
            out = Bunch(
                filtered=res.filtered_state[offset:end],
                filtered_cov=res.filtered_state_cov[offset:end, offset:end],
                smoothed=None, smoothed_cov=None,
                offset=offset)
            if self.smoothed_state is not None:
                out.smoothed = self.smoothed_state[offset:end]
            if self.smoothed_state_cov is not None:
                out.smoothed_cov = (
                    self.smoothed_state_cov[offset:end, offset:end])
        return out

    @cache_readonly
    def coefficients_of_determination(self):
        """
        Coefficients of determination (:math:`R^2`) from regressions of
        individual estimated factors on endogenous variables.
        Returns
        -------
        coefficients_of_determination : array
            A `k_endog` x `k_factors` array, where
            `coefficients_of_determination[i, j]` represents the :math:`R^2`
            value from a regression of factor `j` and a constant on endogenous
            variable `i`.
        Notes
        -----
        Although it can be difficult to interpret the estimated factor loadings
        and factors, it is often helpful to use the cofficients of
        determination from univariate regressions to assess the importance of
        each factor in explaining the variation in each endogenous variable.
        In models with many variables and factors, this can sometimes lend
        interpretation to the factors (for example sometimes one factor will
        load primarily on real variables and another on nominal variables).
        See Also
        --------
        plot_coefficients_of_determination
        """
        from statsmodels.tools import add_constant
        spec = self.specification
        coefficients = np.zeros((spec.k_endog, spec.k_factors))
        which = 'filtered' if self.smoothed_state is None else 'smoothed'

        for i in range(spec.k_factors):
            exog = add_constant(self.factors[which][i])
            for j in range(spec.k_endog):
                endog = self.filter_results.endog[j]
                coefficients[j, i] = OLS(endog, exog).fit().rsquared

        return coefficients

    def plot_coefficients_of_determination(self, endog_labels=None,
                                           fig=None, figsize=None):
        """
        Plot the coefficients of determination
        Parameters
        ----------
        endog_labels : boolean, optional
            Whether or not to label the endogenous variables along the x-axis
            of the plots. Default is to include labels if there are 5 or fewer
            endogenous variables.
        fig : Matplotlib Figure instance, optional
            If given, subplots are created in this figure instead of in a new
            figure. Note that the grid will be created in the provided
            figure using `fig.add_subplot()`.
        figsize : tuple, optional
            If a figure is created, this argument allows specifying a size.
            The tuple is (width, height).
        Notes
        -----
        Produces a `k_factors` x 1 plot grid. The `i`th plot shows a bar plot
        of the coefficients of determination associated with factor `i`. The
        endogenous variables are arranged along the x-axis according to their
        position in the `endog` array.
        See Also
        --------
        coefficients_of_determination
        """
        from statsmodels.graphics.utils import _import_mpl, create_mpl_fig
        _import_mpl()
        fig = create_mpl_fig(fig, figsize)

        spec = self.specification

        # Should we label endogenous variables?
        if endog_labels is None:
            endog_labels = spec.k_endog <= 5

        # Plot the coefficients of determination
        coefficients_of_determination = self.coefficients_of_determination
        plot_idx = 1
        locations = np.arange(spec.k_endog)
        for coeffs in coefficients_of_determination.T:
            # Create the new axis
            ax = fig.add_subplot(spec.k_factors, 1, plot_idx)
            ax.set_ylim((0, 1))
            ax.set(title='Factor %i' % plot_idx, ylabel=r'$R^2$')
            bars = ax.bar(locations, coeffs)

            if endog_labels:
                width = bars[0].get_width()
                ax.xaxis.set_ticks(locations + width / 2)
                ax.xaxis.set_ticklabels(self.model.endog_names)
            else:
                ax.set(xlabel='Endogenous variables')
                ax.xaxis.set_ticks([])

            plot_idx += 1

        return fig

    def predict(self, start=None, end=None, exog=None, dynamic=False,
                **kwargs):
        """
        In-sample prediction and out-of-sample forecasting
        Parameters
        ----------
        start : int, str, or datetime, optional
            Zero-indexed observation number at which to start forecasting, ie.,
            the first forecast is start. Can also be a date string to
            parse or a datetime type. Default is the the zeroth observation.
        end : int, str, or datetime, optional
            Zero-indexed observation number at which to end forecasting, ie.,
            the first forecast is start. Can also be a date string to
            parse or a datetime type. However, if the dates index does not
            have a fixed frequency, end must be an integer index if you
            want out of sample prediction. Default is the last observation in
            the sample.
        exog : array_like, optional
            If the model includes exogenous regressors, you must provide
            exactly enough out-of-sample values for the exogenous variables if
            end is beyond the last observation in the sample.
        dynamic : boolean, int, str, or datetime, optional
            Integer offset relative to `start` at which to begin dynamic
            prediction. Can also be an absolute date string to parse or a
            datetime type (these are not interpreted as offsets).
            Prior to this observation, true endogenous values will be used for
            prediction; starting with this observation and continuing through
            the end of prediction, forecasted endogenous values will be used
            instead.
        **kwargs
            Additional arguments may required for forecasting beyond the end
            of the sample. See `FilterResults.predict` for more details.
        Returns
        -------
        forecast : array
            Array of out of sample forecasts.
        """
        if start is None:
            start = 0

        # Handle end (e.g. date)
        _start = self.model._get_predict_start(start)
        _end, _out_of_sample = self.model._get_predict_end(end)

        # Handle exogenous parameters
        if _out_of_sample and self.model.k_exog > 0:
            # Create a new faux VARMAX model for the extended dataset
            nobs = self.model.data.orig_endog.shape[0] + _out_of_sample
            endog = np.zeros((nobs, self.model.k_endog))

            if self.model.k_exog > 0:
                if exog is None:
                    raise ValueError('Out-of-sample forecasting in a model'
                                     ' with a regression component requires'
                                     ' additional exogenous values via the'
                                     ' `exog` argument.')
                exog = np.array(exog)
                required_exog_shape = (_out_of_sample, self.model.k_exog)
                if not exog.shape == required_exog_shape:
                    raise ValueError('Provided exogenous values are not of the'
                                     ' appropriate shape. Required %s, got %s.'
                                     % (str(required_exog_shape),
                                        str(exog.shape)))
                exog = np.c_[self.model.data.orig_exog.T, exog.T].T

            # TODO replace with init_kwds or specification or similar
            model = DynamicFactor(
                endog,
                k_factors=self.model.k_factors,
                factor_order=self.model.factor_order,
                exog=exog,
                error_order=self.model.error_order,
                error_var=self.model.error_var,
                error_cov_type=self.model.error_cov_type,
                enforce_stationarity=self.model.enforce_stationarity
            )
            model.update(self.params)

            # Set the kwargs with the update time-varying state space
            # representation matrices
            for name in self.filter_results.shapes.keys():
                if name == 'obs':
                    continue
                mat = getattr(model.ssm, name)
                if mat.shape[-1] > 1:
                    if len(mat.shape) == 2:
                        kwargs[name] = mat[:, -_out_of_sample:]
                    else:
                        kwargs[name] = mat[:, :, -_out_of_sample:]
        elif self.model.k_exog == 0 and exog is not None:
            warn('Exogenous array provided to predict, but additional data not'
                 ' required. `exog` argument ignored.')

        return super(DynamicFactorResults, self).predict(
            start=start, end=end, exog=exog, dynamic=dynamic, **kwargs
        )

    def forecast(self, steps=1, exog=None, **kwargs):
        """
        Out-of-sample forecasts
        Parameters
        ----------
        steps : int, optional
            The number of out of sample forecasts from the end of the
            sample. Default is 1.
        exog : array_like, optional
            If the model includes exogenous regressors, you must provide
            exactly enough out-of-sample values for the exogenous variables for
            each step forecasted.
        **kwargs
            Additional arguments may required for forecasting beyond the end
            of the sample. See `FilterResults.predict` for more details.
        Returns
        -------
        forecast : array
            Array of out of sample forecasts.
        """
        return super(DynamicFactorResults, self).forecast(steps, exog=exog,
                                                          **kwargs)

    def summary(self, alpha=.05, start=None, separate_params=True):
        from statsmodels.iolib.summary import summary_params
        spec = self.specification

        # Create the model name
        model_name = []
        if spec.k_factors > 0:
            if spec.factor_order > 0:
                model_type = ('DynamicFactor(factors=%d, order=%d)' %
                              (spec.k_factors, spec.factor_order))
            else:
                model_type = 'StaticFactor(factors=%d)' % spec.k_factors

            model_name.append(model_type)
            if spec.k_exog > 0:
                model_name.append('%d regressors' % spec.k_exog)
        else:
            model_name.append('SUR(%d regressors)' % spec.k_exog)

        if spec.error_order > 0:
            error_type = 'VAR' if spec.error_var else 'AR'
            model_name.append('%s(%d) errors' % (error_type, spec.error_order))

        summary = super(DynamicFactorResults, self).summary(
            alpha=alpha, start=start, model_name=model_name,
            display_params=not separate_params
        )

        if separate_params:
            indices = np.arange(len(self.params))

            def make_table(self, mask, title, strip_end=True):
                res = (self, self.params[mask], self.bse[mask],
                       self.zvalues[mask], self.pvalues[mask],
                       self.conf_int(alpha)[mask])

                param_names = [
                    '.'.join(name.split('.')[:-1]) if strip_end else name
                    for name in
                    np.array(self.data.param_names)[mask].tolist()
                ]

                return summary_params(res, yname=None, xname=param_names,
                                      alpha=alpha, use_t=False, title=title)

            k_endog = self.model.k_endog
            k_exog = self.model.k_exog
            k_factors = self.model.k_factors
            factor_order = self.model.factor_order
            _factor_order = self.model._factor_order
            _error_order = self.model._error_order

            # Add parameter tables for each endogenous variable
            loading_indices = indices[self.model._params_loadings]
            loading_masks = []
            exog_indices = indices[self.model._params_exog]
            exog_masks = []
            for i in range(k_endog):
                offset = 0

                # 1. Factor loadings
                # Recall these are in the form:
                # 'loading.f1.y1', 'loading.f2.y1', 'loading.f1.y2', ...

                loading_mask = (
                    loading_indices[i * k_factors:(i + 1) * k_factors])
                loading_masks.append(loading_mask)

                # 2. Exog
                # Recall these are in the form:
                # beta.x1.y1, beta.x2.y1, beta.x1.y2, ...
                exog_mask = exog_indices[i * k_exog:(i + 1) * k_exog]
                exog_masks.append(exog_mask)

                # Create the table
                mask = np.concatenate([loading_mask, exog_mask])
                title = "Results for equation %s" % self.model.endog_names[i]
                table = make_table(self, mask, title)
                summary.tables.append(table)

            # Add parameter tables for each factor
            factor_indices = indices[self.model._params_factor_transition]
            factor_masks = []
            if factor_order > 0:
                for i in range(k_factors):
                    start = i * _factor_order
                    factor_mask = factor_indices[start: start + _factor_order]
                    factor_masks.append(factor_mask)

                    # Create the table
                    title = "Results for factor equation f%d" % (i + 1)
                    table = make_table(self, factor_mask, title)
                    summary.tables.append(table)

            # Add parameter tables for error transitions
            error_masks = []
            if spec.error_order > 0:
                error_indices = indices[self.model._params_error_transition]
                for i in range(k_endog):
                    if spec.error_var:
                        start = i * _error_order
                        end = (i + 1) * _error_order
                    else:
                        start = i * spec.error_order
                        end = (i + 1) * spec.error_order

                    error_mask = error_indices[start:end]
                    error_masks.append(error_mask)

                    # Create the table
                    title = ("Results for error equation e(%s)" %
                             self.model.endog_names[i])
                    table = make_table(self, error_mask, title)
                    summary.tables.append(table)

            # Error covariance terms
            error_cov_mask = indices[self.model._params_error_cov]
            table = make_table(self, error_cov_mask,
                               "Error covariance matrix", strip_end=False)
            summary.tables.append(table)

            # Add a table for all other parameters
            masks = []
            for m in (loading_masks, exog_masks, factor_masks,
                      error_masks, [error_cov_mask]):
                m = np.array(m).flatten()
                if len(m) > 0:
                    masks.append(m)
            masks = np.concatenate(masks)
            inverse_mask = np.array(list(set(indices).difference(set(masks))))
            if len(inverse_mask) > 0:
                table = make_table(self, inverse_mask, "Other parameters",
                                   strip_end=False)
                summary.tables.append(table)

        return summary

    summary.__doc__ = MLEResults.summary.__doc__

class DynamicFactorResultsWrapper(MLEResultsWrapper):
    _attrs = {}
    _wrap_attrs = wrap.union_dicts(MLEResultsWrapper._wrap_attrs,
                                   _attrs)
    _methods = {}
    _wrap_methods = wrap.union_dicts(MLEResultsWrapper._wrap_methods,
                                     _methods)

wrap.populate_wrapper(DynamicFactorResultsWrapper, DynamicFactorResults)

class _DynamicFactorWithFactorIntercept(DynamicFactor):
    '''
    Extended Dynamic factor model with factor intercept term
    '''

    # Params vector for this model is params vector for parent class
    # concatenated with single value of factor intercept.
    _dynamic_factor_params_idx = np.s_[:-1]
    _factor_intercept_idx = np.s_[-1]

    def __init__(self, *args, **kwargs):

        super(_DynamicFactorWithFactorIntercept, self).__init__(*args, **kwargs)

        # For the sake of clarity
        self.dynamic_factor_k_params = self.k_params

        # One more value for factor intercept
        self.k_params_with_factor_intercept = self.k_params + 1

    @property
    def start_params(self):

        dynamic_factor_params = super(_DynamicFactorWithFactorIntercept,
                self).start_params

        start_params = np.zeros((self.k_params_with_factor_intercept,),
                dtype=self.ssm.dtype)

        start_params[self._dynamic_factor_params_idx] = dynamic_factor_params

        return start_params

    def transform_params(self, unconstrained):

        constrained = np.array(unconstrained)

        dynamic_factor_unconstrained = \
                unconstrained[self._dynamic_factor_params_idx]
        dynamic_factor_constrained = super(_DynamicFactorWithFactorIntercept,
                self).transform_params(dynamic_factor_unconstrained)

        constrained[self._dynamic_factor_params_idx] = \
                dynamic_factor_constrained

        return constrained

    def untransform_params(self, constrained):

        unconstrained = np.array(constrained)

        dynamic_factor_constrained = constrained[self._dynamic_factor_params_idx]
        dynamic_factor_unconstrained = super(_DynamicFactorWithFactorIntercept,
                self).untransform_params(dynamic_factor_constrained)

        unconstrained[self._dynamic_factor_params_idx] = \
                dynamic_factor_unconstrained

        return unconstrained

    def update(self, params, **kwargs):

        k_states = self.k_states
        dtype = self.ssm.dtype

        dynamic_factor_params = params[self._dynamic_factor_params_idx]
        factor_intercept = params[self._factor_intercept_idx]

        super(_DynamicFactorWithFactorIntercept,
                self).update(dynamic_factor_params, **kwargs)

        state_intercept = np.zeros((k_states, 1), dtype=dtype)

        state_intercept[0, 0] = factor_intercept

        self['state_intercept'] = state_intercept

class KimYoo1995Model(SwitchingDynamicFactor):
    '''
    This is switching dynamic factor model with some restrictions on
    parameters. See http://econ.korea.ac.kr/~cjkim/MARKOV/programs/sw_ms.opt.
    '''

    def __init__(self, k_regimes, endog, k_factors, factor_order, **kwargs):

        super(KimYoo1995Model, self).__init__(k_regimes, endog, k_factors,
                factor_order, **kwargs)

        # we need this instance because of its useful methods
        # `_get_dynamic_factor_params`, `_get_params_without_intercept` and
        # others
        self._nonswitching_model = KimYoo1995NonswitchingModel(endog,
                k_factors, factor_order, **kwargs)

        # For the sake of clarity
        self._base_class_parameters = self.parameters

        # params vector for this model differs from params vector in
        # `SwitchingDynamicFactor`.
        self._kimyoo_parameters = MarkovSwitchingParams(k_regimes)

        self._kimyoo_parameters['regime_transition'] = [False] * k_regimes * \
                (k_regimes - 1)

        # Number of nonswitching params is equal to number of parameters in
        # nonswitching model, except of factor intercept (1 value).
        self._kimyoo_parameters['nonswitching_params'] = [False] * \
                (self._nonswitching_model.kimyoo_k_params - 1)

        self._kimyoo_parameters['factor_intercept'] = [True]

        # A dirty hack, required, because Kim-Yoo model's specification is a
        # little different from Statsmodels one.
        self['state_cov', :k_factors, :k_factors] = 0
        self['state_cov', 0, 0] = 1

    def _get_base_class_params(self, params):

        dtype = self.ssm.dtype

        base_class_params = np.zeros((self._base_class_parameters.k_params,),
                dtype=dtype)

        base_class_params[self._base_class_parameters['regime_transition']] = \
                params[self._kimyoo_parameters['regime_transition']]

        params_without_intercept = params[self._kimyoo_parameters[
                'nonswitching_params']]

        base_class_params[self._base_class_parameters['dynamic_factor']] = \
                self._nonswitching_model._get_dynamic_factor_params(
                params_without_intercept)

        base_class_params[self._base_class_parameters['factor_intercept']] = \
                params[self._kimyoo_parameters['factor_intercept']]

        return base_class_params

    def _get_params(self, base_class_params):

        dtype = self.ssm.dtype

        params = np.zeros((self._kimyoo_parameters.k_params,), dtype=dtype)

        params[self._kimyoo_parameters['regime_transition']] = \
                base_class_params[self._base_class_parameters[
                'regime_transition']]

        dynamic_factor_params = base_class_params[self._base_class_parameters[
                'dynamic_factor']]

        params[self._kimyoo_parameters['nonswitching_params']] = \
                self._nonswitching_model._get_params_without_intercept(
                dynamic_factor_params)

        params[self._kimyoo_parameters['factor_intercept']] = \
                base_class_params[self._base_class_parameters[
                'factor_intercept']]

        return params

    @property
    def start_params(self):

        dtype = self.ssm.dtype

        base_start_params = super(KimYoo1995Model, self).start_params

        return self._get_params(base_start_params)

    def get_nonswitching_model(self):

        # don't need to instantiate a new model, since we already have one
        return self._nonswitching_model

    def update_params(self, params, nonswitching_model_params):

        base_class_params = self._get_base_class_params(params)

        nonswitching_base_class_params = \
                self._nonswitching_model._get_base_class_params(
                nonswitching_model_params)

        updated_base_class_params = super(KimYoo1995Model,
                self).update_params(base_class_params,
                nonswitching_base_class_params)

        return self._get_params(updated_base_class_params)

    def transform_model_params(self, unconstrained):

        unconstr_base_class_params = self._get_base_class_params(unconstrained)

        constr_base_class_params = super(KimYoo1995Model,
                self).transform_model_params(unconstr_base_class_params)

        return self._get_params(constr_base_class_params)

    def untransform_model_params(self, constrained):

        constr_base_class_params = self._get_base_class_params(constrained)

        unconstr_base_class_params = super(KimYoo1995Model,
                self).untransform_model_params(constr_base_class_params)

        return self._get_params(unconstr_base_class_params)

    def update(self, params, **kwargs):

        dtype = self.ssm.dtype
        k_regimes = self.k_regimes
        k_states = self.k_states

        base_class_params = self._get_base_class_params(params)

        super(KimYoo1995Model, self).update(base_class_params, **kwargs)

        # Filter initialization.

        initial_state = np.zeros((k_regimes, k_states), dtype=dtype)

        state_intercept = self['state_intercept']

        transition = self['transition'][0]

        raveled_state_cov = self['state_cov'][0].reshape(-1, 1)

        for i in range(k_regimes):
            initial_state[i] = np.linalg.inv(np.identity(k_states,
                    dtype=dtype) - transition).dot(state_intercept[i]
                    ).ravel()

        transition_outer_sqr = np.zeros((k_states * k_states,
                k_states * k_states), dtype=dtype)

        for i in range(k_states):
            for j in range(k_states):
                transition_outer_sqr[i * k_states:i * k_states + k_states,
                        j * k_states:j * k_states + k_states] = \
                        transition * transition[i, j]

        initial_state_cov = np.linalg.inv(np.identity(k_states * k_states,
                dtype=dtype) - transition_outer_sqr).dot(raveled_state_cov
                ).reshape(k_states, k_states).T

        self.initialize_known(initial_state, initial_state_cov)

class KimYoo1995NonswitchingModel(_DynamicFactorWithFactorIntercept):
    '''
    This is dynamic factor model with some restrictions on parameters.
    See http://econ.korea.ac.kr/~cjkim/MARKOV/programs/sw_ms.opt.
    '''

    def __init__(self, endog, k_factors, factor_order, **kwargs):

        super(KimYoo1995NonswitchingModel, self).__init__(endog, k_factors,
                factor_order, **kwargs)

        k_endog = self.k_endog
        error_order = self.error_order

        offset = 0
        self._gamma_idx = np.s_[offset:offset + k_endog + k_factors - 1]
        offset += k_endog + k_factors - 1
        self._phi_idx = np.s_[offset:offset + error_order]
        offset += error_order
        self._psi_idx = np.s_[offset:offset + k_endog * error_order]
        offset += k_endog * error_order
        self._sigma_idx = np.s_[offset:offset + k_endog]
        offset += k_endog
        self._mu_idx = np.s_[offset]
        self._params_without_intercept_idx = np.s_[:offset]
        self._kimyoo_k_params_without_intercept = offset
        offset += 1

        self.kimyoo_k_params = offset

        # For the sake of clarity
        self._base_class_k_params = self.k_params_with_factor_intercept

    def _get_dynamic_factor_params(self, params_without_intercept):
        '''
        params_without_intercept - just a prefix of parameters vector, since
        intercept is the last value
        '''

        dtype = self.ssm.dtype
        k_endog = self.k_endog
        k_factors = self.k_factors
        factor_order = self.factor_order
        error_order = self.error_order

        dynamic_factor_params = np.zeros((self.dynamic_factor_k_params,),
                dtype=dtype)

        # 1. Factor loadings

        factor_loadings_matrix = np.zeros((k_endog, k_factors), dtype=dtype)

        gammas = params_without_intercept[self._gamma_idx]

        factor_loadings_matrix[:, 0] = gammas[:k_endog]
        factor_loadings_matrix[-1, 1:] = gammas[k_endog:]

        dynamic_factor_params[self._params_loadings] = \
                factor_loadings_matrix.ravel()

        # 2. Error covs

        dynamic_factor_params[self._params_error_cov] = \
                params_without_intercept[self._sigma_idx]

        # 3. Factor transition

        phi = params_without_intercept[self._phi_idx]


        # `factor_order` == 1, so this essentially is a square matrix
        factor_transition_params = np.zeros((k_factors,
                k_factors * factor_order), dtype=dtype)

        # TODO: check order of parameters

        factor_transition_params[0, :k_factors - 1] = phi
        factor_transition_params[1:, :-1] = np.identity(k_factors - 1,
                dtype=dtype)

        dynamic_factor_params[self._params_factor_transition] = \
                factor_transition_params.ravel()

        # 4. Error transition

        psi = params_without_intercept[self._psi_idx]

        dynamic_factor_params[self._params_error_transition] = psi

        return dynamic_factor_params

    def _get_params_without_intercept(self, dynamic_factor_params):
        '''
        reverse to previous
        '''

        dtype = self.ssm.dtype
        k_endog = self.k_endog
        k_factors = self.k_factors
        factor_order = self.factor_order
        error_order = self.error_order

        params_without_intercept = np.zeros(( \
                self._kimyoo_k_params_without_intercept,),
                dtype=dtype)

        # 1. Factor loadings

        factor_loadings_matrix = \
                dynamic_factor_params[self._params_loadings].reshape( \
                k_endog, k_factors)

        gammas = np.zeros((k_endog + k_factors - 1,), dtype=dtype)

        gammas[:k_endog] = factor_loadings_matrix[:, 0]
        gammas[k_endog:] = factor_loadings_matrix[-1, 1:]

        params_without_intercept[self._gamma_idx] = gammas

        # 2. Error covs

        params_without_intercept[self._sigma_idx] = dynamic_factor_params[ \
                self._params_error_cov]

        # 3. Factor transition

        # `factor_order` == 1, so this essentially is a square matrix
        factor_transition_params = \
                dynamic_factor_params[self._params_factor_transition \
                ].reshape(k_factors, k_factors * factor_order)

        params_without_intercept[self._phi_idx] = factor_transition_params[0,
                :k_factors - 1]

        # 4. Error transition

        psi = dynamic_factor_params[self._params_error_transition]

        params_without_intercept[self._psi_idx] = psi

        return params_without_intercept

    def _get_base_class_params(self, params):

        dtype = self.ssm.dtype

        base_class_params = np.zeros((self._base_class_k_params,), dtype=dtype)

        base_class_params[self._dynamic_factor_params_idx] = \
                self._get_dynamic_factor_params(
                params[self._params_without_intercept_idx])

        base_class_params[self._factor_intercept_idx] = params[self._mu_idx]

        return base_class_params

    def _get_params(self, base_class_params):

        dtype = self.ssm.dtype

        params_without_intercept = self._get_params_without_intercept(
                base_class_params[self._dynamic_factor_params_idx])

        params = np.zeros((self.kimyoo_k_params,), dtype=dtype)

        params[self._params_without_intercept_idx] = params_without_intercept

        params[self._mu_idx] = base_class_params[self._factor_intercept_idx]

        return params

    @property
    def start_params(self):

        base_start_params = super(KimYoo1995NonswitchingModel,
                self).start_params

        return self._get_params(base_start_params)

    def transform_params(self, unconstrained):

        unconstr_base_class_params = self._get_base_class_params(unconstrained)

        constr_base_class_params = super(KimYoo1995NonswitchingModel,
                self).transform_params(unconstr_base_class_params)

        constrained = self._get_params(constr_base_class_params)

        return constrained

    def untransform_params(self, constrained):

        constr_base_class_params = self._get_base_class_params(constrained)

        unconstr_base_class_params = super(KimYoo1995NonswitchingModel,
                self).untransform_params(constr_base_class_params)

        unconstrained = self._get_params(unconstr_base_class_params)

        return unconstrained

    def update(self, params, **kwargs):

        dtype = self.ssm.dtype
        k_states = self.k_states

        base_class_params = self._get_base_class_params(params)

        super(KimYoo1995NonswitchingModel, self).update(base_class_params,
                **kwargs)

        # Filter initialization.

        state_intercept = self['state_intercept']

        transition = self['transition']

        raveled_state_cov = (self['selection'].dot(self['state_cov']).dot(
                self['selection'].T)).reshape(-1, 1)

        initial_state = np.linalg.inv(np.identity(k_states, dtype=dtype) - \
                transition).dot(state_intercept).ravel()

        transition_outer_sqr = np.zeros((k_states * k_states,
                k_states * k_states), dtype=dtype)

        for i in range(k_states):
            for j in range(k_states):
                transition_outer_sqr[i * k_states:i * k_states + k_states,
                        j * k_states:j * k_states + k_states] = \
                        transition * transition[i, j]

        initial_state_cov = np.linalg.inv(np.identity(k_states * k_states,
                dtype=dtype) - transition_outer_sqr).dot(raveled_state_cov
                ).reshape(k_states, k_states).T

        self.initialize_known(initial_state, initial_state_cov)

class _SwitchingDynamicFactorSmoother(KimSmoother):
    '''
    This is required for compatibility, when we set DynamicFactor `ssm` property
    to KimSmoother instance.
    '''

    def __getitem__(self, key):

        item = super(_SwitchingDynamicFactorSmoother, self).__getitem__(key)

        if key is tuple and key[0] == 'state_cov':
            # `state_cov` is the same for every regime, so any index is OK
            return item[0]
        else:
            return item

